{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow & Keras - Basics of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most importantly... resources\n",
    "\n",
    "https://www.tensorflow.org/api_docs\n",
    "\n",
    "https://keras.io/\n",
    "\n",
    "https://www.tensorflow.org/tutorials/\n",
    "\n",
    "https://www.google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF overview\n",
    "\n",
    "* #### \"End-to-end machine learning platform\" \n",
    "\n",
    "    - Not the only one! Check out PyTorch, Theano, Cognitive Toolkit.\n",
    "   \n",
    "* #### Integrates with high-level APIs like Keras\n",
    "* #### Plays nice with Pandas\n",
    "* #### Makes deep learning *fast* and *easy* *\n",
    "    *<sup>\"easy\"</sup>\n",
    "\n",
    "## Tasks for TensorFlow:\n",
    "\n",
    "* #### Regression\n",
    "    - Predict house prices\n",
    "    - Predict drug metabolic rates\n",
    "    - Predict stock trends *\n",
    "    \n",
    "    *<sup>this is super hard</sup>\n",
    "    \n",
    "    \n",
    "\n",
    "* #### Classification\n",
    "    - Cat or dog?\n",
    "    - Malignant or benign cancer from images\n",
    "    ![](media/dr.png)\n",
    "    <span style=\"font-size:0.75em;\">Google AI Blog: Diabetic Retinopathy</span>\n",
    "\n",
    "\n",
    "\n",
    "* #### Dimensionality reduction\n",
    "    - Visualize high-dimensional data in 2 or 3-D space\n",
    "    - Compress representations for successive ML\n",
    "\n",
    "\n",
    "\n",
    "* #### Generative models\n",
    "    - Create new molecules with desirable properties\n",
    "    - Artificially enhance image resolution\n",
    "    ![](media/molecular_gan.png)\n",
    "    <span style=\"font-size:0.75em;\">Kadurin et al., 2017</span>\n",
    "\n",
    "\n",
    "* #### Reinforcement learning\n",
    "    - Can't beat your friends at chess? Make your computer do it\n",
    "\n",
    "\n",
    "\n",
    "* #### Much more...\n",
    "    - Generic math\n",
    "    - Probabilistic programming with TFP\n",
    "    - Automatic differentiation\n",
    "    - ...\n",
    "\n",
    "\n",
    "## Let's Regress\n",
    "\n",
    "### Imports!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name a more iconic duo, I'll wait\n",
    "\n",
    "#### New imports -- TF and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check our versions for good measure -- these programs may have very different behavior version-to-version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading in housing data as with SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7237550310</td>\n",
       "      <td>20140512T000000</td>\n",
       "      <td>1225000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.50</td>\n",
       "      <td>5420</td>\n",
       "      <td>101930</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>3890</td>\n",
       "      <td>1530</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>98053</td>\n",
       "      <td>47.6561</td>\n",
       "      <td>-122.005</td>\n",
       "      <td>4760</td>\n",
       "      <td>101930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1321400060</td>\n",
       "      <td>20140627T000000</td>\n",
       "      <td>257500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1715</td>\n",
       "      <td>6819</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1715</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>98003</td>\n",
       "      <td>47.3097</td>\n",
       "      <td>-122.327</td>\n",
       "      <td>2238</td>\n",
       "      <td>6819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2008000270</td>\n",
       "      <td>20150115T000000</td>\n",
       "      <td>291850.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1060</td>\n",
       "      <td>9711</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1060</td>\n",
       "      <td>0</td>\n",
       "      <td>1963</td>\n",
       "      <td>0</td>\n",
       "      <td>98198</td>\n",
       "      <td>47.4095</td>\n",
       "      <td>-122.315</td>\n",
       "      <td>1650</td>\n",
       "      <td>9711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2414600126</td>\n",
       "      <td>20150415T000000</td>\n",
       "      <td>229500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1780</td>\n",
       "      <td>7470</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>730</td>\n",
       "      <td>1960</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5123</td>\n",
       "      <td>-122.337</td>\n",
       "      <td>1780</td>\n",
       "      <td>8113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3793500160</td>\n",
       "      <td>20150312T000000</td>\n",
       "      <td>323000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1890</td>\n",
       "      <td>6560</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1890</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98038</td>\n",
       "      <td>47.3684</td>\n",
       "      <td>-122.031</td>\n",
       "      <td>2390</td>\n",
       "      <td>7570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1736800520</td>\n",
       "      <td>20150403T000000</td>\n",
       "      <td>662500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3560</td>\n",
       "      <td>9796</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1860</td>\n",
       "      <td>1700</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98007</td>\n",
       "      <td>47.6007</td>\n",
       "      <td>-122.145</td>\n",
       "      <td>2210</td>\n",
       "      <td>8925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9212900260</td>\n",
       "      <td>20140527T000000</td>\n",
       "      <td>468000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1160</td>\n",
       "      <td>6000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>860</td>\n",
       "      <td>300</td>\n",
       "      <td>1942</td>\n",
       "      <td>0</td>\n",
       "      <td>98115</td>\n",
       "      <td>47.6900</td>\n",
       "      <td>-122.292</td>\n",
       "      <td>1330</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>114101516</td>\n",
       "      <td>20140528T000000</td>\n",
       "      <td>310000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1430</td>\n",
       "      <td>19901</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1430</td>\n",
       "      <td>0</td>\n",
       "      <td>1927</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7558</td>\n",
       "      <td>-122.229</td>\n",
       "      <td>1780</td>\n",
       "      <td>12697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6054650070</td>\n",
       "      <td>20141007T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1370</td>\n",
       "      <td>9680</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1370</td>\n",
       "      <td>0</td>\n",
       "      <td>1977</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6127</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1370</td>\n",
       "      <td>10208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1175000570</td>\n",
       "      <td>20150312T000000</td>\n",
       "      <td>530000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1810</td>\n",
       "      <td>4850</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1810</td>\n",
       "      <td>0</td>\n",
       "      <td>1900</td>\n",
       "      <td>0</td>\n",
       "      <td>98107</td>\n",
       "      <td>47.6700</td>\n",
       "      <td>-122.394</td>\n",
       "      <td>1360</td>\n",
       "      <td>4850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9297300055</td>\n",
       "      <td>20150124T000000</td>\n",
       "      <td>650000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2950</td>\n",
       "      <td>5000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1980</td>\n",
       "      <td>970</td>\n",
       "      <td>1979</td>\n",
       "      <td>0</td>\n",
       "      <td>98126</td>\n",
       "      <td>47.5714</td>\n",
       "      <td>-122.375</td>\n",
       "      <td>2140</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1875500060</td>\n",
       "      <td>20140731T000000</td>\n",
       "      <td>395000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1890</td>\n",
       "      <td>14040</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1890</td>\n",
       "      <td>0</td>\n",
       "      <td>1994</td>\n",
       "      <td>0</td>\n",
       "      <td>98019</td>\n",
       "      <td>47.7277</td>\n",
       "      <td>-121.962</td>\n",
       "      <td>1890</td>\n",
       "      <td>14018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6865200140</td>\n",
       "      <td>20140529T000000</td>\n",
       "      <td>485000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1600</td>\n",
       "      <td>4300</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>1916</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6648</td>\n",
       "      <td>-122.343</td>\n",
       "      <td>1610</td>\n",
       "      <td>4300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>16000397</td>\n",
       "      <td>20141205T000000</td>\n",
       "      <td>189000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1200</td>\n",
       "      <td>9850</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1200</td>\n",
       "      <td>0</td>\n",
       "      <td>1921</td>\n",
       "      <td>0</td>\n",
       "      <td>98002</td>\n",
       "      <td>47.3089</td>\n",
       "      <td>-122.210</td>\n",
       "      <td>1060</td>\n",
       "      <td>5095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7983200060</td>\n",
       "      <td>20150424T000000</td>\n",
       "      <td>230000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1250</td>\n",
       "      <td>9774</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1250</td>\n",
       "      <td>0</td>\n",
       "      <td>1969</td>\n",
       "      <td>0</td>\n",
       "      <td>98003</td>\n",
       "      <td>47.3343</td>\n",
       "      <td>-122.306</td>\n",
       "      <td>1280</td>\n",
       "      <td>8850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6300500875</td>\n",
       "      <td>20140514T000000</td>\n",
       "      <td>385000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1620</td>\n",
       "      <td>4980</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>860</td>\n",
       "      <td>760</td>\n",
       "      <td>1947</td>\n",
       "      <td>0</td>\n",
       "      <td>98133</td>\n",
       "      <td>47.7025</td>\n",
       "      <td>-122.341</td>\n",
       "      <td>1400</td>\n",
       "      <td>4980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2524049179</td>\n",
       "      <td>20140826T000000</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3050</td>\n",
       "      <td>44867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2330</td>\n",
       "      <td>720</td>\n",
       "      <td>1968</td>\n",
       "      <td>0</td>\n",
       "      <td>98040</td>\n",
       "      <td>47.5316</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>4110</td>\n",
       "      <td>20336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7137970340</td>\n",
       "      <td>20140703T000000</td>\n",
       "      <td>285000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2270</td>\n",
       "      <td>6300</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2270</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>98092</td>\n",
       "      <td>47.3266</td>\n",
       "      <td>-122.169</td>\n",
       "      <td>2240</td>\n",
       "      <td>7005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8091400200</td>\n",
       "      <td>20140516T000000</td>\n",
       "      <td>252700.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1070</td>\n",
       "      <td>9643</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1070</td>\n",
       "      <td>0</td>\n",
       "      <td>1985</td>\n",
       "      <td>0</td>\n",
       "      <td>98030</td>\n",
       "      <td>47.3533</td>\n",
       "      <td>-122.166</td>\n",
       "      <td>1220</td>\n",
       "      <td>8386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3814700200</td>\n",
       "      <td>20141120T000000</td>\n",
       "      <td>329000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2450</td>\n",
       "      <td>6500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2450</td>\n",
       "      <td>0</td>\n",
       "      <td>1985</td>\n",
       "      <td>0</td>\n",
       "      <td>98030</td>\n",
       "      <td>47.3739</td>\n",
       "      <td>-122.172</td>\n",
       "      <td>2200</td>\n",
       "      <td>6865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1202000200</td>\n",
       "      <td>20141103T000000</td>\n",
       "      <td>233000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1710</td>\n",
       "      <td>4697</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1710</td>\n",
       "      <td>0</td>\n",
       "      <td>1941</td>\n",
       "      <td>0</td>\n",
       "      <td>98002</td>\n",
       "      <td>47.3048</td>\n",
       "      <td>-122.218</td>\n",
       "      <td>1030</td>\n",
       "      <td>4705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1794500383</td>\n",
       "      <td>20140626T000000</td>\n",
       "      <td>937000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2450</td>\n",
       "      <td>2691</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1750</td>\n",
       "      <td>700</td>\n",
       "      <td>1915</td>\n",
       "      <td>0</td>\n",
       "      <td>98119</td>\n",
       "      <td>47.6386</td>\n",
       "      <td>-122.360</td>\n",
       "      <td>1760</td>\n",
       "      <td>3573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3303700376</td>\n",
       "      <td>20141201T000000</td>\n",
       "      <td>667000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1400</td>\n",
       "      <td>1581</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1400</td>\n",
       "      <td>0</td>\n",
       "      <td>1909</td>\n",
       "      <td>0</td>\n",
       "      <td>98112</td>\n",
       "      <td>47.6221</td>\n",
       "      <td>-122.314</td>\n",
       "      <td>1860</td>\n",
       "      <td>3861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5101402488</td>\n",
       "      <td>20140624T000000</td>\n",
       "      <td>438000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1520</td>\n",
       "      <td>6380</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>790</td>\n",
       "      <td>730</td>\n",
       "      <td>1948</td>\n",
       "      <td>0</td>\n",
       "      <td>98115</td>\n",
       "      <td>47.6950</td>\n",
       "      <td>-122.304</td>\n",
       "      <td>1520</td>\n",
       "      <td>6235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1873100390</td>\n",
       "      <td>20150302T000000</td>\n",
       "      <td>719000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2570</td>\n",
       "      <td>7173</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2570</td>\n",
       "      <td>0</td>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>98052</td>\n",
       "      <td>47.7073</td>\n",
       "      <td>-122.110</td>\n",
       "      <td>2630</td>\n",
       "      <td>6026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21583</th>\n",
       "      <td>2025049203</td>\n",
       "      <td>20140610T000000</td>\n",
       "      <td>399950.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>710</td>\n",
       "      <td>1157</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>710</td>\n",
       "      <td>0</td>\n",
       "      <td>1943</td>\n",
       "      <td>0</td>\n",
       "      <td>98102</td>\n",
       "      <td>47.6413</td>\n",
       "      <td>-122.329</td>\n",
       "      <td>1370</td>\n",
       "      <td>1173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21584</th>\n",
       "      <td>952006823</td>\n",
       "      <td>20141202T000000</td>\n",
       "      <td>380000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1260</td>\n",
       "      <td>900</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>940</td>\n",
       "      <td>320</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98116</td>\n",
       "      <td>47.5621</td>\n",
       "      <td>-122.384</td>\n",
       "      <td>1310</td>\n",
       "      <td>1415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21585</th>\n",
       "      <td>3832050760</td>\n",
       "      <td>20140828T000000</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1870</td>\n",
       "      <td>5000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1870</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98042</td>\n",
       "      <td>47.3339</td>\n",
       "      <td>-122.055</td>\n",
       "      <td>2170</td>\n",
       "      <td>5399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21586</th>\n",
       "      <td>2767604724</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>505000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1430</td>\n",
       "      <td>1201</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1430</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98107</td>\n",
       "      <td>47.6707</td>\n",
       "      <td>-122.381</td>\n",
       "      <td>1430</td>\n",
       "      <td>1249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21587</th>\n",
       "      <td>6632300207</td>\n",
       "      <td>20150305T000000</td>\n",
       "      <td>385000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1520</td>\n",
       "      <td>1488</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1520</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7337</td>\n",
       "      <td>-122.309</td>\n",
       "      <td>1520</td>\n",
       "      <td>1497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21588</th>\n",
       "      <td>2767600688</td>\n",
       "      <td>20141113T000000</td>\n",
       "      <td>414500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1210</td>\n",
       "      <td>1278</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1020</td>\n",
       "      <td>190</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98117</td>\n",
       "      <td>47.6756</td>\n",
       "      <td>-122.375</td>\n",
       "      <td>1210</td>\n",
       "      <td>1118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21589</th>\n",
       "      <td>7570050450</td>\n",
       "      <td>20140910T000000</td>\n",
       "      <td>347500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2540</td>\n",
       "      <td>4760</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2540</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>98038</td>\n",
       "      <td>47.3452</td>\n",
       "      <td>-122.022</td>\n",
       "      <td>2540</td>\n",
       "      <td>4571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21590</th>\n",
       "      <td>7430200100</td>\n",
       "      <td>20140514T000000</td>\n",
       "      <td>1222500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4910</td>\n",
       "      <td>9444</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>3110</td>\n",
       "      <td>1800</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6502</td>\n",
       "      <td>-122.066</td>\n",
       "      <td>4560</td>\n",
       "      <td>11063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21591</th>\n",
       "      <td>4140940150</td>\n",
       "      <td>20141002T000000</td>\n",
       "      <td>572000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2770</td>\n",
       "      <td>3852</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2770</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5001</td>\n",
       "      <td>-122.232</td>\n",
       "      <td>1810</td>\n",
       "      <td>5641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21592</th>\n",
       "      <td>1931300412</td>\n",
       "      <td>20150416T000000</td>\n",
       "      <td>475000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1190</td>\n",
       "      <td>1200</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1190</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6542</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1180</td>\n",
       "      <td>1224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21593</th>\n",
       "      <td>8672200110</td>\n",
       "      <td>20150317T000000</td>\n",
       "      <td>1088000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4170</td>\n",
       "      <td>8142</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>4170</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98056</td>\n",
       "      <td>47.5354</td>\n",
       "      <td>-122.181</td>\n",
       "      <td>3030</td>\n",
       "      <td>7980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21594</th>\n",
       "      <td>5087900040</td>\n",
       "      <td>20141017T000000</td>\n",
       "      <td>350000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2500</td>\n",
       "      <td>5995</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98042</td>\n",
       "      <td>47.3749</td>\n",
       "      <td>-122.107</td>\n",
       "      <td>2530</td>\n",
       "      <td>5988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21595</th>\n",
       "      <td>1972201967</td>\n",
       "      <td>20141031T000000</td>\n",
       "      <td>520000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1530</td>\n",
       "      <td>981</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1480</td>\n",
       "      <td>50</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6533</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "      <td>1282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21596</th>\n",
       "      <td>7502800100</td>\n",
       "      <td>20140813T000000</td>\n",
       "      <td>679950.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3600</td>\n",
       "      <td>9437</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3600</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98059</td>\n",
       "      <td>47.4822</td>\n",
       "      <td>-122.131</td>\n",
       "      <td>3550</td>\n",
       "      <td>9421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21597</th>\n",
       "      <td>191100405</td>\n",
       "      <td>20150421T000000</td>\n",
       "      <td>1575000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3410</td>\n",
       "      <td>10125</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>3410</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98040</td>\n",
       "      <td>47.5653</td>\n",
       "      <td>-122.223</td>\n",
       "      <td>2290</td>\n",
       "      <td>10125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21598</th>\n",
       "      <td>8956200760</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>541800.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3118</td>\n",
       "      <td>7866</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3118</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98001</td>\n",
       "      <td>47.2931</td>\n",
       "      <td>-122.264</td>\n",
       "      <td>2673</td>\n",
       "      <td>6500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21599</th>\n",
       "      <td>7202300110</td>\n",
       "      <td>20140915T000000</td>\n",
       "      <td>810000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3990</td>\n",
       "      <td>7838</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3990</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98053</td>\n",
       "      <td>47.6857</td>\n",
       "      <td>-122.046</td>\n",
       "      <td>3370</td>\n",
       "      <td>6814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21600</th>\n",
       "      <td>249000205</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>1537000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4470</td>\n",
       "      <td>8088</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>4470</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98004</td>\n",
       "      <td>47.6321</td>\n",
       "      <td>-122.200</td>\n",
       "      <td>2780</td>\n",
       "      <td>8964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21601</th>\n",
       "      <td>5100403806</td>\n",
       "      <td>20150407T000000</td>\n",
       "      <td>467000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1425</td>\n",
       "      <td>1179</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1425</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.6963</td>\n",
       "      <td>-122.318</td>\n",
       "      <td>1285</td>\n",
       "      <td>1253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21602</th>\n",
       "      <td>844000965</td>\n",
       "      <td>20140626T000000</td>\n",
       "      <td>224000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1500</td>\n",
       "      <td>11968</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98010</td>\n",
       "      <td>47.3095</td>\n",
       "      <td>-122.002</td>\n",
       "      <td>1320</td>\n",
       "      <td>11303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21603</th>\n",
       "      <td>7852140040</td>\n",
       "      <td>20140825T000000</td>\n",
       "      <td>507250.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2270</td>\n",
       "      <td>5536</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2270</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98065</td>\n",
       "      <td>47.5389</td>\n",
       "      <td>-121.881</td>\n",
       "      <td>2270</td>\n",
       "      <td>5731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21604</th>\n",
       "      <td>9834201367</td>\n",
       "      <td>20150126T000000</td>\n",
       "      <td>429000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1490</td>\n",
       "      <td>1126</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1490</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5699</td>\n",
       "      <td>-122.288</td>\n",
       "      <td>1400</td>\n",
       "      <td>1230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21605</th>\n",
       "      <td>3448900210</td>\n",
       "      <td>20141014T000000</td>\n",
       "      <td>610685.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2520</td>\n",
       "      <td>6023</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2520</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98056</td>\n",
       "      <td>47.5137</td>\n",
       "      <td>-122.167</td>\n",
       "      <td>2520</td>\n",
       "      <td>6023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21606</th>\n",
       "      <td>7936000429</td>\n",
       "      <td>20150326T000000</td>\n",
       "      <td>1007500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3510</td>\n",
       "      <td>7200</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2600</td>\n",
       "      <td>910</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5537</td>\n",
       "      <td>-122.398</td>\n",
       "      <td>2050</td>\n",
       "      <td>6200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21607</th>\n",
       "      <td>2997800021</td>\n",
       "      <td>20150219T000000</td>\n",
       "      <td>475000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1310</td>\n",
       "      <td>1294</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1180</td>\n",
       "      <td>130</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98116</td>\n",
       "      <td>47.5773</td>\n",
       "      <td>-122.409</td>\n",
       "      <td>1330</td>\n",
       "      <td>1265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21608</th>\n",
       "      <td>263000018</td>\n",
       "      <td>20140521T000000</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1530</td>\n",
       "      <td>1131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1530</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6993</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21609</th>\n",
       "      <td>6600060120</td>\n",
       "      <td>20150223T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2310</td>\n",
       "      <td>5813</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2310</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5107</td>\n",
       "      <td>-122.362</td>\n",
       "      <td>1830</td>\n",
       "      <td>7200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21610</th>\n",
       "      <td>1523300141</td>\n",
       "      <td>20140623T000000</td>\n",
       "      <td>402101.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1350</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5944</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21611</th>\n",
       "      <td>291310100</td>\n",
       "      <td>20150116T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1600</td>\n",
       "      <td>2388</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>98027</td>\n",
       "      <td>47.5345</td>\n",
       "      <td>-122.069</td>\n",
       "      <td>1410</td>\n",
       "      <td>1287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21612</th>\n",
       "      <td>1523300157</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1076</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5941</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>1357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21613 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id             date      price  bedrooms  bathrooms  \\\n",
       "0      7129300520  20141013T000000   221900.0         3       1.00   \n",
       "1      6414100192  20141209T000000   538000.0         3       2.25   \n",
       "2      5631500400  20150225T000000   180000.0         2       1.00   \n",
       "3      2487200875  20141209T000000   604000.0         4       3.00   \n",
       "4      1954400510  20150218T000000   510000.0         3       2.00   \n",
       "5      7237550310  20140512T000000  1225000.0         4       4.50   \n",
       "6      1321400060  20140627T000000   257500.0         3       2.25   \n",
       "7      2008000270  20150115T000000   291850.0         3       1.50   \n",
       "8      2414600126  20150415T000000   229500.0         3       1.00   \n",
       "9      3793500160  20150312T000000   323000.0         3       2.50   \n",
       "10     1736800520  20150403T000000   662500.0         3       2.50   \n",
       "11     9212900260  20140527T000000   468000.0         2       1.00   \n",
       "12      114101516  20140528T000000   310000.0         3       1.00   \n",
       "13     6054650070  20141007T000000   400000.0         3       1.75   \n",
       "14     1175000570  20150312T000000   530000.0         5       2.00   \n",
       "15     9297300055  20150124T000000   650000.0         4       3.00   \n",
       "16     1875500060  20140731T000000   395000.0         3       2.00   \n",
       "17     6865200140  20140529T000000   485000.0         4       1.00   \n",
       "18       16000397  20141205T000000   189000.0         2       1.00   \n",
       "19     7983200060  20150424T000000   230000.0         3       1.00   \n",
       "20     6300500875  20140514T000000   385000.0         4       1.75   \n",
       "21     2524049179  20140826T000000  2000000.0         3       2.75   \n",
       "22     7137970340  20140703T000000   285000.0         5       2.50   \n",
       "23     8091400200  20140516T000000   252700.0         2       1.50   \n",
       "24     3814700200  20141120T000000   329000.0         3       2.25   \n",
       "25     1202000200  20141103T000000   233000.0         3       2.00   \n",
       "26     1794500383  20140626T000000   937000.0         3       1.75   \n",
       "27     3303700376  20141201T000000   667000.0         3       1.00   \n",
       "28     5101402488  20140624T000000   438000.0         3       1.75   \n",
       "29     1873100390  20150302T000000   719000.0         4       2.50   \n",
       "...           ...              ...        ...       ...        ...   \n",
       "21583  2025049203  20140610T000000   399950.0         2       1.00   \n",
       "21584   952006823  20141202T000000   380000.0         3       2.50   \n",
       "21585  3832050760  20140828T000000   270000.0         3       2.50   \n",
       "21586  2767604724  20141015T000000   505000.0         2       2.50   \n",
       "21587  6632300207  20150305T000000   385000.0         3       2.50   \n",
       "21588  2767600688  20141113T000000   414500.0         2       1.50   \n",
       "21589  7570050450  20140910T000000   347500.0         3       2.50   \n",
       "21590  7430200100  20140514T000000  1222500.0         4       3.50   \n",
       "21591  4140940150  20141002T000000   572000.0         4       2.75   \n",
       "21592  1931300412  20150416T000000   475000.0         3       2.25   \n",
       "21593  8672200110  20150317T000000  1088000.0         5       3.75   \n",
       "21594  5087900040  20141017T000000   350000.0         4       2.75   \n",
       "21595  1972201967  20141031T000000   520000.0         2       2.25   \n",
       "21596  7502800100  20140813T000000   679950.0         5       2.75   \n",
       "21597   191100405  20150421T000000  1575000.0         4       3.25   \n",
       "21598  8956200760  20141013T000000   541800.0         4       2.50   \n",
       "21599  7202300110  20140915T000000   810000.0         4       3.00   \n",
       "21600   249000205  20141015T000000  1537000.0         5       3.75   \n",
       "21601  5100403806  20150407T000000   467000.0         3       2.50   \n",
       "21602   844000965  20140626T000000   224000.0         3       1.75   \n",
       "21603  7852140040  20140825T000000   507250.0         3       2.50   \n",
       "21604  9834201367  20150126T000000   429000.0         3       2.00   \n",
       "21605  3448900210  20141014T000000   610685.0         4       2.50   \n",
       "21606  7936000429  20150326T000000  1007500.0         4       3.50   \n",
       "21607  2997800021  20150219T000000   475000.0         3       2.50   \n",
       "21608   263000018  20140521T000000   360000.0         3       2.50   \n",
       "21609  6600060120  20150223T000000   400000.0         4       2.50   \n",
       "21610  1523300141  20140623T000000   402101.0         2       0.75   \n",
       "21611   291310100  20150116T000000   400000.0         3       2.50   \n",
       "21612  1523300157  20141015T000000   325000.0         2       0.75   \n",
       "\n",
       "       sqft_living  sqft_lot  floors  waterfront  view  ...  grade  \\\n",
       "0             1180      5650     1.0           0     0  ...      7   \n",
       "1             2570      7242     2.0           0     0  ...      7   \n",
       "2              770     10000     1.0           0     0  ...      6   \n",
       "3             1960      5000     1.0           0     0  ...      7   \n",
       "4             1680      8080     1.0           0     0  ...      8   \n",
       "5             5420    101930     1.0           0     0  ...     11   \n",
       "6             1715      6819     2.0           0     0  ...      7   \n",
       "7             1060      9711     1.0           0     0  ...      7   \n",
       "8             1780      7470     1.0           0     0  ...      7   \n",
       "9             1890      6560     2.0           0     0  ...      7   \n",
       "10            3560      9796     1.0           0     0  ...      8   \n",
       "11            1160      6000     1.0           0     0  ...      7   \n",
       "12            1430     19901     1.5           0     0  ...      7   \n",
       "13            1370      9680     1.0           0     0  ...      7   \n",
       "14            1810      4850     1.5           0     0  ...      7   \n",
       "15            2950      5000     2.0           0     3  ...      9   \n",
       "16            1890     14040     2.0           0     0  ...      7   \n",
       "17            1600      4300     1.5           0     0  ...      7   \n",
       "18            1200      9850     1.0           0     0  ...      7   \n",
       "19            1250      9774     1.0           0     0  ...      7   \n",
       "20            1620      4980     1.0           0     0  ...      7   \n",
       "21            3050     44867     1.0           0     4  ...      9   \n",
       "22            2270      6300     2.0           0     0  ...      8   \n",
       "23            1070      9643     1.0           0     0  ...      7   \n",
       "24            2450      6500     2.0           0     0  ...      8   \n",
       "25            1710      4697     1.5           0     0  ...      6   \n",
       "26            2450      2691     2.0           0     0  ...      8   \n",
       "27            1400      1581     1.5           0     0  ...      8   \n",
       "28            1520      6380     1.0           0     0  ...      7   \n",
       "29            2570      7173     2.0           0     0  ...      8   \n",
       "...            ...       ...     ...         ...   ...  ...    ...   \n",
       "21583          710      1157     2.0           0     0  ...      7   \n",
       "21584         1260       900     2.0           0     0  ...      7   \n",
       "21585         1870      5000     2.0           0     0  ...      7   \n",
       "21586         1430      1201     3.0           0     0  ...      8   \n",
       "21587         1520      1488     3.0           0     0  ...      8   \n",
       "21588         1210      1278     2.0           0     0  ...      8   \n",
       "21589         2540      4760     2.0           0     0  ...      8   \n",
       "21590         4910      9444     1.5           0     0  ...     11   \n",
       "21591         2770      3852     2.0           0     0  ...      8   \n",
       "21592         1190      1200     3.0           0     0  ...      8   \n",
       "21593         4170      8142     2.0           0     2  ...     10   \n",
       "21594         2500      5995     2.0           0     0  ...      8   \n",
       "21595         1530       981     3.0           0     0  ...      8   \n",
       "21596         3600      9437     2.0           0     0  ...      9   \n",
       "21597         3410     10125     2.0           0     0  ...     10   \n",
       "21598         3118      7866     2.0           0     2  ...      9   \n",
       "21599         3990      7838     2.0           0     0  ...      9   \n",
       "21600         4470      8088     2.0           0     0  ...     11   \n",
       "21601         1425      1179     3.0           0     0  ...      8   \n",
       "21602         1500     11968     1.0           0     0  ...      6   \n",
       "21603         2270      5536     2.0           0     0  ...      8   \n",
       "21604         1490      1126     3.0           0     0  ...      8   \n",
       "21605         2520      6023     2.0           0     0  ...      9   \n",
       "21606         3510      7200     2.0           0     0  ...      9   \n",
       "21607         1310      1294     2.0           0     0  ...      8   \n",
       "21608         1530      1131     3.0           0     0  ...      8   \n",
       "21609         2310      5813     2.0           0     0  ...      8   \n",
       "21610         1020      1350     2.0           0     0  ...      7   \n",
       "21611         1600      2388     2.0           0     0  ...      8   \n",
       "21612         1020      1076     2.0           0     0  ...      7   \n",
       "\n",
       "       sqft_above  sqft_basement  yr_built  yr_renovated  zipcode      lat  \\\n",
       "0            1180              0      1955             0    98178  47.5112   \n",
       "1            2170            400      1951          1991    98125  47.7210   \n",
       "2             770              0      1933             0    98028  47.7379   \n",
       "3            1050            910      1965             0    98136  47.5208   \n",
       "4            1680              0      1987             0    98074  47.6168   \n",
       "5            3890           1530      2001             0    98053  47.6561   \n",
       "6            1715              0      1995             0    98003  47.3097   \n",
       "7            1060              0      1963             0    98198  47.4095   \n",
       "8            1050            730      1960             0    98146  47.5123   \n",
       "9            1890              0      2003             0    98038  47.3684   \n",
       "10           1860           1700      1965             0    98007  47.6007   \n",
       "11            860            300      1942             0    98115  47.6900   \n",
       "12           1430              0      1927             0    98028  47.7558   \n",
       "13           1370              0      1977             0    98074  47.6127   \n",
       "14           1810              0      1900             0    98107  47.6700   \n",
       "15           1980            970      1979             0    98126  47.5714   \n",
       "16           1890              0      1994             0    98019  47.7277   \n",
       "17           1600              0      1916             0    98103  47.6648   \n",
       "18           1200              0      1921             0    98002  47.3089   \n",
       "19           1250              0      1969             0    98003  47.3343   \n",
       "20            860            760      1947             0    98133  47.7025   \n",
       "21           2330            720      1968             0    98040  47.5316   \n",
       "22           2270              0      1995             0    98092  47.3266   \n",
       "23           1070              0      1985             0    98030  47.3533   \n",
       "24           2450              0      1985             0    98030  47.3739   \n",
       "25           1710              0      1941             0    98002  47.3048   \n",
       "26           1750            700      1915             0    98119  47.6386   \n",
       "27           1400              0      1909             0    98112  47.6221   \n",
       "28            790            730      1948             0    98115  47.6950   \n",
       "29           2570              0      2005             0    98052  47.7073   \n",
       "...           ...            ...       ...           ...      ...      ...   \n",
       "21583         710              0      1943             0    98102  47.6413   \n",
       "21584         940            320      2007             0    98116  47.5621   \n",
       "21585        1870              0      2009             0    98042  47.3339   \n",
       "21586        1430              0      2009             0    98107  47.6707   \n",
       "21587        1520              0      2006             0    98125  47.7337   \n",
       "21588        1020            190      2007             0    98117  47.6756   \n",
       "21589        2540              0      2010             0    98038  47.3452   \n",
       "21590        3110           1800      2007             0    98074  47.6502   \n",
       "21591        2770              0      2014             0    98178  47.5001   \n",
       "21592        1190              0      2008             0    98103  47.6542   \n",
       "21593        4170              0      2006             0    98056  47.5354   \n",
       "21594        2500              0      2008             0    98042  47.3749   \n",
       "21595        1480             50      2006             0    98103  47.6533   \n",
       "21596        3600              0      2014             0    98059  47.4822   \n",
       "21597        3410              0      2007             0    98040  47.5653   \n",
       "21598        3118              0      2014             0    98001  47.2931   \n",
       "21599        3990              0      2003             0    98053  47.6857   \n",
       "21600        4470              0      2008             0    98004  47.6321   \n",
       "21601        1425              0      2008             0    98125  47.6963   \n",
       "21602        1500              0      2014             0    98010  47.3095   \n",
       "21603        2270              0      2003             0    98065  47.5389   \n",
       "21604        1490              0      2014             0    98144  47.5699   \n",
       "21605        2520              0      2014             0    98056  47.5137   \n",
       "21606        2600            910      2009             0    98136  47.5537   \n",
       "21607        1180            130      2008             0    98116  47.5773   \n",
       "21608        1530              0      2009             0    98103  47.6993   \n",
       "21609        2310              0      2014             0    98146  47.5107   \n",
       "21610        1020              0      2009             0    98144  47.5944   \n",
       "21611        1600              0      2004             0    98027  47.5345   \n",
       "21612        1020              0      2008             0    98144  47.5941   \n",
       "\n",
       "          long  sqft_living15  sqft_lot15  \n",
       "0     -122.257           1340        5650  \n",
       "1     -122.319           1690        7639  \n",
       "2     -122.233           2720        8062  \n",
       "3     -122.393           1360        5000  \n",
       "4     -122.045           1800        7503  \n",
       "5     -122.005           4760      101930  \n",
       "6     -122.327           2238        6819  \n",
       "7     -122.315           1650        9711  \n",
       "8     -122.337           1780        8113  \n",
       "9     -122.031           2390        7570  \n",
       "10    -122.145           2210        8925  \n",
       "11    -122.292           1330        6000  \n",
       "12    -122.229           1780       12697  \n",
       "13    -122.045           1370       10208  \n",
       "14    -122.394           1360        4850  \n",
       "15    -122.375           2140        4000  \n",
       "16    -121.962           1890       14018  \n",
       "17    -122.343           1610        4300  \n",
       "18    -122.210           1060        5095  \n",
       "19    -122.306           1280        8850  \n",
       "20    -122.341           1400        4980  \n",
       "21    -122.233           4110       20336  \n",
       "22    -122.169           2240        7005  \n",
       "23    -122.166           1220        8386  \n",
       "24    -122.172           2200        6865  \n",
       "25    -122.218           1030        4705  \n",
       "26    -122.360           1760        3573  \n",
       "27    -122.314           1860        3861  \n",
       "28    -122.304           1520        6235  \n",
       "29    -122.110           2630        6026  \n",
       "...        ...            ...         ...  \n",
       "21583 -122.329           1370        1173  \n",
       "21584 -122.384           1310        1415  \n",
       "21585 -122.055           2170        5399  \n",
       "21586 -122.381           1430        1249  \n",
       "21587 -122.309           1520        1497  \n",
       "21588 -122.375           1210        1118  \n",
       "21589 -122.022           2540        4571  \n",
       "21590 -122.066           4560       11063  \n",
       "21591 -122.232           1810        5641  \n",
       "21592 -122.346           1180        1224  \n",
       "21593 -122.181           3030        7980  \n",
       "21594 -122.107           2530        5988  \n",
       "21595 -122.346           1530        1282  \n",
       "21596 -122.131           3550        9421  \n",
       "21597 -122.223           2290       10125  \n",
       "21598 -122.264           2673        6500  \n",
       "21599 -122.046           3370        6814  \n",
       "21600 -122.200           2780        8964  \n",
       "21601 -122.318           1285        1253  \n",
       "21602 -122.002           1320       11303  \n",
       "21603 -121.881           2270        5731  \n",
       "21604 -122.288           1400        1230  \n",
       "21605 -122.167           2520        6023  \n",
       "21606 -122.398           2050        6200  \n",
       "21607 -122.409           1330        1265  \n",
       "21608 -122.346           1530        1509  \n",
       "21609 -122.362           1830        7200  \n",
       "21610 -122.299           1020        2007  \n",
       "21611 -122.069           1410        1287  \n",
       "21612 -122.299           1020        1357  \n",
       "\n",
       "[21613 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('kc_house_data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_selection = [\"bedrooms\",\"bathrooms\",\"sqft_living\",\"sqft_lot\",\n",
    "                    \"floors\",\"condition\",\"grade\",\"sqft_above\",\n",
    "                    \"sqft_basement\",\"sqft_living15\",\"sqft_lot15\",\n",
    "                    \"lat\", \"long\",\"yr_built\",\"yr_renovated\",\"waterfront\"]\n",
    "\n",
    "selected_feature = np.array(data[column_selection])\n",
    "price = np.array(data[\"price\"])\n",
    "selected_feature_train = selected_feature[:20000]\n",
    "price_train = price[:20000]\n",
    "\n",
    "selected_feature_test = selected_feature[20000:]\n",
    "price_test = price[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y,y_pred):\n",
    "    return np.mean(np.abs(y-y_pred)/y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_len = len(column_selection)\n",
    "model.add(keras.layers.Dense(50, input_dim=input_len, activation='relu'))\n",
    "model.add(keras.layers.Dense(50, activation='relu'))\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "20000/20000 [==============================] - 2s 98us/step - loss: 365669033495.7568\n",
      "Epoch 2/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 256892456638.8736\n",
      "Epoch 3/50\n",
      "20000/20000 [==============================] - 1s 46us/step - loss: 118581314053.7344\n",
      "Epoch 4/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 77825994561.9456\n",
      "Epoch 5/50\n",
      "20000/20000 [==============================] - 1s 46us/step - loss: 74435019171.4304\n",
      "Epoch 6/50\n",
      "20000/20000 [==============================] - 1s 42us/step - loss: 72377106844.8768\n",
      "Epoch 7/50\n",
      "20000/20000 [==============================] - 1s 43us/step - loss: 70845085161.8816\n",
      "Epoch 8/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 69536943413.6576\n",
      "Epoch 9/50\n",
      "20000/20000 [==============================] - 1s 43us/step - loss: 68377267208.1920\n",
      "Epoch 10/50\n",
      "20000/20000 [==============================] - 1s 43us/step - loss: 67398507482.3168\n",
      "Epoch 11/50\n",
      "20000/20000 [==============================] - 1s 41us/step - loss: 66449516580.0448\n",
      "Epoch 12/50\n",
      "20000/20000 [==============================] - 1s 43us/step - loss: 65753233070.4896\n",
      "Epoch 13/50\n",
      "20000/20000 [==============================] - 1s 45us/step - loss: 65246045116.8256\n",
      "Epoch 14/50\n",
      "20000/20000 [==============================] - 1s 47us/step - loss: 64865689862.1440\n",
      "Epoch 15/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 64658290678.1696\n",
      "Epoch 16/50\n",
      "20000/20000 [==============================] - 1s 55us/step - loss: 64555758970.4704\n",
      "Epoch 17/50\n",
      "20000/20000 [==============================] - 1s 57us/step - loss: 64348194576.7936\n",
      "Epoch 18/50\n",
      "20000/20000 [==============================] - 1s 47us/step - loss: 64161415351.5008\n",
      "Epoch 19/50\n",
      "20000/20000 [==============================] - 1s 49us/step - loss: 64153920097.4848\n",
      "Epoch 20/50\n",
      "20000/20000 [==============================] - 1s 56us/step - loss: 64164327273.2672\n",
      "Epoch 21/50\n",
      "20000/20000 [==============================] - 1s 43us/step - loss: 64090946404.3520\n",
      "Epoch 22/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 63971375454.6176\n",
      "Epoch 23/50\n",
      "20000/20000 [==============================] - 1s 50us/step - loss: 63800793130.5984\n",
      "Epoch 24/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 63907617269.3504\n",
      "Epoch 25/50\n",
      "20000/20000 [==============================] - 1s 42us/step - loss: 63738145996.8000\n",
      "Epoch 26/50\n",
      "20000/20000 [==============================] - 1s 43us/step - loss: 63752756081.4592\n",
      "Epoch 27/50\n",
      "20000/20000 [==============================] - 1s 45us/step - loss: 63687096870.5024\n",
      "Epoch 28/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 63496693121.0240\n",
      "Epoch 29/50\n",
      "20000/20000 [==============================] - 1s 42us/step - loss: 63599389573.1200\n",
      "Epoch 30/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 63432194706.6368\n",
      "Epoch 31/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 63428794292.6336\n",
      "Epoch 32/50\n",
      "20000/20000 [==============================] - 1s 45us/step - loss: 63189229558.1696\n",
      "Epoch 33/50\n",
      "20000/20000 [==============================] - 1s 54us/step - loss: 63168384650.4448\n",
      "Epoch 34/50\n",
      "20000/20000 [==============================] - 1s 61us/step - loss: 63209628644.1472\n",
      "Epoch 35/50\n",
      "20000/20000 [==============================] - 1s 49us/step - loss: 63039056320.9216\n",
      "Epoch 36/50\n",
      "20000/20000 [==============================] - 1s 49us/step - loss: 63310884844.3392\n",
      "Epoch 37/50\n",
      "20000/20000 [==============================] - 1s 43us/step - loss: 62910145744.0768\n",
      "Epoch 38/50\n",
      "20000/20000 [==============================] - 1s 41us/step - loss: 63133198516.2240\n",
      "Epoch 39/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 62568853700.6080\n",
      "Epoch 40/50\n",
      "20000/20000 [==============================] - 1s 46us/step - loss: 62554151891.7632\n",
      "Epoch 41/50\n",
      "20000/20000 [==============================] - 1s 43us/step - loss: 62506338392.4736\n",
      "Epoch 42/50\n",
      "20000/20000 [==============================] - 1s 42us/step - loss: 62140409171.1488\n",
      "Epoch 43/50\n",
      "20000/20000 [==============================] - 1s 46us/step - loss: 62015432962.8672\n",
      "Epoch 44/50\n",
      "20000/20000 [==============================] - 1s 52us/step - loss: 61866165347.9424\n",
      "Epoch 45/50\n",
      "20000/20000 [==============================] - 1s 46us/step - loss: 61890855593.5744\n",
      "Epoch 46/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 61635160932.3520\n",
      "Epoch 47/50\n",
      "20000/20000 [==============================] - 1s 43us/step - loss: 61604617453.5680\n",
      "Epoch 48/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 61802675044.3520\n",
      "Epoch 49/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 61439805371.1872\n",
      "Epoch 50/50\n",
      "20000/20000 [==============================] - 1s 45us/step - loss: 61255870644.2240\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(selected_feature_train, price_train, epochs=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5347612664268558"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(selected_feature_test)\n",
    "score(preds, price_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Like SKLearn, it's easy to train and evaluate simple models.\n",
    "#### ... but we should try to do better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Deep Learning -- What you need to know\n",
    "### Train, Validation, Test:\n",
    "   * Optimize parameters with Train (weights, biases)\n",
    "   * Optimize hyperparameters with Validation (layer width & depth, activation functions, etc.)\n",
    "   * Optimize NOTHING with Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split out a validation set for hyperparameter optimization\n",
    "\n",
    "selected_feature_train = selected_feature[:18000]\n",
    "price_train = price[:18000]\n",
    "\n",
    "selected_feature_val = selected_feature[18000:20000]\n",
    "price_val = price[18000:20000]\n",
    "\n",
    "selected_feature_test = selected_feature[20000:]\n",
    "price_test = price[20000:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the future, try better validation schemes like [k-fold cross validation](https://chrisalbon.com/deep_learning/keras/k-fold_cross-validating_neural_networks/), though 80/20 or 90/10 train/val like this works in a pinch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a hyperparameter optimization:\n",
    "\n",
    "### Try three activation functions to use for dense layers in the neural network above. Save the model that achieves the best validation loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint: [activation functions](http://letmegooglethat.com/?q=keras+activation+functions)\n",
    "\n",
    "#### Hint: `model.fit` has argument \"`validation_data`\" which takes a tuple of features and targets\n",
    "\n",
    "#### Hint: Use `model.save(\"filename.h5\")` to save a model locally. If you want to use it later, just call `keras.models.load_model(\"filename.h5\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For easy looping, define neural network model as a function\n",
    "def nn_model(optimizer='adam',\n",
    "             activation='relu',\n",
    "             layers=[20,20],\n",
    "             loss='mean_squared_error'):\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(50, input_dim=input_len, activation=activ))\n",
    "    model.add(keras.layers.Dense(50, activation=activ))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 2s 112us/step - loss: 532972.9934 - val_loss: 557913.1590\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 532965.4812 - val_loss: 557906.6270\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532959.0822 - val_loss: 557900.4470\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 532952.9774 - val_loss: 557894.4835\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532947.1255 - val_loss: 557888.6410\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 532941.3981 - val_loss: 557882.8710\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532935.7283 - val_loss: 557877.4890\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 532930.0904 - val_loss: 557871.7055\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 532924.4947 - val_loss: 557865.9605\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 532918.9111 - val_loss: 557860.6070\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 1s 68us/step - loss: 532913.3341 - val_loss: 557854.8710\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 1s 71us/step - loss: 532907.7839 - val_loss: 557849.5335\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532901.9453 - val_loss: 557843.4790\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532895.9439 - val_loss: 557837.5335\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 532890.1252 - val_loss: 557831.6970\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 532884.3696 - val_loss: 557825.8085\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532878.6352 - val_loss: 557819.9945\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532872.9156 - val_loss: 557814.5770\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 532867.2186 - val_loss: 557808.8160\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 1s 66us/step - loss: 532861.5358 - val_loss: 557803.0325\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532855.8446 - val_loss: 557797.5335\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 532850.1772 - val_loss: 557791.7380\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 532844.5163 - val_loss: 557785.9525\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 1s 59us/step - loss: 532838.8477 - val_loss: 557780.4795\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532833.1863 - val_loss: 557774.7880\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 532827.5322 - val_loss: 557769.0405\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 532821.8691 - val_loss: 557763.5375\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 532816.2164 - val_loss: 557757.7920\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 532810.5812 - val_loss: 557751.9805\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 532804.9208 - val_loss: 557746.6030\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 532799.2741 - val_loss: 557740.8360\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532793.5208 - val_loss: 557734.8200\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 532787.3027 - val_loss: 557728.6670\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 532781.3026 - val_loss: 557722.7840\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 532775.3936 - val_loss: 557716.8360\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 532769.5154 - val_loss: 557710.8790\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 532763.6704 - val_loss: 557705.0485\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532757.8221 - val_loss: 557699.4930\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532751.9981 - val_loss: 557693.6650\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 532746.1829 - val_loss: 557687.7030\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 532740.3766 - val_loss: 557681.7960\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 532734.5782 - val_loss: 557675.9395\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 532728.7747 - val_loss: 557670.4250\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 1s 60us/step - loss: 532722.9705 - val_loss: 557664.5770\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 1s 66us/step - loss: 532717.1744 - val_loss: 557658.6670\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 532711.3830 - val_loss: 557652.8460\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532705.5920 - val_loss: 557647.0325\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532699.7998 - val_loss: 557641.4890\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 532694.0074 - val_loss: 557635.6650\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532688.2255 - val_loss: 557629.7320\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 2s 114us/step - loss: 532969.4615 - val_loss: 557908.8550\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532960.5804 - val_loss: 557901.0625\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 532953.0155 - val_loss: 557893.7140\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 1s 65us/step - loss: 532945.6238 - val_loss: 557886.4470\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532938.2886 - val_loss: 557879.0035\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 532930.9872 - val_loss: 557871.7380\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 1s 59us/step - loss: 532923.7338 - val_loss: 557864.6030\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 532916.4709 - val_loss: 557857.4650\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 532909.2345 - val_loss: 557849.9195\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 1s 61us/step - loss: 532902.0029 - val_loss: 557842.8240\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 532894.7726 - val_loss: 557835.6670\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 532887.5627 - val_loss: 557828.4470\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 532880.3550 - val_loss: 557821.0485\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 532873.1299 - val_loss: 557813.8125\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532865.9312 - val_loss: 557806.7180\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 532858.7204 - val_loss: 557799.6650\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 532851.5249 - val_loss: 557792.4430\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 532844.3173 - val_loss: 557785.0485\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 532837.1185 - val_loss: 557777.8125\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 532829.9180 - val_loss: 557770.7180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532822.7204 - val_loss: 557763.6650\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532815.5310 - val_loss: 557756.4430\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532808.3276 - val_loss: 557749.0485\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 532801.1254 - val_loss: 557741.8125\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532793.9388 - val_loss: 557734.7840\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 1s 62us/step - loss: 532786.7395 - val_loss: 557727.6650\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 1s 60us/step - loss: 532779.5508 - val_loss: 557720.4430\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 532772.3604 - val_loss: 557713.0485\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532765.1579 - val_loss: 557705.8150\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 532757.9744 - val_loss: 557698.8200\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 532750.7775 - val_loss: 557691.6710\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532743.5873 - val_loss: 557684.4610\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532736.3942 - val_loss: 557677.0550\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 532729.1906 - val_loss: 557669.8510\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 532722.0071 - val_loss: 557662.8320\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 532714.8171 - val_loss: 557655.6930\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 532707.6368 - val_loss: 557648.4835\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 532700.4407 - val_loss: 557641.4630\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532693.2517 - val_loss: 557633.9335\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 532686.0556 - val_loss: 557626.8360\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 532678.8629 - val_loss: 557619.6970\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532671.6769 - val_loss: 557612.5770\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 1s 78us/step - loss: 532664.4811 - val_loss: 557605.4790\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 1s 68us/step - loss: 532657.2927 - val_loss: 557597.9605\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532650.1028 - val_loss: 557590.8545\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 532642.9075 - val_loss: 557583.7340\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532635.7223 - val_loss: 557576.6070\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 532628.5351 - val_loss: 557569.4930\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 532621.3477 - val_loss: 557561.9805\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 532614.1543 - val_loss: 557554.8650\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 2s 126us/step - loss: 473474.6380 - val_loss: 413479.7805\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 311701.9595 - val_loss: 238720.2897\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 177717.3461 - val_loss: 176674.8415\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 165702.1944 - val_loss: 173740.2236\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 163023.6944 - val_loss: 172911.6689\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 161592.3479 - val_loss: 170356.5775\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 1s 67us/step - loss: 160595.3379 - val_loss: 169564.1972\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 1s 62us/step - loss: 159563.0270 - val_loss: 169075.9058\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 1s 61us/step - loss: 158936.9240 - val_loss: 169378.4607\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 158417.0369 - val_loss: 168726.6674\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 158135.0323 - val_loss: 168189.4449\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 157500.0795 - val_loss: 167740.8010\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 1s 62us/step - loss: 157478.7909 - val_loss: 168931.5367\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 157147.1388 - val_loss: 167586.1533\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 156899.1208 - val_loss: 167479.6567\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 156502.6856 - val_loss: 169067.9069\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 156680.5061 - val_loss: 167523.2093\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 156250.6915 - val_loss: 167288.3122\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 155912.9115 - val_loss: 167496.0700\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 1s 71us/step - loss: 155810.9597 - val_loss: 167560.7419\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 155664.5519 - val_loss: 167398.7350\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 155513.0575 - val_loss: 167598.4900\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 1s 61us/step - loss: 155440.0806 - val_loss: 167145.4009\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 155200.4419 - val_loss: 167086.7709\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 155105.6803 - val_loss: 166942.2639\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 155076.0269 - val_loss: 168459.3681\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 154917.6791 - val_loss: 167848.5089\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 154819.9892 - val_loss: 166797.6650\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 154607.6815 - val_loss: 167957.5102\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 154687.5079 - val_loss: 166521.8550\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 154585.8208 - val_loss: 166653.5413\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 154360.5145 - val_loss: 166449.0111\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 154157.0921 - val_loss: 166427.1099\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 154447.5907 - val_loss: 167256.7032\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 153966.0643 - val_loss: 166471.1829\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 153897.3701 - val_loss: 166541.3500\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 153736.3488 - val_loss: 166071.9037\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 154012.2405 - val_loss: 166842.5271\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 153807.5414 - val_loss: 166096.6701\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 1s 65us/step - loss: 153637.4484 - val_loss: 166785.5749\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 73us/step - loss: 153582.2294 - val_loss: 166084.0080\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 1s 62us/step - loss: 153728.3001 - val_loss: 166531.7260\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 153505.1782 - val_loss: 166503.1762\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 153311.5970 - val_loss: 165629.9862\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 153443.5973 - val_loss: 165838.2530\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 153198.0919 - val_loss: 165850.1219\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 153267.9505 - val_loss: 166884.6760\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 153107.3403 - val_loss: 165938.7787\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 153064.2821 - val_loss: 166416.5878\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 153076.0387 - val_loss: 165770.9463\n",
      "BEST ACTIVATION FUNCTION relu WITH SCORE 0.6244128664776986\n"
     ]
    }
   ],
   "source": [
    "best_score = 1000.0 # bad\n",
    "\n",
    "# loop over chosen activation functions, train, evaluate on validation\n",
    "for activ in ['sigmoid', 'tanh', 'relu']:\n",
    "    model = nn_model(activation=activ)\n",
    "\n",
    "    history = model.fit(selected_feature_train, price_train,\n",
    "                epochs=50, batch_size=128,\n",
    "                validation_data=(selected_feature_val, price_val))\n",
    "    model_score = score(model.predict(selected_feature_val), price_val)\n",
    "\n",
    "    if model_score < best_score:\n",
    "        best_score = model_score\n",
    "        best_activ = activ\n",
    "        best_model = model\n",
    "        best_train = history\n",
    "\n",
    "print(f\"BEST ACTIVATION FUNCTION {best_activ} WITH SCORE {best_score}\")\n",
    "best_model.save(\"awesome_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize your training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEWCAYAAACwtjr+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xt83FWd//HXZyaTTJImbdqmUNpiq1QurbVArV1xFcTV4g1cca2XXfSnsj8WV1wvK7i/XURlV38PBZddxUVBUVGseIH1x2WRi4rKpWgtLRVboUJp6b1NmvvMfH5/nDPJJJ0k0yaTNJn38/H4Pr7f7/nezncymc+c8z1zjrk7IiIi5ZQY7wyIiMjkp2AjIiJlp2AjIiJlp2AjIiJlp2AjIiJlp2AjIiJlp2AjMkJmljSzg2Z2/GjuKzKZKNhIxYkf9vkpZ2YdBevvPNzzuXvW3ae4+9Ojue+RMLOTzOwWM9tjZvvNbK2ZfcjM9L8u40pvQKk48cN+irtPAZ4G3liQdtPA/c2sauxzefjMbCHwIPAksNjdpwFvB/4MqDuC802I+5aJQcFGZAAz+4yZfc/MvmtmrcC7zOzPzOzBWFrYbmbXmFkq7l9lZm5m8+P6t+P2O8ys1cx+bWYLDnffuP0cM/uDmR0ws/8ws1+a2bsHyfqngZ+5+z+6+3YAd9/o7m9z94Nm9moz2zLgXrea2ZmD3PdlZtZuZlML9n+Jme3MByIze5+Z/d7M9sV7mDfCl18mKQUbkeLeDHwHmAp8D8gAlwAzgTOAlcDfDnH8O4B/BqYTSk+fPtx9zWwWsBr4WLzuU8DyIc7zauCWoW9rWIX3/XlgDfCXA/K62t0zZnZ+zNu5QDPwUDxW5BAKNiLFPeDu/+3uOXfvcPdH3P0hd8+4+5PAdcArhzj+Fndf4+49wE3A0iPY9w3AWne/NW67Gtg9xHmmA9tLvcFB9LtvQvB4O0B87vM2+gLK3wL/6u5PuHsG+Ayw3MzmjDAPMgkp2IgU90zhSnzw/v/M7DkzawE+RShtDOa5guV2YMoR7HtcYT489Jq7dYjz7AVmD7G9FM8MWP8+8OdmdgxwFtDp7r+K254HfClWLe4nBMIcMHeEeZBJSMFGpLiB3aH/F7AeOMHdG4F/AazMedhOwQe3mRkwVKnhp8BbhtjeRkFDgfjcZcaAffrdt7vvAe4F3kqoQvtuweZngPe6+7SCqdbdHxoiD1KhFGxEStMAHADazOxkhn5eM1p+ApxmZm+MgeESwrORwfwLcKaZ/ZuZHQtgZi80s++Y2RTg90CDmb02Nm64HEiVkI/vABcQnt0UPpP5CvBP8fXAzKbF5zgih1CwESnNRwgfuK2EUs73yn1Bd99BeEZyFbAHeAHwW6BrkP3/QGjm/ELg8Vi1tZrQHLrd3fcBfw/cCDxLqHZ7rti5BvgxcArwtLtvKLje92Pevh+rFtcBrz38O5VKYBo8TWRiMLMksA04391/Md75ETkcKtmIHMXMbKWZTTWzGkLz6Azw8DhnS+SwKdiIHN1eTugRYDfhtz3nuXvRajSRo5mq0UREpOxUshERkbJTR3vRzJkzff78+eOdDRGRCeXRRx/d7e5DNckHFGx6zZ8/nzVr1ox3NkREJhQz+1Mp+6kaTUREyk7BRkREyk7BRkREyk7PbERk0unp6WHr1q10dnaOd1YmjXQ6zdy5c0mlSulO71AKNiIy6WzdupWGhgbmz59P6CxbRsLd2bNnD1u3bmXBggXDH1CEqtFEZNLp7OxkxowZCjSjxMyYMWPGiEqKCjYiMikp0Iyukb6eCjYjdM/GHXz5/s3jnQ0RkaOags0I/WLTbr5y/x/HOxsicpTZv38/X/7ylw/7uNe97nXs37+/DDkaXwo2I9SYrqK1K0Mupw5NRaTPYMEmm80Oedztt9/OtGnTypWtcaPWaCPUWJvCHQ52Z2hMH1mTQBGZfC699FL++Mc/snTpUlKpFFOmTGH27NmsXbuWxx9/nPPOO49nnnmGzs5OLrnkEi688EKgr+usgwcPcs455/Dyl7+cX/3qV8yZM4dbb72V2tracb6zI6NgM0L5ANPS0aNgI3IUuuK/N/D4tpZRPecpxzVy+RsXDbnPZz/7WdavX8/atWu5//77ef3rX8/69et7mw7fcMMNTJ8+nY6ODl7ykpfwlre8hRkzZvQ7x6ZNm/jud7/LV7/6Vf7qr/6KH/zgB7zrXe8a1XsZK6pGG6GGdIjXLR2Zcc6JiBzNli9f3u83Ktdccw0vfvGLWbFiBc888wybNm065JgFCxawdOlSAE4//XS2bNkyVtkddSrZjFBjbSzZdPaMc05EpJjhSiBjpb6+vnf5/vvv56c//Sm//vWvqaur48wzzyz6G5aampre5WQySUdHx5jktRxUshmhwmo0EZG8hoYGWltbi247cOAATU1N1NXV8fvf/54HH3xwjHM39lSyGaHG2liN1qlqNBHpM2PGDM444wwWL15MbW0txxxzTO+2lStX8pWvfIUlS5Zw4oknsmLFinHM6dhQsBkhlWxEZDDf+c53iqbX1NRwxx13FN2Wfy4zc+ZM1q9f35v+0Y9+dNTzN5ZUjTZCvQ0E9MxGRGRQCjYjVJVMUF+dVGs0EZEhKNiMgsbalEo2IiJDKHuwMbOkmf3WzH4S179hZk+Z2do4LY3pZmbXmNlmM1tnZqcVnOMCM9sUpwsK0k83s8fiMddY7JbUzKab2d1x/7vNrKmc99iYTtGqYCMiMqixKNlcAmwckPYxd18ap7Ux7RxgYZwuBK6FEDiAy4GXAsuBywuCx7Vx3/xxK2P6pcA97r4QuCeul01jbZWq0UREhlDWYGNmc4HXA18rYfdzgW968CAwzcxmA68F7nb3ve6+D7gbWBm3Nbr7r93dgW8C5xWc68a4fGNBelk0plWNJiIylHKXbL4I/COQG5B+Zawqu9rM8j+RnQM8U7DP1pg2VPrWIukAx7j7doA4n1Usc2Z2oZmtMbM1u3btOuyby9MzGxEZqSlTpgCwbds2zj///KL7nHnmmaxZs2bI83zxi1+kvb29d/1oGbKgbMHGzN4A7HT3Rwdsugw4CXgJMB34eP6QIqfxI0gvmbtf5+7L3H1Zc3Pz4RzaT2Na1WgiMjqOO+44brnlliM+fmCwOVqGLChnyeYM4E1mtgW4GXiVmX3b3bfHqrIu4OuE5zAQSibzCo6fC2wbJn1ukXSAHbGajTjfOZo3NlBjbWggoDFtRCTv4x//eL/xbD75yU9yxRVXcPbZZ3Paaafxohe9iFtvvfWQ47Zs2cLixYsB6OjoYNWqVSxZsoS3ve1t/fpGu+iii1i2bBmLFi3i8ssvB0Lnntu2beOss87irLPOAsKQBbt37wbgqquuYvHixSxevJgvfvGLvdc7+eSTef/738+iRYt4zWteU5Y+2MrWg4C7X0YoxWBmZwIfdfd3mdlsd98eW46dB+R/Insb8AEzu5nQGOBA3O8u4F8LGgW8BrjM3feaWauZrQAeAv4G+I+Cc10AfDbOD/2LjqLGdIqcQ1t3hgYNMyBydLnjUnjusdE957EvgnM+O+Quq1at4kMf+hB/93d/B8Dq1au58847+Yd/+AcaGxvZvXs3K1as4E1vehOxIe0hrr32Wurq6li3bh3r1q3jtNN6G+ly5ZVXMn36dLLZLGeffTbr1q3jgx/8IFdddRX33XcfM2fO7HeuRx99lK9//es89NBDuDsvfelLeeUrX0lTU9OYDGUwHr+zucnMHgMeA2YCn4nptwNPApuBrwJ/B+Due4FPA4/E6VMxDeAiQuODzcAfgXz/D58F/sLMNgF/EdfLRv2jichAp556Kjt37mTbtm387ne/o6mpidmzZ/OJT3yCJUuW8OpXv5pnn32WHTt2DHqOn//8570f+kuWLGHJkiW921avXs1pp53GqaeeyoYNG3j88ceHzM8DDzzAm9/8Zurr65kyZQp/+Zd/yS9+8QtgbIYyGJO+0dz9fuD+uPyqQfZx4OJBtt0A3FAkfQ2wuEj6HuDsI87wYSrsH23OtIk5ip7IpDVMCaSczj//fG655Raee+45Vq1axU033cSuXbt49NFHSaVSzJ8/v+jQAoWKlXqeeuopPv/5z/PII4/Q1NTEu9/97mHPEz5iixuLoQzUg8Ao6B3TRp1xikiBVatWcfPNN3PLLbdw/vnnc+DAAWbNmkUqleK+++7jT3/605DHv+IVr+Cmm24CYP369axbtw6AlpYW6uvrmTp1Kjt27OjXqedgQxu84hWv4Mc//jHt7e20tbXxox/9iD//8z8fxbsdmnp9HgV9nXGqGk1E+ixatIjW1lbmzJnD7Nmzeec738kb3/hGli1bxtKlSznppJOGPP6iiy7iPe95D0uWLGHp0qUsXx7aU734xS/m1FNPZdGiRTz/+c/njDPO6D3mwgsv5JxzzmH27Nncd999vemnnXYa7373u3vP8b73vY9TTz11zEb/tKGKVpVk2bJlPlz79cFs2d3GmZ+/ny+89cW85fS5wx8gImW1ceNGTj755PHOxqRT7HU1s0fdfdlwx6oabRRoaGgRkaEp2IyC3mo0/bBTRKQoBZtRkEomqKtOqmQjchTRI4LRNdLXU8FmlGiYAZGjRzqdZs+ePQo4o8Td2bNnD+l0+ojPodZoo0TDDIgcPebOncvWrVsZSQe70l86nWbu3CNvAKVgM0o0zIDI0SOVSrFgwYLxzoYUUDXaKNEwAyIig1OwGSUaZkBEZHAKNqNEJRsRkcEp2IySxnSKlo4etX4RESlCwWaUNNZWxTFtsuOdFRGRo46CzSgpHGZARET6U7AZqT/8D/zy39U/mojIEBRsRmrz3fCLLxSUbNQiTURkIAWbkapphK5WGtNJQNVoIiLFKNiMVLoRPEdjshtQNZqISDEKNiNV0wDAVAtjdqtkIyJyKAWbkappBKCedkBDQ4uIFFP2YGNmSTP7rZn9JK4vMLOHzGyTmX3PzKpjek1c3xy3zy84x2Ux/Qkze21B+sqYttnMLi1IL3qNskhPBaA6c5DaVFLDDIiIFDEWJZtLgI0F658Drnb3hcA+4L0x/b3APnc/Abg67oeZnQKsAhYBK4EvxwCWBL4EnAOcArw97jvUNUZfLNnQ2aJhBkREBlHWYGNmc4HXA1+L6wa8Crgl7nIjcF5cPjeuE7efHfc/F7jZ3bvc/SlgM7A8Tpvd/Ul37wZuBs4d5hqjLz6zoatFwwyIiAyi3CWbLwL/COTi+gxgv7vnv/5vBebE5TnAMwBx+4G4f2/6gGMGSx/qGqMvHUs2XS3qjFNEZBBlCzZm9gZgp7s/WphcZFcfZttopRfL44VmtsbM1hzxiH6F1WgaZkBEpKhylmzOAN5kZlsIVVyvIpR0pplZfoTQucC2uLwVmAcQt08F9hamDzhmsPTdQ1yjH3e/zt2Xufuy5ubmI7vL6imAqWQjIjKEsgUbd7/M3ee6+3zCA/573f2dwH3A+XG3C4Bb4/JtcZ24/V4P/fXfBqyKrdUWAAuBh4FHgIWx5Vl1vMZt8ZjBrjH6Eonw3KartXeYARER6W88fmfzceDDZraZ8Hzl+ph+PTAjpn8YuBTA3TcAq4HHgTuBi909G5/JfAC4i9DabXXcd6hrlEdNY19rtM6MxrQRERmgavhdRs7d7wfuj8tPElqSDdynE3jrIMdfCVxZJP124PYi6UWvUTbpxlCNNj1FNue0d2eprxmTl1ZEZEIYsmQTf8/y7bHKzIRV0widBzTMgIjIIIYMNu6eBZrL+gv8yaDgmQ1omAERkYFKqevZAvzSzG4D2vKJ7n5VuTI14aQbYe8faawNL6dKNiIi/ZUSbLbFKQE0lDc7E1S+gYCGhhYRKWrYYOPuVwCYWUNY9YNlz9VEk28goGc2IiJFDdv02cwWm9lvgfXABjN71MwWlT9rE0hNI2S7aajKAnpmIyIyUCm/s7kO+LC7P8/dnwd8BPhqebM1wcQuaxosjGmjYQZERPorJdjUu/t9+ZX4m5n6suVoIoqdcdZk2kinEhpATURkgFIaCDxpZv8MfCuuvwt4qnxZmoBqCnp+Vpc1IiKHKKVk87+AZuCHcZoJvKecmZpw0oUDqKkzThGRgYYs2cTRMD/h7h8co/xMTP0GUJuuBgIiIgOU0oPA6WOUl4mrRiUbEZGhlPLM5rex94Dv078HgR+WLVcTTXpqmMcua7bsbht6fxGRClNKsJkO7CEMfpbnhOc3Av2r0eIwAyIi0qeUZzbr3P3qMcrPxJRMQVVt6Pk5tkZzd8yKjVAtIlJ5Snlm86YxysvEVtBlTSbndPRkxztHIiJHjVKq0X5lZv8JfI/+z2x+U7ZcTUQ1jYcMM1BXrQHURESgtGDzsjj/VEGa0/8ZjqT7hoaG0BnnsVPT45wpEZGjQym9Pp81FhmZ8GoaensQAA0zICJSqJRen48xs+vN7I64foqZvbf8WZtg8mPaaJgBEZFDlNJdzTeAu4Dj4vofgA+VK0MTVjr/zCZWo6kXARGRXqUEm5nuvhrIAbh7BlBTq4FqpvYbQE3DDIiI9Ckl2LSZ2QxCowDMbAVwYLiDzCxtZg+b2e/MbIOZ5Uf8/IaZPWVma+O0NKabmV1jZpvNbJ2ZnVZwrgvMbFOcLihIP93MHovHXGPxhy1mNt3M7o77321mTYf1qhyJmgboPkhDdfhtjX7YKSLSp5Rg82HgNuAFZvZL4JvA35dwXBfwKnd/MbAUWBkDFcDH3H1pnNbGtHOAhXG6ELgWQuAALgdeCiwHLi8IHtfGffPHrYzplwL3uPtC4J64Xl75MW2ybdRUJdRAQESkQCmt0X5jZq8ETgQMeMLdh/0kdXcHDsbVVJx8iEPOBb4Zj3vQzKaZ2WzgTOBud98LYGZ3EwLX/UCju/86pn8TOA+4I57rzHjeG4H7gY8Pl+cR6R3TplWdcYqIDFBKyQZ3z7j7BndfX0qgyTOzpJmtBXYSAsZDcdOVsarsajOriWlzgGcKDt8a04ZK31okHeAYd98e874dmDVI/i40szVmtmbXrl2l3lZxhWPapKvUQEBEpEBJweZIuXvW3ZcCc4HlZrYYuAw4CXgJoZPPfImjWEdifgTph5O/69x9mbsva25uPpxDD9WvM06VbERECpU12OS5+35CVdZKd9/uQRfwdcJzGAglk3kFh80Ftg2TPrdIOsCOWAVHnO8c1RsqpiYOM9CpoaFFRAYaNNiY2WlDTcOd2MyazWxaXK4FXg38viAIGOEZy/p4yG3A38RWaSuAA7EK7C7gNWbWFBsGvAa4K25rNbMV8Vx/A9xacK58q7ULCtLLJz3wmY2q0URE8oZqIPCFOE8Dy4DfEaqulgAPAS8f5tyzgRvjMAUJYLW7/8TM7jWz5niutcD/jvvfDrwO2Ay0A+8BcPe9ZvZp4JG436fyjQWAiwg/Oq0lNAy4I6Z/Flgdezp4GnjrMHkdud4GAgfiMxuVbERE8gYNNvk+0czsZuBCd38sri8GPjrcid19HXBqkfSiHXjGVmgXD7LtBuCGIulrgMVF0vcAZw+Xx1GVf2ZTMDS0xrQREQlKeWZzUj7QALj7esLvZqRQqhYSVb2dcfZknc6e3HjnSkTkqFDKEAMbzexrwLcJrb3eBWwsa64mIrO+MW0a+oYZqK1OjnPGRETGXyklm/cAG4BLCB1wPh7TZKD8mDYaZkBEpJ9SehDoNLOvALe7+xNjkKeJKz+mjYYZEBHpp5TxbN5EaDV2Z1xfama3lTtjE1LN1N4eBEDDDIiI5JVSjXY54YeX+wFix5nzy5iniSs/po1KNiIi/ZQSbDLuPuyQAkJsIHCg75mNftgpIgKU1hptvZm9A0ia2ULgg8CvyputCaqmATpbaOitRlPJRkQESivZ/D2wiDA+zXcIA6dpWOhiYjVauipBdVVC1WgiItGQJZvY1cwV7v4x4J/GJksTWE0jeBZ62mNnnKpGExGBYUo27p4FTh+jvEx8hWPa1FapZCMiEpXyzOa3sanz94G2fKK7/7BsuZqoejvj1DADIiKFSgk204E9QGEHmg4o2AxUU1iySXFAwUZEBCitBwF1TVOqdGHJpomte9vHNz8iIkeJYYONmaWB9xJapKXz6e7+v8qYr4mpsBqtdpae2YiIRKU0ff4WcCzwWuBnhOGXW8uZqQmrsIFAbI0WhukREalspQSbE9z9n4E2d78ReD3wovJma4LKD6DW1UpjbRXd2RxdGY1pIyJSSrDJ1wXtj6N0TkV9oxVX3QBYb2s0QI0EREQoLdhcZ2ZNwD8DtxHGs/m/Zc3VRJVI9HZZM7VWwUZEJK+U1mhfi4s/A55f3uxMAjWN0NVCU101APvausc5QyIi46+U1mj/Uizd3T81+tmZBOIAatPqQslmX7tKNiIipfyos61gOQ28AdhYnuxMAnFo6On1oWSzv10lGxGRYZ/ZuPsXCqYrgTOBOcMdZ2ZpM3vYzH5nZhvM7IqYvsDMHjKzTWb2PTOrjuk1cX1z3D6/4FyXxfQnzOy1BekrY9pmM7u0IL3oNcbEgGq0vQo2IiIlNRAYqI7Snt10Aa9y9xcDS4GVZrYC+BxwtbsvBPYRfjBKnO9z9xOAq+N+mNkpwCrCj0pXAl82s2TskfpLwDnAKcDb474McY3yiyWb2uokNVUJ9qsaTURk+GBjZo+Z2bo4bQCeAP59uOM8OBhXU3FyQh9rt8T0G4Hz4vK5cZ24/Wwzs5h+s7t3uftTwGbCMNXLgc3u/qS7dwM3A+fGYwa7RvnVNEBX+M1rU121GgiIiFDaM5s3FCxngB3uXtJALbH08ShwAqEU8kdgf8HxW+mrkpsDPAPg7hkzOwDMiOkPFpy28JhnBqS/NB4z2DUG5u9C4EKA448/vpRbGl6sRgOYVpdSAwEREUqrRmstmDqARjObnp+GOtDds+6+lNDFzXLg5GK7xbkNsm200ovl7zp3X+buy5qbm4vtcvjSjZDphEw30+ur1UBARITSSja/AeYRnn0YMA14Om5zSnh+4+77zex+YAUwzcyqYsljLrAt7rY1XmermVUReirYW5CeV3hMsfTdQ1yj/GqmhnlsJLDxuZYxu7SIyNGqlJLNncAb3X2mu88gVKv90N0XuPuggcbMms1sWlyuBV5NaDJ9H3B+3O0C4Na4fFtcJ26/10MvlrcBq2JrtQXAQuBh4BFgYWx5Vk1oRHBbPGawa5Rfb/9o4bc2aiAgIlJasHmJu9+eX3H3O4BXlnDcbOA+M1tHCAx3u/tPgI8DHzazzYTnK9fH/a8HZsT0DwOXxuttAFYTusm5E7g4Vs9lgA8AdxGC2Oq4L0Nco/wKen5uqgvVaLmcen4WkcpWSjXabjP7P8C3CdVm7yKM3Dkkd18HnFok/UnC85uB6Z3AWwc515XAlUXSbwduL5Je9BpjomBMm6b6qeQcWjszTI09CoiIVKJSSjZvB5qBHwE/jstvL2emJrR+JZsQYPTDThGpdKV0xLkXuAR6mzLXu7ueeg+mYEyb3s4427tZQP04ZkpEZHyV8qPO75hZo5nVAxuAJ8zsY+XP2gRV0Bot3xmnmj+LSKUrpRrtlFiSOY/wfOR44K/LmquJrKAaLd8Z5742tUgTkcpWSrBJmVmKEGxudfceBvmRpADJFFTVQtcBphVUo4mIVLJSgs1/AVuAeuDnZvY8QM9shhL7R2tMV5FMmIKNiFS8UoYYuMbd57j76+IPJp8Gzip/1iaw2POzmTGtVv2jiYiU8jubfmLAKakjzopV0Blnk/pHExE5ovFsZDixZAPQVJdSAwERqXgKNuVQMKbNtLpqPbMRkYpXUjWamb0MmF+4v7t/s0x5mvhqpvZVo9WlWLdVwUZEKtuwwcbMvgW8AFgLZGOyAwo2g+lXjVbNvvYe3J0wiKiISOUppWSzjPDDTv22plQ1jdDdCrksTfXVdGdydPRkqas+7PYYIiKTQinPbNYDx5Y7I5NKvn+07oN9nXG2qSpNRCpXKV+1ZwKPm9nDQFc+0d3fVLZcTXQFXdbkexHY397D3KZxzJOIyDgqJdh8styZmHQKx7Spmw2oyxoRqWylDDHws7HIyKTSrzPO4wHUi4CIVLRShhhYYWaPmNlBM+s2s6yZqW+0ofSWbFr7OuPUMxsRqWClNBD4T8LInJuAWuB9MU0GU1CNNq02NBBQNZqIVLKS2uK6+2YzS7p7Fvi6mf2qzPma2Hqr0Q5QlUzQkK5iv6rRRKSClRJs2s2sGlhrZv8X2A4a43hIBdVoANPr1WWNiFS2UqrR/jru9wGgDZgHvKWcmZrwUrWQqOrtsmZa7EVARKRSlTKezZ8AA2a7+xXu/mF33zzccWY2z8zuM7ONZrbBzC6J6Z80s2fNbG2cXldwzGVmttnMnjCz1xakr4xpm83s0oL0BWb2kJltMrPvxRIYZlYT1zfH7fMP50UZMbPww85+PT+rZCMilauU1mhvJPSLdmdcX2pmt5Vw7gzwEXc/GVgBXGxmp8RtV7v70jjdHs97CrAKWASsBL5sZkkzSwJfAs4BTgHeXnCez8VzLQT2Ae+N6e8F9rn7CcDVcb+xVTimjXp+FpEKV0o12ieB5cB+AHdfS+gBekjuvt3dfxOXW4GNwJwhDjkXuNndu9z9KWBzvO5yYLO7P+nu3cDNwLkWerV8FXBLPP5G4LyCc90Yl28Bzrax7gUz3dj7zKaprloNBESkopUSbDLufmAkF4nVWKcCD8WkD5jZOjO7wczynbjMAZ4pOGxrTBssfQaw390zA9L7nStuPxD3H5ivC81sjZmt2bVr10hu8VA1U/tVox3sytCdyY3uNUREJoiSOuI0s3cASTNbaGb/AZTc9NnMpgA/AD7k7i3AtYQhC5YSWrZ9Ib9rkcP9CNKHOlf/BPfr3H2Zuy9rbm4e8j4OW00DdIUYPa0+3z+aqtJEpDKVEmz+nvAcpQv4LtACfKiUk5tZihBobnL3HwK4+w53z7p7DvgqoZoMQslkXsHhc4FtQ6TvBqaZWdWA9H7nitunAntLyfOoGTA0NKjLGhGpXKW0Rmt3939y95fEUsA/uXvncMfFZyTXAxvd/aqC9NkFu72ZMIQBwG3AqtiSbAGwEHgYeARYGFvNHvRzAAAXi0lEQVSeVRMaEdwWx9e5Dzg/Hn8BcGvBuS6Iy+cD9475eDw1/Z/ZgHoREJHKNeiPOodrcVbCEANnEH6j85iZrY1pnyC0JltKqNbaAvxtPN8GM1sNPE5oyXZx7LEAM/sAcBeQBG5w9w3xfB8HbjazzwC/JQQ34vxbZraZUKJZNUxeR186tkZz7w02qkYTkUo1VA8Cf0Z4yP5dwoP9w2rN5e4PDHLM7UMccyVwZZH024sd5+5P0lcNV5jeCbz1cPI76moaIJeBng6a6vMDqKkaTUQq01DB5ljgLwidcL4D+H/AdwtKFTKUfmPahMYHqkYTkUo16DOb+BD/Tne/gPCjzM3A/Wb292OWu4lsyjFh3vIs6VSSdCqhajQRqVhDdsRpZjXA6wmlm/nANcAPy5+tSaD5pDDf9QTMOZ3p6h9NRCrYUA0EbgQWA3cAV7j7+sH2lSKa5kOyOgQbQmecKtmISKUaqmTz14Renl8IfLCgtxcD3N0by5y3iS1ZBTNO6A02TfUp9qozThGpUIMGG3cv5QefMpSZL4Tn1gGhZLN9v0bTFpHKpIBSTs0nwb4t0NMZn9moZCMilUnBppyaXwiegz2baapLcaCjh2xubDsyEBE5GijYlFNvi7TfM62umpxDS4dapIlI5VGwKacZJ4AlYPcfensRUFWaiFQiBZtyqqqBpgWw6/cFnXGqZCMilUfBptyaT4Rdf1BnnCJS0RRsyq35xNBAIB1eav3WRkQqkYJNuTWfBLkepndvBWC/qtFEpAIp2JTbzBcCUH9gM1UJUwMBEalICjblFoON7X6CaeqMU0QqlIJNudVMganzYiOBFPv0zEZEKpCCzVhoPrG3+bOq0USkEinYjIWZJ8LuTTTVJtVAQEQqkoLNWGg+ETIdPL96r0o2IlKRFGzGQvOJACzgWfa39+CuzjhFpLIo2IyF2CJtXuZpurM52rqz45whEZGxVbZgY2bzzOw+M9toZhvM7JKYPt3M7jazTXHeFNPNzK4xs81mts7MTis41wVx/01mdkFB+ulm9lg85hqLw4kOdo1xUzcd6mdxTNefANQiTUQqTjlLNhngI+5+MrACuNjMTgEuBe5x94XAPXEd4BxgYZwuBK6FEDiAy4GXAsuBywuCx7Vx3/xxK2P6YNcYP80nMr3jKUC9CIhI5SlbsHH37e7+m7jcCmwE5gDnAjfG3W4EzovL5wLf9OBBYJqZzQZeC9zt7nvdfR9wN7Aybmt09197eAjyzQHnKnaN8dN8IlNa/gi4GgmISMUZk2c2ZjYfOBV4CDjG3bdDCEjArLjbHOCZgsO2xrSh0rcWSWeIawzM14VmtsbM1uzatetIb680zSdR1dPKLPYr2IhIxSl7sDGzKcAPgA+5e8tQuxZJ8yNIL5m7X+fuy9x9WXNz8+Ecevhii7QTEs/qmY2IVJyyBhszSxECzU3u/sOYvCNWgRHnO2P6VmBeweFzgW3DpM8tkj7UNcbPzBBsXmjPqn80Eak45WyNZsD1wEZ3v6pg021AvkXZBcCtBel/E1ulrQAOxCqwu4DXmFlTbBjwGuCuuK3VzFbEa/3NgHMVu8b4mTIL0tM4ObVdA6iJSMWpKuO5zwD+GnjMzNbGtE8AnwVWm9l7gaeBt8ZttwOvAzYD7cB7ANx9r5l9Gngk7vcpd98bly8CvgHUAnfEiSGuMX7MoPlETnz2WR5QyUZEKkzZgo27P0Dx5yoAZxfZ34GLBznXDcANRdLXAIuLpO8pdo1x13wi87feqpKNiFQc9SAwlmaeyDQ/QPZgmVu+iYgcZRRsxlLzSQBMa9syvvkQERljCjZjqTn0kTarc8v45kNEZIwp2Iylxrl0J2o5PvcMXRl1xikilUPBZiwlErROWcAJ9qz6RxORiqJgM8Y6pi0MvQioRZqIVBAFmzGWnb6Q42wvLfv2Dr+ziMgkoWAzxqqOPRmAPY/fCxqxU0QqhILNGJt94kvJUMU5j/0DnZ9fBD/5MDxxJ3S3j3fWRETKppzd1UgRiaZ5tF38W756/VdY2vYwZ/7uuyTWXA/JGljw5zDrFKifCXUzob4Z6meE5YbZUFU93tkXETkiCjbjoKH5eM5////hvC//kuZq+OGbof7pe2HzT+GpX0C269CDUnUw76Uw/+Ww4BVw3KmQTI195kVEjoC5nhsAsGzZMl+zZs2YXvPhp/byzq89yPIF0/nGe5aTSibCc5zug9C2G9r3QNuusPzcY7DlF7Dz8XBw9RQ4fgXMfUkoAdXN6JvqZ0Jtk4KRiJSdmT3q7suG208lm3G0fMF0/vXNL+Jjt6zjiv/ewKfPXYyZQU1DmKYvOPSgtt0h6Dz1izDf/NPBL1CVhur6EJiqp0DNlFBC8ixkuiDTCZnuOO8K1XTpaZCeCrVxnl/P56mmIZ6rMZyvuj6cs7oektWhd2sI5215Fg5shQPPhHnLtnDepvnQtCDMG+dAcpi3oXs4164nYPcm2P0HaNsZjp/5QpixEGYuDIHWBuv7dYLraoU9m+HAs1BdBzVTId0Y/zaNkEof3vnyXzIn6+slRx0Fm3H21mXz2LzrIP/1sydZOKuBC142f+gD6mfCojeHCcKHesfeWAqKpaH2PdCxL3xAdbeFklJ3W1w/CIkqSNWGQFJVE4JSVXUIOJ0HoGM/tG7vWy5WrVdMogpS9aFE1b6HQwZOrZsRzpnL9D9m6rxQErMEJJJgyThPhP13b4Ketr5j0lOhfhb84X/65y09DWa8IATDZHWcUn1zz0G2B3I9YZ5fzmXDh6/nQp49F5ct5C+RjPOqcB5L9D9PLtM37w3wBVOqru84LMzN4vkTfee2ZN/1ejpCcNmzOdz/weeGfu2TNTDlGGg4FhpnQ8Nxcfm48GXiwNYQqFq29i17Lnyhmf78MM14QZg3zomvRzbcUy4bl3Mhb/nXM1HVt2z5tkYxeOWDWL6U3rYb2gvm2Z5YEp/ev1Re0xjPU/B3yAfGwi9PA7+g5HLQuR/a4/9CR/xpQe30cI3a6eGLTiLZd0xPR3h/d+wLU097uH5tU5ymHVo74B72624P78lctuB+C+49VReuOdwXqcOR6Q7vg9bnwv9noir8zafMCv8Pg33hcA/vgWw3VDeE99w4UDVaNB7VaHnZnPO333qUe3+/gw+evZCzTpzF4jlTSSaOkm+dPZ3hQ6OrtW/Kr3e3xX++g/EfsD0ErYZjYercEEimzg0fYKl0+OdseRb2bQnT3qfCvKs1fqBl+z5kctnwLX7mC/um5hNDtaFZ2L7/6fiB/IfwobzvqZDfbHcMKN19y5YI//zJakikwnIifmjmA4BZX1DAwzVy+Q/dGFByub5jez904zzTGQN8e1+g72kP189/gJaqtqmv1DbjBWF52ry+LwUDp4M7QumxdTu0bO8foLEYfObEv0sc5HbvU7D3yfC6ZTpH6Q0zjGRN+Bt0t47sHPlg3tMegsWwr62FLypV6RCYSrnf6inhmGx33/u75NHnLQS6+ua+Kd3Y9+Vk4BeW3s/igvN7Dtr2hL9p++6hL5eeGoJPqja+/w72vQd7XxuLAXVq/5qLV3w0PAc+AqVWoynYROMZbADaujJc+K01/HLzHgAa01W87AUzOWPhTF5+wkzmz6gLVWwy8bnHDxYvKDVk+gJZLhOCV930kV2nsyV8SFWlh2/NmMtB67YQeFqfO7SUmQ/IvR+U3f2XcQ753Zh7+LJQNzO2sIzPE6unhKCe6Q5BIl8ab98TgmY+4BdO7pDpiB+eBaX17rbw4ZovGdXmS0pNIQ/t+2LJf2/fPNsVPmTzpZfaprBeXQ9dLf1LOx37Q2BKVvevMs4vJ6oG3Hucd7fFEt3OvueubbvC36TwC0qyuqBkW1Di6P1ft3A/DceGv2G+tDrlmPC+ObgzfMk4uKNvuacjVp3X981rpoRrdLXGezoQ7itfe/HGf4fjX3pEbzMFm8M03sEmb/fBLn71xz08sGkXD2zazbYD4dtXU12K+TPrmT8jTjPrmD+jnnnT62iqSykQici4ULA5TEdLsCnk7mzZ084Dm3fz+LYW/rSnjS2723oDUF51MkFzQw2zGmuY1VDDMY1pZjXUMKsxzbGNaY5pTHNMYw1TaxWURGR0qTXaJGBmLJhZz4KZ9f3SO3uyPLO3nad2t7F1Xwc7W7vY2dLJztYuntzVxoNP7uVAx6G9StdUJZjVWENTXTWN6RSNtVU01IR5YzrFlHQV9TVVTIlTfrmuOkl1VYLqZIKaVJhXJdX5hIiUTsFmAkqnkiw8poGFxzQMuk9nT5adLV3saO1kR0snzx0IwWhHSycHOnpo6ejhuZZOWjp6aOnsobPnMB5cAwmD2lSSxtpUb+AK8xRTaqoOadyQL1BVVyWoqUqSTvXN01UhmKWSCaqrjOpkklTSSFUlqKlKUJtKUludpDaVJJ1KUlOV6C2huTvZnJOJUzbn1MTjVIoTOXoo2ExS6VSS42fUcfyMupL278pkaevK0taV4WBXhrauDK1x3tGdpTubo6snR3c2R3cmTO3dWVo7Q7Bq6cjwXEsnf9jZSmtnJj4DD1W0vRW1Dl3x+JEwC1WH+SBTTMKgrrqK2uokdQWBKp1K9AasdCpJuipJqspIWN+UTBCWE0ZVwkjGed96ojc9WbA9meg7h1nIg8X1qqRRnQwBNZW0GFgT4fj8tfPXNSMRW9Ia4VzWe19GMlmYr0TvdUSOZgo2AkBNVZKaqiTT68vf/1ou573BqzOTpbMnS082R1cmR0/W6ckHtPw+PVk6erJ0dId5Z0+W7kyu90O+Khk+tFPJ8EHdlcnR3p2hvTsc0x6nrkyWrp4ce9u66ezJ0pUJ585knaw7uZyTc8jF0lI2F9InwmPN3qAVA1YyH/BiQMoHuHzQq0qGfXIemt7n4n3m3HFCoOwLviHQJowQJGPArBqwnErmr5VPC+fvzuToymTjvO/LRl11FfU1yTCvTlJfE74cFAbpRGyNns/HwGBflTAyOe99v+S/CHVlwzVSif55q0pa33LBe6fwy0L+evkvDGDk3Mlkw+sUStA5sjl6X5PCLxNV+dc6MfB1McysX2k8/17LOX1/o4RNyi8PZQs2ZnYD8AZgp7svjmmfBN4P7Iq7fcLdb4/bLgPeC2SBD7r7XTF9JfDvQBL4mrt/NqYvAG4GpgO/Af7a3bvNrAb4JnA6sAd4m7tvKdd9yuFLJIx0IpQ0pnL0d6mTi0EnH4DywSmTy/UFpZzTkw3NmfMBK5cDJ8wzuYJAms3REwNrJpeLHzj5Y/qOd4D8PCz2C4SZgnkmmyPnfdWKvXmIH449mVz4UM7myGTDtbM5LyiN0Vu6Cvfcd3zh+XqyOTp6srR0hg/1nniuTDZHT8xHJhuvk3MSFr7IVMeqzfyzP4D27ixt3Rnau0LJuRKYlTaySD6Ip6rCa5XLhS8DTt/fZWAATiWt39/TzIixEqN/6Xdgw7B/+8slLF8wwqb2wyhnyeYbwH8SPvgLXe3uny9MMLNTgFXAIuA44Kdm9sK4+UvAXwBbgUfM7DZ3fxz4XDzXzWb2FUKgujbO97n7CWa2Ku73tnLcoFSGRMJIYKSSw+8rR6Y7kwul0J5Mb9D03oDZF4gz/QJtCHRVCQtBLAay/DJAJhu+HPTkQhAMwTEXA2f/YJ3Jf6DHAF9Y2qtKJEgm6C1VJWJJ0smXxD1+ocjRk+l/vfz5e7J9QaJ/tWwIAuELS47ufD5jaS0fKKy3tElvCSkTv/zkX4/865MPTN77pSV88TEKSkwFi/U15X9zly3YuPvPzWx+ibufC9zs7l3AU2a2GVget2129ycBzOxm4Fwz2wi8CnhH3OdG4JOEYHNuXAa4BfhPMzNXG2+Ro1Y+QEyEkq4cmfFov/oBM1tnZjeYWfyZL3OAZwr22RrTBkufAex398yA9H7nitsPxP0PYWYXmtkaM1uza9euYruIiMgoGOtgcy3wAmApsB34Qkwv9jTMjyB9qHMdmuh+nbsvc/dlzc3NQ+VbRERGYEyDjbvvcPesu+eAr9JXVbYVmFew61xg2xDpu4FpZlY1IL3fueL2qcDe0b8bEREp1ZgGGzObXbD6ZmB9XL4NWGVmNbGV2ULgYeARYKGZLTCzakIjgtvi85f7gPPj8RcAtxac64K4fD5wr57XiIiMr3I2ff4ucCYw08y2ApcDZ5rZUkK11hbgbwHcfYOZrQYeBzLAxe6ejef5AHAXoenzDe6+IV7i48DNZvYZ4LfA9TH9euBbsZHBXkKAEhGRcaSOOKOjsSNOEZGjXakdcao3RRERKTsFGxERKTtVo0Vmtgv40xEePpPQQq7S6L4rT6Xeu+57cM9z92F/O6JgMwrMbE0pdZaTje678lTqveu+R07VaCIiUnYKNiIiUnYKNqPjuvHOwDjRfVeeSr133fcI6ZmNiIiUnUo2IiJSdgo2IiJSdgo2I2RmK83sCTPbbGaXjnd+yiWOP7TTzNYXpE03s7vNbFOcNw11jonIzOaZ2X1mttHMNpjZJTF9Ut+7maXN7GEz+1287yti+gIzeyje9/diB7mTjpklzey3ZvaTuD7p79vMtpjZY2a21szWxLRRe58r2IyAmSUJw1afA5wCvD0OcT0ZfQNYOSDtUuAed18I3BPXJ5sM8BF3PxlYAVwc/8aT/d67gFe5+4sJ40+tNLMV9A3HvhDYRxiGfTK6BNhYsF4p932Wuy8t+G3NqL3PFWxGZjlx2Gp37wZuJgxLPem4+885dFygcwlDchPn541ppsaAu29399/E5VbCB9AcJvm9e3Awrqbi5ITh2G+J6ZPuvgHMbC7weuBrcd2ogPsexKi9zxVsRmawYasrxTHuvh3ChzIwa5zzU1ZmNh84FXiICrj3WJW0FtgJ3A38kcGHY59Mvgj8I5CL60MNQz+ZOPA/ZvaomV0Y00btfV628WwqRMlDUMvEZmZTgB8AH3L3lvBld3KLY0otNbNpwI+Ak4vtNra5Ki8zewOw090fNbMz88lFdp1U9x2d4e7bzGwWcLeZ/X40T66SzcgMNmx1pdiRH301zneOc37KwsxShEBzk7v/MCZXxL0DuPt+4H7CM6vBhmOfLM4A3mRmWwjV4q8ilHQm+33j7tvifCfhy8VyRvF9rmAzMkWHrR7nPI2lwiG4C4fmnjRiff31wEZ3v6pg06S+dzNrjiUazKwWeDXhedVgw7FPCu5+mbvPdff5hP/ne939nUzy+zazejNryC8DrwHWM4rvc/UgMEJm9jrCN5/8sNVXjnOWyqJwmG9gB2GY7x8Dq4HjgaeBt7r7wEYEE5qZvRz4BfAYfXX4nyA8t5m0925mSwgPhJOEL6Wr3f1TZvZ8wjf+6YTh2N/l7l3jl9PyidVoH3X3N0z2+47396O4WgV8x92vNLMZjNL7XMFGRETKTtVoIiJSdgo2IiJSdgo2IiJSdgo2IiJSdgo2IiJSdgo2ImVmZtnYk25+GrVOO81sfmFP3CJHK3VXI1J+He6+dLwzITKeVLIRGSdx/JDPxXFjHjazE2L688zsHjNbF+fHx/RjzOxHcYyZ35nZy+Kpkmb21TjuzP/EX/xjZh80s8fjeW4ep9sUARRsRMZC7YBqtLcVbGtx9+XAfxJ6oiAuf9PdlwA3AdfE9GuAn8UxZk4DNsT0hcCX3H0RsB94S0y/FDg1nud/l+vmREqhHgREyszMDrr7lCLpWwgDlD0ZO/t8zt1nmNluYLa798T07e4+08x2AXMLu0mJwx7cHQe3wsw+DqTc/TNmdidwkNCt0I8LxqcRGXMq2YiMLx9kebB9iinsoytL37PY1xNGkj0deLSg12KRMadgIzK+3lYw/3Vc/hWhx2GAdwIPxOV7gIugd2CzxsFOamYJYJ6730cYCGwacEjpSmSs6JuOSPnVxhEv8+5093zz5xoze4jwxe/tMe2DwA1m9jFgF/CemH4JcJ2ZvZdQgrkI2D7INZPAt81sKmHwr6vjuDQi40LPbETGSXxms8zdd493XkTKTdVoIiJSdirZiIhI2alkIyIiZadgIyIiZadgIyIiZadgIyIiZadgIyIiZff/ATU2sFk3ZuC0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot loss during training\n",
    "def plot_loss(hist):\n",
    "    %matplotlib inline\n",
    "    plt.title('Training Curve')\n",
    "    plt.plot(hist.history['loss'], label='train')\n",
    "    plt.plot(hist.history['val_loss'], label='validation')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Mean squared error\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(best_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Standardize your features:\n",
    "* Typically assumes normally distributed feature, shifting mean to 0 and standard deviation to 1\n",
    "* In theory does not matter for neural networks\n",
    "* In practice tends to matter for neural networks\n",
    "* Scale if using:\n",
    "    - Logistic regression\n",
    "    - Support vector machines\n",
    "    - Perceptrons\n",
    "    - Neural networks\n",
    "    - Principle component analysis\n",
    "* Don't bother if using:\n",
    "    - \"Forest\" methods\n",
    "    - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.158106372019069e-17\n",
      "1.0\n",
      "0.016119707489855875\n",
      "0.9970087526331801\n",
      "0.09330835574898895\n",
      "0.9414925643047213\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate StandardScaler\n",
    "in_scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler to the training set and perform the transformation\n",
    "selected_feature_train = in_scaler.fit_transform(selected_feature_train)\n",
    "\n",
    "# Use the fitted scaler to transform validation and test features\n",
    "selected_feature_val = in_scaler.transform(selected_feature_val)\n",
    "selected_feature_test = in_scaler.transform(selected_feature_test)\n",
    "\n",
    "# Check appropriate scaling\n",
    "print(np.mean(selected_feature_train[:,0]))\n",
    "print(np.std(selected_feature_train[:,0]))\n",
    "\n",
    "print(np.mean(selected_feature_val[:,0]))\n",
    "print(np.std(selected_feature_val[:,0]))\n",
    "\n",
    "print(np.mean(selected_feature_test[:,0]))\n",
    "print(np.std(selected_feature_test[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "18000/18000 [==============================] - 3s 139us/step - loss: 416643937206.2720 - val_loss: 456228442472.4481\n",
      "Epoch 2/100\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 416174375756.6862 - val_loss: 455081836937.2160\n",
      "Epoch 3/100\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 414203507230.4924 - val_loss: 451577699893.2479\n",
      "Epoch 4/100\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 409586504768.6258 - val_loss: 444620050268.1600\n",
      "Epoch 5/100\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 401539772274.0053 - val_loss: 433381241782.2720\n",
      "Epoch 6/100\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 389566765349.0916 - val_loss: 417698372124.6720\n",
      "Epoch 7/100\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 373364203949.6249 - val_loss: 397285734744.0640\n",
      "Epoch 8/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 353101567229.9520 - val_loss: 372417820622.8480\n",
      "Epoch 9/100\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 329312872839.3956 - val_loss: 344258190508.0320\n",
      "Epoch 10/100\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 302557358893.7386 - val_loss: 313068861259.7760\n",
      "Epoch 11/100\n",
      "18000/18000 [==============================] - 1s 60us/step - loss: 274020907148.1742 - val_loss: 280811497062.4000\n",
      "Epoch 12/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 244866139961.5715 - val_loss: 248419530571.7760\n",
      "Epoch 13/100\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 216229962099.3707 - val_loss: 217221567741.9520\n",
      "Epoch 14/100\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 189302895877.2338 - val_loss: 188751049785.3440\n",
      "Epoch 15/100\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 165017417722.5387 - val_loss: 163427171696.6400\n",
      "Epoch 16/100\n",
      "18000/18000 [==============================] - 1s 69us/step - loss: 143970203962.9369 - val_loss: 142328171462.6560\n",
      "Epoch 17/100\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 126420409676.2311 - val_loss: 125076506542.0800\n",
      "Epoch 18/100\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 112366287344.9813 - val_loss: 111713749696.5120\n",
      "Epoch 19/100\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 101538687607.6942 - val_loss: 101760894042.1120\n",
      "Epoch 20/100\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 93451293491.2000 - val_loss: 94405932482.5600\n",
      "Epoch 21/100\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 87541428970.3822 - val_loss: 89203500056.5760\n",
      "Epoch 22/100\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 83292959379.0009 - val_loss: 85525006974.9760\n",
      "Epoch 23/100\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 80174258847.7440 - val_loss: 82767259238.4000\n",
      "Epoch 24/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 77821882255.1324 - val_loss: 80662439133.1840\n",
      "Epoch 25/100\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 75947373767.3387 - val_loss: 78957149814.7840\n",
      "Epoch 26/100\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 74406805620.5085 - val_loss: 77520016244.7360\n",
      "Epoch 27/100\n",
      "18000/18000 [==============================] - 1s 59us/step - loss: 73080771670.4711 - val_loss: 76261211439.1040\n",
      "Epoch 28/100\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 71892029035.8613 - val_loss: 75108657463.2960\n",
      "Epoch 29/100\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 70810310388.3947 - val_loss: 74050766241.7920\n",
      "Epoch 30/100\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 69810142336.3413 - val_loss: 73068397002.7520\n",
      "Epoch 31/100\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 68863906523.8187 - val_loss: 72136109359.1040\n",
      "Epoch 32/100\n",
      "18000/18000 [==============================] - 1s 64us/step - loss: 67964299021.8809 - val_loss: 71227444297.7280\n",
      "Epoch 33/100\n",
      "18000/18000 [==============================] - 1s 62us/step - loss: 67089260827.9893 - val_loss: 70354857590.7840\n",
      "Epoch 34/100\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 66242910268.0747 - val_loss: 69508503470.0800\n",
      "Epoch 35/100\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 65416282341.3760 - val_loss: 68692503461.8880\n",
      "Epoch 36/100\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 64603796883.2284 - val_loss: 67874351972.3520\n",
      "Epoch 37/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 63798609000.6756 - val_loss: 67077668798.4640\n",
      "Epoch 38/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 63007733794.5884 - val_loss: 66283447713.7920\n",
      "Epoch 39/100\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 62218075770.4249 - val_loss: 65515756945.4080\n",
      "Epoch 40/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 61427563153.1804 - val_loss: 64733354196.9920\n",
      "Epoch 41/100\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 60632534679.5520 - val_loss: 63957731180.5440\n",
      "Epoch 42/100\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 59852953536.2844 - val_loss: 63183502737.4080\n",
      "Epoch 43/100\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 59067614487.4382 - val_loss: 62410280960.0000\n",
      "Epoch 44/100\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 58286037853.0702 - val_loss: 61636835999.7440\n",
      "Epoch 45/100\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 57501479246.9618 - val_loss: 60864916029.4400\n",
      "Epoch 46/100\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 56718312192.2276 - val_loss: 60109806469.1200\n",
      "Epoch 47/100\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 55948789059.1289 - val_loss: 59336187281.4080\n",
      "Epoch 48/100\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 55172900651.0080 - val_loss: 58568358428.6720\n",
      "Epoch 49/100\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 54380224027.7618 - val_loss: 57797157388.2880\n",
      "Epoch 50/100\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 53590874091.9751 - val_loss: 57050233077.7600\n",
      "Epoch 51/100\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 52817828868.5511 - val_loss: 56300603572.2240\n",
      "Epoch 52/100\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 52044515277.0276 - val_loss: 55553473347.5840\n",
      "Epoch 53/100\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 51270324475.2213 - val_loss: 54820043358.2080\n",
      "Epoch 54/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 50511495637.6747 - val_loss: 54087951384.5760\n",
      "Epoch 55/100\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 49745678594.5031 - val_loss: 53363701972.9920\n",
      "Epoch 56/100\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 49005519130.1689 - val_loss: 52669399629.8240\n",
      "Epoch 57/100\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 48268064605.9804 - val_loss: 51976019705.8560\n",
      "Epoch 58/100\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 47540415834.7947 - val_loss: 51323064811.5200\n",
      "Epoch 59/100\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 46841074206.4924 - val_loss: 50662825721.8560\n",
      "Epoch 60/100\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 46144753238.0160 - val_loss: 50033851858.9440\n",
      "Epoch 61/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 45475056225.8489 - val_loss: 49427176718.3360\n",
      "Epoch 62/100\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 44828079645.5822 - val_loss: 48837293277.1840\n",
      "Epoch 63/100\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 44200363753.4720 - val_loss: 48283644362.7520\n",
      "Epoch 64/100\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 43594410324.4231 - val_loss: 47741760372.7360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100\n",
      "18000/18000 [==============================] - 1s 64us/step - loss: 43003461207.8364 - val_loss: 47218661457.9200\n",
      "Epoch 66/100\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 42454042512.9529 - val_loss: 46734514487.2960\n",
      "Epoch 67/100\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 41924461709.0844 - val_loss: 46282613555.2000\n",
      "Epoch 68/100\n",
      "18000/18000 [==============================] - 1s 75us/step - loss: 41436036686.7342 - val_loss: 45856274087.9360\n",
      "Epoch 69/100\n",
      "18000/18000 [==============================] - 1s 62us/step - loss: 40972497791.6587 - val_loss: 45486265991.1680\n",
      "Epoch 70/100\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 40554537524.3378 - val_loss: 45118544904.1920\n",
      "Epoch 71/100\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 40166073561.5431 - val_loss: 44796936945.6640\n",
      "Epoch 72/100\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 39799767800.9458 - val_loss: 44523846402.0480\n",
      "Epoch 73/100\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 39477650653.1840 - val_loss: 44263251247.1040\n",
      "Epoch 74/100\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 39176442332.9564 - val_loss: 44026805026.8160\n",
      "Epoch 75/100\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 38908903540.5084 - val_loss: 43846190563.3280\n",
      "Epoch 76/100\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 38656108538.5387 - val_loss: 43633884659.7120\n",
      "Epoch 77/100\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 38419792904.1920 - val_loss: 43461187305.4720\n",
      "Epoch 78/100\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 38209022492.6720 - val_loss: 43294594859.0080\n",
      "Epoch 79/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 38008762641.0667 - val_loss: 43155454885.8880\n",
      "Epoch 80/100\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 37825704640.5120 - val_loss: 42998542630.9120\n",
      "Epoch 81/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 37644078636.1458 - val_loss: 42849999388.6720\n",
      "Epoch 82/100\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 37473258577.0098 - val_loss: 42728433844.2240\n",
      "Epoch 83/100\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 37307832358.2293 - val_loss: 42574815232.0000\n",
      "Epoch 84/100\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 37145550334.6347 - val_loss: 42445937704.9600\n",
      "Epoch 85/100\n",
      "18000/18000 [==============================] - 1s 60us/step - loss: 36993879330.3609 - val_loss: 42312019771.3920\n",
      "Epoch 86/100\n",
      "18000/18000 [==============================] - 1s 65us/step - loss: 36872011029.6178 - val_loss: 42216560721.9200\n",
      "Epoch 87/100\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 36718543110.1440 - val_loss: 42057112682.4960\n",
      "Epoch 88/100\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 36575518548.8782 - val_loss: 41937540644.8640\n",
      "Epoch 89/100\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 36455208958.1796 - val_loss: 41832251883.5200\n",
      "Epoch 90/100\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 36319646063.7298 - val_loss: 41691414003.7120\n",
      "Epoch 91/100\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 36187254645.6462 - val_loss: 41587124764.6720\n",
      "Epoch 92/100\n",
      "18000/18000 [==============================] - 1s 59us/step - loss: 36073976934.8551 - val_loss: 41505920057.3440\n",
      "Epoch 93/100\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 35957195563.9182 - val_loss: 41391218556.9280\n",
      "Epoch 94/100\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 35848688677.3191 - val_loss: 41245494050.8160\n",
      "Epoch 95/100\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 35734628342.8978 - val_loss: 41151293227.0080\n",
      "Epoch 96/100\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 35626233211.5627 - val_loss: 41010026512.3840\n",
      "Epoch 97/100\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 35522794287.5591 - val_loss: 40936995618.8160\n",
      "Epoch 98/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 35425966059.9751 - val_loss: 40846830305.2800\n",
      "Epoch 99/100\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 35350887210.0978 - val_loss: 40750329987.0720\n",
      "Epoch 100/100\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 35247365306.5956 - val_loss: 40630580871.1680\n",
      "0.7200331607852261\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = nn_model()\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "history = model.fit(selected_feature_train, price_train,\n",
    "            epochs=100, batch_size=128,\n",
    "            validation_data=(selected_feature_val, price_val))\n",
    "model_score = score(model.predict(selected_feature_val), price_val)\n",
    "print(model_score)\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the future, consider standardizing outputs as well\n",
    "\n",
    "### Regularize:\n",
    "* Heavily parameterized models like neural networks are prone to overfitting\n",
    "* Popular off-the-shelf tools exist to regularize models and prevent overfitting:\n",
    "    - L2 regularization (weight decay)\n",
    "    - Dropout\n",
    "    - Batch normalization\n",
    "    \n",
    "#### These tools come as standard Keras/TF layers!\n",
    "`model.add(keras.layers.Dropout(rate)`\n",
    "`model.add(keras.layers.ActivityRegularization(l1=0.0, l2=0.0)`\n",
    "`model.add(keras.layers.BatchNormalization())`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping and model checkpointing:\n",
    "#### It's unlikely the last iteration is the best, and who knows how long until the thing is converged. Just grab the best validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/400\n",
      "18000/18000 [==============================] - 2s 122us/step - loss: 416645088222.3218 - val_loss: 456236671434.7521\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 456236671434.75201, saving model to best_model.h5\n",
      "Epoch 2/400\n",
      "18000/18000 [==============================] - 1s 62us/step - loss: 416197657056.5974 - val_loss: 455164537864.1920\n",
      "\n",
      "Epoch 00002: val_loss improved from 456236671434.75201 to 455164537864.19202, saving model to best_model.h5\n",
      "Epoch 3/400\n",
      "18000/18000 [==============================] - 1s 66us/step - loss: 414400001401.7422 - val_loss: 452025910820.8641\n",
      "\n",
      "Epoch 00003: val_loss improved from 455164537864.19202 to 452025910820.86401, saving model to best_model.h5\n",
      "Epoch 4/400\n",
      "18000/18000 [==============================] - 1s 63us/step - loss: 410371744945.4933 - val_loss: 445906115887.1040\n",
      "\n",
      "Epoch 00004: val_loss improved from 452025910820.86401 to 445906115887.10400, saving model to best_model.h5\n",
      "Epoch 5/400\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 403276284967.1395 - val_loss: 435968490078.2080\n",
      "\n",
      "Epoch 00005: val_loss improved from 445906115887.10400 to 435968490078.20801, saving model to best_model.h5\n",
      "Epoch 6/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 392518803273.9555 - val_loss: 421799506477.0560\n",
      "\n",
      "Epoch 00006: val_loss improved from 435968490078.20801 to 421799506477.05603, saving model to best_model.h5\n",
      "Epoch 7/400\n",
      "18000/18000 [==============================] - 1s 65us/step - loss: 377905444203.1786 - val_loss: 403042142257.1520\n",
      "\n",
      "Epoch 00007: val_loss improved from 421799506477.05603 to 403042142257.15198, saving model to best_model.h5\n",
      "Epoch 8/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 359439171758.7627 - val_loss: 380294197673.9840\n",
      "\n",
      "Epoch 00008: val_loss improved from 403042142257.15198 to 380294197673.98401, saving model to best_model.h5\n",
      "Epoch 9/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 337293403342.6205 - val_loss: 353580240863.2320\n",
      "\n",
      "Epoch 00009: val_loss improved from 380294197673.98401 to 353580240863.23199, saving model to best_model.h5\n",
      "Epoch 10/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 312120224048.0142 - val_loss: 323927445864.4480\n",
      "\n",
      "Epoch 00010: val_loss improved from 353580240863.23199 to 323927445864.44800, saving model to best_model.h5\n",
      "Epoch 11/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 284900121611.8329 - val_loss: 292531108839.4240\n",
      "\n",
      "Epoch 00011: val_loss improved from 323927445864.44800 to 292531108839.42401, saving model to best_model.h5\n",
      "Epoch 12/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 256797564250.7947 - val_loss: 261136424370.1760\n",
      "\n",
      "Epoch 00012: val_loss improved from 292531108839.42401 to 261136424370.17599, saving model to best_model.h5\n",
      "Epoch 13/400\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 228902969555.1715 - val_loss: 230564901683.2000\n",
      "\n",
      "Epoch 00013: val_loss improved from 261136424370.17599 to 230564901683.20001, saving model to best_model.h5\n",
      "Epoch 14/400\n",
      "18000/18000 [==============================] - 1s 64us/step - loss: 202254983204.4089 - val_loss: 201884320661.5040\n",
      "\n",
      "Epoch 00014: val_loss improved from 230564901683.20001 to 201884320661.50400, saving model to best_model.h5\n",
      "Epoch 15/400\n",
      "18000/18000 [==============================] - 1s 61us/step - loss: 177782115015.7938 - val_loss: 176104090370.0480\n",
      "\n",
      "Epoch 00015: val_loss improved from 201884320661.50400 to 176104090370.04800, saving model to best_model.h5\n",
      "Epoch 16/400\n",
      "18000/18000 [==============================] - 1s 62us/step - loss: 156188070830.9902 - val_loss: 154209125662.7200\n",
      "\n",
      "Epoch 00016: val_loss improved from 176104090370.04800 to 154209125662.72000, saving model to best_model.h5\n",
      "Epoch 17/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 137806850788.9209 - val_loss: 136010958635.0080\n",
      "\n",
      "Epoch 00017: val_loss improved from 154209125662.72000 to 136010958635.00800, saving model to best_model.h5\n",
      "Epoch 18/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 122601081201.5502 - val_loss: 121301015920.6400\n",
      "\n",
      "Epoch 00018: val_loss improved from 136010958635.00800 to 121301015920.64000, saving model to best_model.h5\n",
      "Epoch 19/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 110464811663.3600 - val_loss: 109919576457.2160\n",
      "\n",
      "Epoch 00019: val_loss improved from 121301015920.64000 to 109919576457.21600, saving model to best_model.h5\n",
      "Epoch 20/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 101121743585.2800 - val_loss: 101571065085.9520\n",
      "\n",
      "Epoch 00020: val_loss improved from 109919576457.21600 to 101571065085.95200, saving model to best_model.h5\n",
      "Epoch 21/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 94068384865.3938 - val_loss: 95244408913.9200\n",
      "\n",
      "Epoch 00021: val_loss improved from 101571065085.95200 to 95244408913.92000, saving model to best_model.h5\n",
      "Epoch 22/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 88756543400.6187 - val_loss: 90569775710.2080\n",
      "\n",
      "Epoch 00022: val_loss improved from 95244408913.92000 to 90569775710.20799, saving model to best_model.h5\n",
      "Epoch 23/400\n",
      "18000/18000 [==============================] - 1s 59us/step - loss: 84712594633.1591 - val_loss: 87002215809.0240\n",
      "\n",
      "Epoch 00023: val_loss improved from 90569775710.20799 to 87002215809.02400, saving model to best_model.h5\n",
      "Epoch 24/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 81603228612.8356 - val_loss: 84256221429.7600\n",
      "\n",
      "Epoch 00024: val_loss improved from 87002215809.02400 to 84256221429.75999, saving model to best_model.h5\n",
      "Epoch 25/400\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 79173733173.0204 - val_loss: 82061945143.2960\n",
      "\n",
      "Epoch 00025: val_loss improved from 84256221429.75999 to 82061945143.29601, saving model to best_model.h5\n",
      "Epoch 26/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 77199145748.2524 - val_loss: 80246319415.2960\n",
      "\n",
      "Epoch 00026: val_loss improved from 82061945143.29601 to 80246319415.29601, saving model to best_model.h5\n",
      "Epoch 27/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 75571666361.4578 - val_loss: 78672987553.7920\n",
      "\n",
      "Epoch 00027: val_loss improved from 80246319415.29601 to 78672987553.79201, saving model to best_model.h5\n",
      "Epoch 28/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 74170761594.6525 - val_loss: 77334466396.1600\n",
      "\n",
      "Epoch 00028: val_loss improved from 78672987553.79201 to 77334466396.16000, saving model to best_model.h5\n",
      "Epoch 29/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 72970862060.4302 - val_loss: 76142404042.7520\n",
      "\n",
      "Epoch 00029: val_loss improved from 77334466396.16000 to 76142404042.75200, saving model to best_model.h5\n",
      "Epoch 30/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 71886676237.4258 - val_loss: 75062966616.0640\n",
      "\n",
      "Epoch 00030: val_loss improved from 76142404042.75200 to 75062966616.06400, saving model to best_model.h5\n",
      "Epoch 31/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 70911220005.0916 - val_loss: 74074677116.9280\n",
      "\n",
      "Epoch 00031: val_loss improved from 75062966616.06400 to 74074677116.92799, saving model to best_model.h5\n",
      "Epoch 32/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 69992349397.4471 - val_loss: 73150938775.5520\n",
      "\n",
      "Epoch 00032: val_loss improved from 74074677116.92799 to 73150938775.55200, saving model to best_model.h5\n",
      "Epoch 33/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 69135029010.4320 - val_loss: 72271365406.7200\n",
      "\n",
      "Epoch 00033: val_loss improved from 73150938775.55200 to 72271365406.72000, saving model to best_model.h5\n",
      "Epoch 34/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 68318729768.5049 - val_loss: 71427720773.6320\n",
      "\n",
      "Epoch 00034: val_loss improved from 72271365406.72000 to 71427720773.63200, saving model to best_model.h5\n",
      "Epoch 35/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 67522541114.7093 - val_loss: 70637041188.8640\n",
      "\n",
      "Epoch 00035: val_loss improved from 71427720773.63200 to 70637041188.86400, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/400\n",
      "18000/18000 [==============================] - 2s 98us/step - loss: 66751103209.0169 - val_loss: 69857330593.7920\n",
      "\n",
      "Epoch 00036: val_loss improved from 70637041188.86400 to 69857330593.79201, saving model to best_model.h5\n",
      "Epoch 37/400\n",
      "18000/18000 [==============================] - 1s 65us/step - loss: 66001783550.4071 - val_loss: 69088201703.4240\n",
      "\n",
      "Epoch 00037: val_loss improved from 69857330593.79201 to 69088201703.42400, saving model to best_model.h5\n",
      "Epoch 38/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 65259026411.9751 - val_loss: 68354475753.4720\n",
      "\n",
      "Epoch 00038: val_loss improved from 69088201703.42400 to 68354475753.47200, saving model to best_model.h5\n",
      "Epoch 39/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 64521657971.1431 - val_loss: 67613030645.7600\n",
      "\n",
      "Epoch 00039: val_loss improved from 68354475753.47200 to 67613030645.76000, saving model to best_model.h5\n",
      "Epoch 40/400\n",
      "18000/18000 [==============================] - 1s 59us/step - loss: 63805681441.9058 - val_loss: 66886318161.9200\n",
      "\n",
      "Epoch 00040: val_loss improved from 67613030645.76000 to 66886318161.92000, saving model to best_model.h5\n",
      "Epoch 41/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 63085259157.9591 - val_loss: 66176570851.3280\n",
      "\n",
      "Epoch 00041: val_loss improved from 66886318161.92000 to 66176570851.32800, saving model to best_model.h5\n",
      "Epoch 42/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 62352147873.7920 - val_loss: 65469578805.2480\n",
      "\n",
      "Epoch 00042: val_loss improved from 66176570851.32800 to 65469578805.24800, saving model to best_model.h5\n",
      "Epoch 43/400\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 61642574128.0142 - val_loss: 64760455593.9840\n",
      "\n",
      "Epoch 00043: val_loss improved from 65469578805.24800 to 64760455593.98400, saving model to best_model.h5\n",
      "Epoch 44/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 60922253053.4969 - val_loss: 64051841368.0640\n",
      "\n",
      "Epoch 00044: val_loss improved from 64760455593.98400 to 64051841368.06400, saving model to best_model.h5\n",
      "Epoch 45/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 60212032700.4160 - val_loss: 63356539469.8240\n",
      "\n",
      "Epoch 00045: val_loss improved from 64051841368.06400 to 63356539469.82400, saving model to best_model.h5\n",
      "Epoch 46/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 59512277110.3289 - val_loss: 62666123214.8480\n",
      "\n",
      "Epoch 00046: val_loss improved from 63356539469.82400 to 62666123214.84800, saving model to best_model.h5\n",
      "Epoch 47/400\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 58790632022.9262 - val_loss: 61966838890.4960\n",
      "\n",
      "Epoch 00047: val_loss improved from 62666123214.84800 to 61966838890.49600, saving model to best_model.h5\n",
      "Epoch 48/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 58078336268.5156 - val_loss: 61275309015.0400\n",
      "\n",
      "Epoch 00048: val_loss improved from 61966838890.49600 to 61275309015.04000, saving model to best_model.h5\n",
      "Epoch 49/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 57367972988.7004 - val_loss: 60577946206.2080\n",
      "\n",
      "Epoch 00049: val_loss improved from 61275309015.04000 to 60577946206.20800, saving model to best_model.h5\n",
      "Epoch 50/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 56665332933.5182 - val_loss: 59889384423.4240\n",
      "\n",
      "Epoch 00050: val_loss improved from 60577946206.20800 to 59889384423.42400, saving model to best_model.h5\n",
      "Epoch 51/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 55965308481.9911 - val_loss: 59207354744.8320\n",
      "\n",
      "Epoch 00051: val_loss improved from 59889384423.42400 to 59207354744.83200, saving model to best_model.h5\n",
      "Epoch 52/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 55272883413.9022 - val_loss: 58513943822.3360\n",
      "\n",
      "Epoch 00052: val_loss improved from 59207354744.83200 to 58513943822.33600, saving model to best_model.h5\n",
      "Epoch 53/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 54556523415.3244 - val_loss: 57838029209.6000\n",
      "\n",
      "Epoch 00053: val_loss improved from 58513943822.33600 to 57838029209.60000, saving model to best_model.h5\n",
      "Epoch 54/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 53875024891.4489 - val_loss: 57163240898.5600\n",
      "\n",
      "Epoch 00054: val_loss improved from 57838029209.60000 to 57163240898.56000, saving model to best_model.h5\n",
      "Epoch 55/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 53175751162.0835 - val_loss: 56490331766.7840\n",
      "\n",
      "Epoch 00055: val_loss improved from 57163240898.56000 to 56490331766.78400, saving model to best_model.h5\n",
      "Epoch 56/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 52504312232.1636 - val_loss: 55827264897.0240\n",
      "\n",
      "Epoch 00056: val_loss improved from 56490331766.78400 to 55827264897.02400, saving model to best_model.h5\n",
      "Epoch 57/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 51812337684.0249 - val_loss: 55167151210.4960\n",
      "\n",
      "Epoch 00057: val_loss improved from 55827264897.02400 to 55167151210.49600, saving model to best_model.h5\n",
      "Epoch 58/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 51132023271.8791 - val_loss: 54514120097.7920\n",
      "\n",
      "Epoch 00058: val_loss improved from 55167151210.49600 to 54514120097.79200, saving model to best_model.h5\n",
      "Epoch 59/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 50464173471.9716 - val_loss: 53875745554.4320\n",
      "\n",
      "Epoch 00059: val_loss improved from 54514120097.79200 to 53875745554.43200, saving model to best_model.h5\n",
      "Epoch 60/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 49799531772.1316 - val_loss: 53240766070.7840\n",
      "\n",
      "Epoch 00060: val_loss improved from 53875745554.43200 to 53240766070.78400, saving model to best_model.h5\n",
      "Epoch 61/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 49152544068.9493 - val_loss: 52635786477.5680\n",
      "\n",
      "Epoch 00061: val_loss improved from 53240766070.78400 to 52635786477.56800, saving model to best_model.h5\n",
      "Epoch 62/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 48499499022.5636 - val_loss: 52019728154.6240\n",
      "\n",
      "Epoch 00062: val_loss improved from 52635786477.56800 to 52019728154.62400, saving model to best_model.h5\n",
      "Epoch 63/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 47870956175.3600 - val_loss: 51429909331.9680\n",
      "\n",
      "Epoch 00063: val_loss improved from 52019728154.62400 to 51429909331.96800, saving model to best_model.h5\n",
      "Epoch 64/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 47250644325.7173 - val_loss: 50849696153.6000\n",
      "\n",
      "Epoch 00064: val_loss improved from 51429909331.96800 to 50849696153.60000, saving model to best_model.h5\n",
      "Epoch 65/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 46627214440.6756 - val_loss: 50274833858.5600\n",
      "\n",
      "Epoch 00065: val_loss improved from 50849696153.60000 to 50274833858.56000, saving model to best_model.h5\n",
      "Epoch 66/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 46028059985.6924 - val_loss: 49711005827.0720\n",
      "\n",
      "Epoch 00066: val_loss improved from 50274833858.56000 to 49711005827.07200, saving model to best_model.h5\n",
      "Epoch 67/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 45431022849.5929 - val_loss: 49183650480.1280\n",
      "\n",
      "Epoch 00067: val_loss improved from 49711005827.07200 to 49183650480.12800, saving model to best_model.h5\n",
      "Epoch 68/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 44862129751.8364 - val_loss: 48653637189.6320\n",
      "\n",
      "Epoch 00068: val_loss improved from 49183650480.12800 to 48653637189.63200, saving model to best_model.h5\n",
      "Epoch 69/400\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 44295542971.5058 - val_loss: 48132611080.1920\n",
      "\n",
      "Epoch 00069: val_loss improved from 48653637189.63200 to 48132611080.19200, saving model to best_model.h5\n",
      "Epoch 70/400\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 43738164989.4969 - val_loss: 47630259355.6480\n",
      "\n",
      "Epoch 00070: val_loss improved from 48132611080.19200 to 47630259355.64800, saving model to best_model.h5\n",
      "Epoch 71/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 48us/step - loss: 43203405716.5938 - val_loss: 47153690738.6880\n",
      "\n",
      "Epoch 00071: val_loss improved from 47630259355.64800 to 47153690738.68800, saving model to best_model.h5\n",
      "Epoch 72/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 42679820593.8347 - val_loss: 46684520611.8400\n",
      "\n",
      "Epoch 00072: val_loss improved from 47153690738.68800 to 46684520611.84000, saving model to best_model.h5\n",
      "Epoch 73/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 42188650075.4773 - val_loss: 46244600971.2640\n",
      "\n",
      "Epoch 00073: val_loss improved from 46684520611.84000 to 46244600971.26400, saving model to best_model.h5\n",
      "Epoch 74/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 41699817934.3929 - val_loss: 45842708004.8640\n",
      "\n",
      "Epoch 00074: val_loss improved from 46244600971.26400 to 45842708004.86400, saving model to best_model.h5\n",
      "Epoch 75/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 41247992497.9484 - val_loss: 45455436611.5840\n",
      "\n",
      "Epoch 00075: val_loss improved from 45842708004.86400 to 45455436611.58400, saving model to best_model.h5\n",
      "Epoch 76/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 40832375728.8107 - val_loss: 45075864322.0480\n",
      "\n",
      "Epoch 00076: val_loss improved from 45455436611.58400 to 45075864322.04800, saving model to best_model.h5\n",
      "Epoch 77/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 40425152253.4969 - val_loss: 44754962612.2240\n",
      "\n",
      "Epoch 00077: val_loss improved from 45075864322.04800 to 44754962612.22400, saving model to best_model.h5\n",
      "Epoch 78/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 40050075356.7289 - val_loss: 44456154431.4880\n",
      "\n",
      "Epoch 00078: val_loss improved from 44754962612.22400 to 44456154431.48800, saving model to best_model.h5\n",
      "Epoch 79/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 39703157855.5733 - val_loss: 44172761563.1360\n",
      "\n",
      "Epoch 00079: val_loss improved from 44456154431.48800 to 44172761563.13600, saving model to best_model.h5\n",
      "Epoch 80/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 39387283319.4667 - val_loss: 43907886809.0880\n",
      "\n",
      "Epoch 00080: val_loss improved from 44172761563.13600 to 43907886809.08800, saving model to best_model.h5\n",
      "Epoch 81/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 39083220001.6782 - val_loss: 43669057470.4640\n",
      "\n",
      "Epoch 00081: val_loss improved from 43907886809.08800 to 43669057470.46400, saving model to best_model.h5\n",
      "Epoch 82/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 38810774277.6889 - val_loss: 43458989686.7840\n",
      "\n",
      "Epoch 00082: val_loss improved from 43669057470.46400 to 43458989686.78400, saving model to best_model.h5\n",
      "Epoch 83/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 38558436786.1760 - val_loss: 43272881373.1840\n",
      "\n",
      "Epoch 00083: val_loss improved from 43458989686.78400 to 43272881373.18400, saving model to best_model.h5\n",
      "Epoch 84/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 38332917643.4916 - val_loss: 43102741364.7360\n",
      "\n",
      "Epoch 00084: val_loss improved from 43272881373.18400 to 43102741364.73600, saving model to best_model.h5\n",
      "Epoch 85/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 38134555435.9182 - val_loss: 42940679946.2400\n",
      "\n",
      "Epoch 00085: val_loss improved from 43102741364.73600 to 42940679946.24000, saving model to best_model.h5\n",
      "Epoch 86/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 37955865983.2036 - val_loss: 42818599026.6880\n",
      "\n",
      "Epoch 00086: val_loss improved from 42940679946.24000 to 42818599026.68800, saving model to best_model.h5\n",
      "Epoch 87/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 37772205965.3120 - val_loss: 42683706572.8000\n",
      "\n",
      "Epoch 00087: val_loss improved from 42818599026.68800 to 42683706572.80000, saving model to best_model.h5\n",
      "Epoch 88/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 37610136460.4018 - val_loss: 42555996372.9920\n",
      "\n",
      "Epoch 00088: val_loss improved from 42683706572.80000 to 42555996372.99200, saving model to best_model.h5\n",
      "Epoch 89/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 37465811721.3298 - val_loss: 42423765237.7600\n",
      "\n",
      "Epoch 00089: val_loss improved from 42555996372.99200 to 42423765237.76000, saving model to best_model.h5\n",
      "Epoch 90/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 37325928704.6827 - val_loss: 42354548867.0720\n",
      "\n",
      "Epoch 00090: val_loss improved from 42423765237.76000 to 42354548867.07200, saving model to best_model.h5\n",
      "Epoch 91/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 37198905689.8844 - val_loss: 42212640423.9360\n",
      "\n",
      "Epoch 00091: val_loss improved from 42354548867.07200 to 42212640423.93600, saving model to best_model.h5\n",
      "Epoch 92/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 37075291034.9653 - val_loss: 42115949166.5920\n",
      "\n",
      "Epoch 00092: val_loss improved from 42212640423.93600 to 42115949166.59200, saving model to best_model.h5\n",
      "Epoch 93/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 36960614470.9973 - val_loss: 42047881707.5200\n",
      "\n",
      "Epoch 00093: val_loss improved from 42115949166.59200 to 42047881707.52000, saving model to best_model.h5\n",
      "Epoch 94/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 36847697748.8782 - val_loss: 41958313590.7840\n",
      "\n",
      "Epoch 00094: val_loss improved from 42047881707.52000 to 41958313590.78400, saving model to best_model.h5\n",
      "Epoch 95/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 36746772348.9280 - val_loss: 41892034248.7040\n",
      "\n",
      "Epoch 00095: val_loss improved from 41958313590.78400 to 41892034248.70400, saving model to best_model.h5\n",
      "Epoch 96/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 36648524440.4622 - val_loss: 41777164386.3040\n",
      "\n",
      "Epoch 00096: val_loss improved from 41892034248.70400 to 41777164386.30400, saving model to best_model.h5\n",
      "Epoch 97/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 36547636146.6311 - val_loss: 41662185406.4640\n",
      "\n",
      "Epoch 00097: val_loss improved from 41777164386.30400 to 41662185406.46400, saving model to best_model.h5\n",
      "Epoch 98/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 36446482890.7520 - val_loss: 41581772079.1040\n",
      "\n",
      "Epoch 00098: val_loss improved from 41662185406.46400 to 41581772079.10400, saving model to best_model.h5\n",
      "Epoch 99/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 36346986167.4098 - val_loss: 41517011468.2880\n",
      "\n",
      "Epoch 00099: val_loss improved from 41581772079.10400 to 41517011468.28800, saving model to best_model.h5\n",
      "Epoch 100/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 36259087999.8862 - val_loss: 41427063930.8800\n",
      "\n",
      "Epoch 00100: val_loss improved from 41517011468.28800 to 41427063930.88000, saving model to best_model.h5\n",
      "Epoch 101/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 36178341631.3173 - val_loss: 41330681839.6160\n",
      "\n",
      "Epoch 00101: val_loss improved from 41427063930.88000 to 41330681839.61600, saving model to best_model.h5\n",
      "Epoch 102/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 36082635148.8569 - val_loss: 41224239513.6000\n",
      "\n",
      "Epoch 00102: val_loss improved from 41330681839.61600 to 41224239513.60000, saving model to best_model.h5\n",
      "Epoch 103/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 35997897573.2622 - val_loss: 41157933662.2080\n",
      "\n",
      "Epoch 00103: val_loss improved from 41224239513.60000 to 41157933662.20800, saving model to best_model.h5\n",
      "Epoch 104/400\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 35904001887.8009 - val_loss: 41071589883.9040\n",
      "\n",
      "Epoch 00104: val_loss improved from 41157933662.20800 to 41071589883.90400, saving model to best_model.h5\n",
      "Epoch 105/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 35819676627.3991 - val_loss: 40975216181.2480\n",
      "\n",
      "Epoch 00105: val_loss improved from 41071589883.90400 to 40975216181.24800, saving model to best_model.h5\n",
      "Epoch 106/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 49us/step - loss: 35734134747.5911 - val_loss: 40943851732.9920\n",
      "\n",
      "Epoch 00106: val_loss improved from 40975216181.24800 to 40943851732.99200, saving model to best_model.h5\n",
      "Epoch 107/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 35662241264.0711 - val_loss: 40829336387.5840\n",
      "\n",
      "Epoch 00107: val_loss improved from 40943851732.99200 to 40829336387.58400, saving model to best_model.h5\n",
      "Epoch 108/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 35598459100.2738 - val_loss: 40757999697.9200\n",
      "\n",
      "Epoch 00108: val_loss improved from 40829336387.58400 to 40757999697.92000, saving model to best_model.h5\n",
      "Epoch 109/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 35503797390.9049 - val_loss: 40652122750.9760\n",
      "\n",
      "Epoch 00109: val_loss improved from 40757999697.92000 to 40652122750.97600, saving model to best_model.h5\n",
      "Epoch 110/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 35426803278.7342 - val_loss: 40556733726.7200\n",
      "\n",
      "Epoch 00110: val_loss improved from 40652122750.97600 to 40556733726.72000, saving model to best_model.h5\n",
      "Epoch 111/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 35356519759.8720 - val_loss: 40471838654.4640\n",
      "\n",
      "Epoch 00111: val_loss improved from 40556733726.72000 to 40471838654.46400, saving model to best_model.h5\n",
      "Epoch 112/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 35281241465.7422 - val_loss: 40392378253.3120\n",
      "\n",
      "Epoch 00112: val_loss improved from 40471838654.46400 to 40392378253.31200, saving model to best_model.h5\n",
      "Epoch 113/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 35210804501.6178 - val_loss: 40340045004.8000\n",
      "\n",
      "Epoch 00113: val_loss improved from 40392378253.31200 to 40340045004.80000, saving model to best_model.h5\n",
      "Epoch 114/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 35140929020.8142 - val_loss: 40258438103.0400\n",
      "\n",
      "Epoch 00114: val_loss improved from 40340045004.80000 to 40258438103.04000, saving model to best_model.h5\n",
      "Epoch 115/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 35077332802.6738 - val_loss: 40180619378.6880\n",
      "\n",
      "Epoch 00115: val_loss improved from 40258438103.04000 to 40180619378.68800, saving model to best_model.h5\n",
      "Epoch 116/400\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 35005742954.7236 - val_loss: 40121166626.8160\n",
      "\n",
      "Epoch 00116: val_loss improved from 40180619378.68800 to 40121166626.81600, saving model to best_model.h5\n",
      "Epoch 117/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 34944534608.0996 - val_loss: 40068653776.8960\n",
      "\n",
      "Epoch 00117: val_loss improved from 40121166626.81600 to 40068653776.89600, saving model to best_model.h5\n",
      "Epoch 118/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 34878449454.6489 - val_loss: 40017548607.4880\n",
      "\n",
      "Epoch 00118: val_loss improved from 40068653776.89600 to 40017548607.48800, saving model to best_model.h5\n",
      "Epoch 119/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 34823080516.7218 - val_loss: 39914759979.0080\n",
      "\n",
      "Epoch 00119: val_loss improved from 40017548607.48800 to 39914759979.00800, saving model to best_model.h5\n",
      "Epoch 120/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 34754789215.8009 - val_loss: 39857587814.4000\n",
      "\n",
      "Epoch 00120: val_loss improved from 39914759979.00800 to 39857587814.40000, saving model to best_model.h5\n",
      "Epoch 121/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 34694084513.3369 - val_loss: 39801243566.0800\n",
      "\n",
      "Epoch 00121: val_loss improved from 39857587814.40000 to 39801243566.08000, saving model to best_model.h5\n",
      "Epoch 122/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 34627529823.5733 - val_loss: 39761618239.4880\n",
      "\n",
      "Epoch 00122: val_loss improved from 39801243566.08000 to 39761618239.48800, saving model to best_model.h5\n",
      "Epoch 123/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 34572439009.5076 - val_loss: 39677768040.4480\n",
      "\n",
      "Epoch 00123: val_loss improved from 39761618239.48800 to 39677768040.44800, saving model to best_model.h5\n",
      "Epoch 124/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 34525818759.8507 - val_loss: 39604220788.7360\n",
      "\n",
      "Epoch 00124: val_loss improved from 39677768040.44800 to 39604220788.73600, saving model to best_model.h5\n",
      "Epoch 125/400\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 34456151676.2453 - val_loss: 39527747256.3200\n",
      "\n",
      "Epoch 00125: val_loss improved from 39604220788.73600 to 39527747256.32000, saving model to best_model.h5\n",
      "Epoch 126/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 34412487147.5200 - val_loss: 39495456587.7760\n",
      "\n",
      "Epoch 00126: val_loss improved from 39527747256.32000 to 39495456587.77600, saving model to best_model.h5\n",
      "Epoch 127/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 34360288342.4711 - val_loss: 39408326049.7920\n",
      "\n",
      "Epoch 00127: val_loss improved from 39495456587.77600 to 39408326049.79200, saving model to best_model.h5\n",
      "Epoch 128/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 34299272559.7298 - val_loss: 39382138880.0000\n",
      "\n",
      "Epoch 00128: val_loss improved from 39408326049.79200 to 39382138880.00000, saving model to best_model.h5\n",
      "Epoch 129/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 34249322011.7618 - val_loss: 39304483110.9120\n",
      "\n",
      "Epoch 00129: val_loss improved from 39382138880.00000 to 39304483110.91200, saving model to best_model.h5\n",
      "Epoch 130/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 34207225879.6658 - val_loss: 39256446763.0080\n",
      "\n",
      "Epoch 00130: val_loss improved from 39304483110.91200 to 39256446763.00800, saving model to best_model.h5\n",
      "Epoch 131/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 34147421315.9822 - val_loss: 39216078454.7840\n",
      "\n",
      "Epoch 00131: val_loss improved from 39256446763.00800 to 39216078454.78400, saving model to best_model.h5\n",
      "Epoch 132/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 34096112285.9236 - val_loss: 39169674608.6400\n",
      "\n",
      "Epoch 00132: val_loss improved from 39216078454.78400 to 39169674608.64000, saving model to best_model.h5\n",
      "Epoch 133/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 34042727195.5342 - val_loss: 39110353518.5920\n",
      "\n",
      "Epoch 00133: val_loss improved from 39169674608.64000 to 39110353518.59200, saving model to best_model.h5\n",
      "Epoch 134/400\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 34000066867.6551 - val_loss: 39095101784.0640\n",
      "\n",
      "Epoch 00134: val_loss improved from 39110353518.59200 to 39095101784.06400, saving model to best_model.h5\n",
      "Epoch 135/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 33945684069.9449 - val_loss: 39013146099.7120\n",
      "\n",
      "Epoch 00135: val_loss improved from 39095101784.06400 to 39013146099.71200, saving model to best_model.h5\n",
      "Epoch 136/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 33900133646.3360 - val_loss: 38977756069.8880\n",
      "\n",
      "Epoch 00136: val_loss improved from 39013146099.71200 to 38977756069.88800, saving model to best_model.h5\n",
      "Epoch 137/400\n",
      "18000/18000 [==============================] - 1s 37us/step - loss: 33863613846.8693 - val_loss: 38907818999.8080\n",
      "\n",
      "Epoch 00137: val_loss improved from 38977756069.88800 to 38907818999.80800, saving model to best_model.h5\n",
      "Epoch 138/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 33816919667.1431 - val_loss: 38869480570.8800\n",
      "\n",
      "Epoch 00138: val_loss improved from 38907818999.80800 to 38869480570.88000, saving model to best_model.h5\n",
      "Epoch 139/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 33769286493.0702 - val_loss: 38835417055.2320\n",
      "\n",
      "Epoch 00139: val_loss improved from 38869480570.88000 to 38835417055.23200, saving model to best_model.h5\n",
      "Epoch 140/400\n",
      "18000/18000 [==============================] - 1s 38us/step - loss: 33736138264.1209 - val_loss: 38772498497.5360\n",
      "\n",
      "Epoch 00140: val_loss improved from 38835417055.23200 to 38772498497.53600, saving model to best_model.h5\n",
      "Epoch 141/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 48us/step - loss: 33696992580.9493 - val_loss: 38764493045.7600\n",
      "\n",
      "Epoch 00141: val_loss improved from 38772498497.53600 to 38764493045.76000, saving model to best_model.h5\n",
      "Epoch 142/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 33650815842.5316 - val_loss: 38692661329.9200\n",
      "\n",
      "Epoch 00142: val_loss improved from 38764493045.76000 to 38692661329.92000, saving model to best_model.h5\n",
      "Epoch 143/400\n",
      "18000/18000 [==============================] - 1s 61us/step - loss: 33599817602.3893 - val_loss: 38661002297.3440\n",
      "\n",
      "Epoch 00143: val_loss improved from 38692661329.92000 to 38661002297.34400, saving model to best_model.h5\n",
      "Epoch 144/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 33556876561.9769 - val_loss: 38601671704.5760\n",
      "\n",
      "Epoch 00144: val_loss improved from 38661002297.34400 to 38601671704.57600, saving model to best_model.h5\n",
      "Epoch 145/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 33527061834.4107 - val_loss: 38563541614.5920\n",
      "\n",
      "Epoch 00145: val_loss improved from 38601671704.57600 to 38563541614.59200, saving model to best_model.h5\n",
      "Epoch 146/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 33482383301.7458 - val_loss: 38534326190.0800\n",
      "\n",
      "Epoch 00146: val_loss improved from 38563541614.59200 to 38534326190.08000, saving model to best_model.h5\n",
      "Epoch 147/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 33450574816.1422 - val_loss: 38526453383.1680\n",
      "\n",
      "Epoch 00147: val_loss improved from 38534326190.08000 to 38526453383.16800, saving model to best_model.h5\n",
      "Epoch 148/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 33417307979.7760 - val_loss: 38463937380.3520\n",
      "\n",
      "Epoch 00148: val_loss improved from 38526453383.16800 to 38463937380.35200, saving model to best_model.h5\n",
      "Epoch 149/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 33383097546.9796 - val_loss: 38405969543.1680\n",
      "\n",
      "Epoch 00149: val_loss improved from 38463937380.35200 to 38405969543.16800, saving model to best_model.h5\n",
      "Epoch 150/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 33343809164.6293 - val_loss: 38398706647.0400\n",
      "\n",
      "Epoch 00150: val_loss improved from 38405969543.16800 to 38398706647.04000, saving model to best_model.h5\n",
      "Epoch 151/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 33314583004.9564 - val_loss: 38365830545.4080\n",
      "\n",
      "Epoch 00151: val_loss improved from 38398706647.04000 to 38365830545.40800, saving model to best_model.h5\n",
      "Epoch 152/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 33269216420.7502 - val_loss: 38295932043.2640\n",
      "\n",
      "Epoch 00152: val_loss improved from 38365830545.40800 to 38295932043.26400, saving model to best_model.h5\n",
      "Epoch 153/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 33237643158.4142 - val_loss: 38252653740.0320\n",
      "\n",
      "Epoch 00153: val_loss improved from 38295932043.26400 to 38252653740.03200, saving model to best_model.h5\n",
      "Epoch 154/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 33207404207.2178 - val_loss: 38242748104.7040\n",
      "\n",
      "Epoch 00154: val_loss improved from 38252653740.03200 to 38242748104.70400, saving model to best_model.h5\n",
      "Epoch 155/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 33181476101.2338 - val_loss: 38207427149.8240\n",
      "\n",
      "Epoch 00155: val_loss improved from 38242748104.70400 to 38207427149.82400, saving model to best_model.h5\n",
      "Epoch 156/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 33148104620.2596 - val_loss: 38219259052.0320\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 38207427149.82400\n",
      "Epoch 157/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 33117534981.6889 - val_loss: 38169959399.4240\n",
      "\n",
      "Epoch 00157: val_loss improved from 38207427149.82400 to 38169959399.42400, saving model to best_model.h5\n",
      "Epoch 158/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 33083978980.4658 - val_loss: 38128672538.6240\n",
      "\n",
      "Epoch 00158: val_loss improved from 38169959399.42400 to 38128672538.62400, saving model to best_model.h5\n",
      "Epoch 159/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 33055623803.3351 - val_loss: 38104472518.6560\n",
      "\n",
      "Epoch 00159: val_loss improved from 38128672538.62400 to 38104472518.65600, saving model to best_model.h5\n",
      "Epoch 160/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 33030845819.5627 - val_loss: 38050051588.0960\n",
      "\n",
      "Epoch 00160: val_loss improved from 38104472518.65600 to 38050051588.09600, saving model to best_model.h5\n",
      "Epoch 161/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 33003038649.0027 - val_loss: 38024709996.5440\n",
      "\n",
      "Epoch 00161: val_loss improved from 38050051588.09600 to 38024709996.54400, saving model to best_model.h5\n",
      "Epoch 162/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 32978092068.4089 - val_loss: 37985452359.6800\n",
      "\n",
      "Epoch 00162: val_loss improved from 38024709996.54400 to 37985452359.68000, saving model to best_model.h5\n",
      "Epoch 163/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 32952904169.6996 - val_loss: 37950014291.9680\n",
      "\n",
      "Epoch 00163: val_loss improved from 37985452359.68000 to 37950014291.96800, saving model to best_model.h5\n",
      "Epoch 164/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 32922112342.2436 - val_loss: 37944827445.2480\n",
      "\n",
      "Epoch 00164: val_loss improved from 37950014291.96800 to 37944827445.24800, saving model to best_model.h5\n",
      "Epoch 165/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 32901939822.5920 - val_loss: 37927616806.9120\n",
      "\n",
      "Epoch 00165: val_loss improved from 37944827445.24800 to 37927616806.91200, saving model to best_model.h5\n",
      "Epoch 166/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 32874172237.5964 - val_loss: 37875793133.5680\n",
      "\n",
      "Epoch 00166: val_loss improved from 37927616806.91200 to 37875793133.56800, saving model to best_model.h5\n",
      "Epoch 167/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 32855672127.4880 - val_loss: 37850992934.9120\n",
      "\n",
      "Epoch 00167: val_loss improved from 37875793133.56800 to 37850992934.91200, saving model to best_model.h5\n",
      "Epoch 168/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 32833501797.4898 - val_loss: 37854491049.9840\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 37850992934.91200\n",
      "Epoch 169/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 32802107472.0996 - val_loss: 37792669990.9120\n",
      "\n",
      "Epoch 00169: val_loss improved from 37850992934.91200 to 37792669990.91200, saving model to best_model.h5\n",
      "Epoch 170/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 32793616982.9262 - val_loss: 37775277621.2480\n",
      "\n",
      "Epoch 00170: val_loss improved from 37792669990.91200 to 37775277621.24800, saving model to best_model.h5\n",
      "Epoch 171/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 32764825609.1022 - val_loss: 37755452555.2640\n",
      "\n",
      "Epoch 00171: val_loss improved from 37775277621.24800 to 37755452555.26400, saving model to best_model.h5\n",
      "Epoch 172/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 32735820910.1369 - val_loss: 37756339093.5040\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 37755452555.26400\n",
      "Epoch 173/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 32725035748.9209 - val_loss: 37703976288.2560\n",
      "\n",
      "Epoch 00173: val_loss improved from 37755452555.26400 to 37703976288.25600, saving model to best_model.h5\n",
      "Epoch 174/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 32696046611.1147 - val_loss: 37715366707.2000\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 37703976288.25600\n",
      "Epoch 175/400\n",
      "18000/18000 [==============================] - 1s 38us/step - loss: 32676534059.0080 - val_loss: 37698144731.1360\n",
      "\n",
      "Epoch 00175: val_loss improved from 37703976288.25600 to 37698144731.13600, saving model to best_model.h5\n",
      "Epoch 176/400\n",
      "18000/18000 [==============================] - 1s 39us/step - loss: 32661015862.3858 - val_loss: 37658896662.5280\n",
      "\n",
      "Epoch 00176: val_loss improved from 37698144731.13600 to 37658896662.52800, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 32635938470.1156 - val_loss: 37633564672.0000\n",
      "\n",
      "Epoch 00177: val_loss improved from 37658896662.52800 to 37633564672.00000, saving model to best_model.h5\n",
      "Epoch 178/400\n",
      "18000/18000 [==============================] - 1s 61us/step - loss: 32617037973.2764 - val_loss: 37613019136.0000\n",
      "\n",
      "Epoch 00178: val_loss improved from 37633564672.00000 to 37613019136.00000, saving model to best_model.h5\n",
      "Epoch 179/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 32604205929.8133 - val_loss: 37593009618.9440\n",
      "\n",
      "Epoch 00179: val_loss improved from 37613019136.00000 to 37593009618.94400, saving model to best_model.h5\n",
      "Epoch 180/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 32580118923.0364 - val_loss: 37583775039.4880\n",
      "\n",
      "Epoch 00180: val_loss improved from 37593009618.94400 to 37583775039.48800, saving model to best_model.h5\n",
      "Epoch 181/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 32562514065.6356 - val_loss: 37531538358.2720\n",
      "\n",
      "Epoch 00181: val_loss improved from 37583775039.48800 to 37531538358.27200, saving model to best_model.h5\n",
      "Epoch 182/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 32539511848.9600 - val_loss: 37535664275.4560\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 37531538358.27200\n",
      "Epoch 183/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 32516917669.4329 - val_loss: 37530230816.7680\n",
      "\n",
      "Epoch 00183: val_loss improved from 37531538358.27200 to 37530230816.76800, saving model to best_model.h5\n",
      "Epoch 184/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 32516181002.0124 - val_loss: 37478621970.4320\n",
      "\n",
      "Epoch 00184: val_loss improved from 37530230816.76800 to 37478621970.43200, saving model to best_model.h5\n",
      "Epoch 185/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 32489994611.3707 - val_loss: 37462110240.7680\n",
      "\n",
      "Epoch 00185: val_loss improved from 37478621970.43200 to 37462110240.76800, saving model to best_model.h5\n",
      "Epoch 186/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 32473291463.7938 - val_loss: 37428731379.7120\n",
      "\n",
      "Epoch 00186: val_loss improved from 37462110240.76800 to 37428731379.71200, saving model to best_model.h5\n",
      "Epoch 187/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 32459515437.9662 - val_loss: 37460495073.2800\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 37428731379.71200\n",
      "Epoch 188/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 32428426166.2720 - val_loss: 37445266735.1040\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 37428731379.71200\n",
      "Epoch 189/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 32415488225.7351 - val_loss: 37419267293.1840\n",
      "\n",
      "Epoch 00189: val_loss improved from 37428731379.71200 to 37419267293.18400, saving model to best_model.h5\n",
      "Epoch 190/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 32401736013.1413 - val_loss: 37395112722.4320\n",
      "\n",
      "Epoch 00190: val_loss improved from 37419267293.18400 to 37395112722.43200, saving model to best_model.h5\n",
      "Epoch 191/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 32382055048.0782 - val_loss: 37396381433.8560\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 37395112722.43200\n",
      "Epoch 192/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 32367028943.0756 - val_loss: 37372049457.1520\n",
      "\n",
      "Epoch 00192: val_loss improved from 37395112722.43200 to 37372049457.15200, saving model to best_model.h5\n",
      "Epoch 193/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 32348244435.8542 - val_loss: 37340409069.5680\n",
      "\n",
      "Epoch 00193: val_loss improved from 37372049457.15200 to 37340409069.56800, saving model to best_model.h5\n",
      "Epoch 194/400\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 32334587252.2809 - val_loss: 37352293957.6320\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 37340409069.56800\n",
      "Epoch 195/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 32309357840.1564 - val_loss: 37316448616.4480\n",
      "\n",
      "Epoch 00195: val_loss improved from 37340409069.56800 to 37316448616.44800, saving model to best_model.h5\n",
      "Epoch 196/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 32297495629.3689 - val_loss: 37289642000.3840\n",
      "\n",
      "Epoch 00196: val_loss improved from 37316448616.44800 to 37289642000.38400, saving model to best_model.h5\n",
      "Epoch 197/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 32276998524.4729 - val_loss: 37288914550.7840\n",
      "\n",
      "Epoch 00197: val_loss improved from 37289642000.38400 to 37288914550.78400, saving model to best_model.h5\n",
      "Epoch 198/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 32264187106.6453 - val_loss: 37284457676.8000\n",
      "\n",
      "Epoch 00198: val_loss improved from 37288914550.78400 to 37284457676.80000, saving model to best_model.h5\n",
      "Epoch 199/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 32245145574.5138 - val_loss: 37217404649.4720\n",
      "\n",
      "Epoch 00199: val_loss improved from 37284457676.80000 to 37217404649.47200, saving model to best_model.h5\n",
      "Epoch 200/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 32229146667.6907 - val_loss: 37223668711.4240\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 37217404649.47200\n",
      "Epoch 201/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 32218305704.3911 - val_loss: 37222694715.3920\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 37217404649.47200\n",
      "Epoch 202/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 32197559502.6204 - val_loss: 37184812810.2400\n",
      "\n",
      "Epoch 00202: val_loss improved from 37217404649.47200 to 37184812810.24000, saving model to best_model.h5\n",
      "Epoch 203/400\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 32177727078.4000 - val_loss: 37198403403.7760\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 37184812810.24000\n",
      "Epoch 204/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 32168662237.1840 - val_loss: 37165747732.4800\n",
      "\n",
      "Epoch 00204: val_loss improved from 37184812810.24000 to 37165747732.48000, saving model to best_model.h5\n",
      "Epoch 205/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 32151417207.0116 - val_loss: 37141246738.4320\n",
      "\n",
      "Epoch 00205: val_loss improved from 37165747732.48000 to 37141246738.43200, saving model to best_model.h5\n",
      "Epoch 206/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 32129678367.8578 - val_loss: 37141981790.2080\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 37141246738.43200\n",
      "Epoch 207/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 32116178041.0596 - val_loss: 37134686453.7600\n",
      "\n",
      "Epoch 00207: val_loss improved from 37141246738.43200 to 37134686453.76000, saving model to best_model.h5\n",
      "Epoch 208/400\n",
      "18000/18000 [==============================] - 1s 64us/step - loss: 32096660937.8418 - val_loss: 37101655097.3440\n",
      "\n",
      "Epoch 00208: val_loss improved from 37134686453.76000 to 37101655097.34400, saving model to best_model.h5\n",
      "Epoch 209/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 32091785536.3982 - val_loss: 37086656135.1680\n",
      "\n",
      "Epoch 00209: val_loss improved from 37101655097.34400 to 37086656135.16800, saving model to best_model.h5\n",
      "Epoch 210/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 32068059618.4178 - val_loss: 37059672178.6880\n",
      "\n",
      "Epoch 00210: val_loss improved from 37086656135.16800 to 37059672178.68800, saving model to best_model.h5\n",
      "Epoch 211/400\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 32055697000.2204 - val_loss: 37083339522.0480\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 37059672178.68800\n",
      "Epoch 212/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 32046330178.2187 - val_loss: 37048108417.0240\n",
      "\n",
      "Epoch 00212: val_loss improved from 37059672178.68800 to 37048108417.02400, saving model to best_model.h5\n",
      "Epoch 213/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 32022041417.0453 - val_loss: 37044407304.1920\n",
      "\n",
      "Epoch 00213: val_loss improved from 37048108417.02400 to 37044407304.19200, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 32003748646.4569 - val_loss: 37021387816.9600\n",
      "\n",
      "Epoch 00214: val_loss improved from 37044407304.19200 to 37021387816.96000, saving model to best_model.h5\n",
      "Epoch 215/400\n",
      "18000/18000 [==============================] - 1s 75us/step - loss: 31999111532.0889 - val_loss: 37007264350.2080\n",
      "\n",
      "Epoch 00215: val_loss improved from 37021387816.96000 to 37007264350.20800, saving model to best_model.h5\n",
      "Epoch 216/400\n",
      "18000/18000 [==============================] - 2s 84us/step - loss: 31977107976.6471 - val_loss: 36994526281.7280\n",
      "\n",
      "Epoch 00216: val_loss improved from 37007264350.20800 to 36994526281.72800, saving model to best_model.h5\n",
      "Epoch 217/400\n",
      "18000/18000 [==============================] - 1s 78us/step - loss: 31959212053.8453 - val_loss: 36972025118.7200\n",
      "\n",
      "Epoch 00217: val_loss improved from 36994526281.72800 to 36972025118.72000, saving model to best_model.h5\n",
      "Epoch 218/400\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 31954721705.5289 - val_loss: 36935423000.5760\n",
      "\n",
      "Epoch 00218: val_loss improved from 36972025118.72000 to 36935423000.57600, saving model to best_model.h5\n",
      "Epoch 219/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 31950323566.3644 - val_loss: 36941218086.9120\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 36935423000.57600\n",
      "Epoch 220/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 31939434417.7209 - val_loss: 36939945443.3280\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 36935423000.57600\n",
      "Epoch 221/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 31906310677.3902 - val_loss: 36920803098.6240\n",
      "\n",
      "Epoch 00221: val_loss improved from 36935423000.57600 to 36920803098.62400, saving model to best_model.h5\n",
      "Epoch 222/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 31881853155.5556 - val_loss: 36892125331.4560\n",
      "\n",
      "Epoch 00222: val_loss improved from 36920803098.62400 to 36892125331.45600, saving model to best_model.h5\n",
      "Epoch 223/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 31880552184.0356 - val_loss: 36838996180.9920\n",
      "\n",
      "Epoch 00223: val_loss improved from 36892125331.45600 to 36838996180.99200, saving model to best_model.h5\n",
      "Epoch 224/400\n",
      "18000/18000 [==============================] - 1s 60us/step - loss: 31862093484.4871 - val_loss: 36860079603.7120\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 36838996180.99200\n",
      "Epoch 225/400\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 31843552403.4560 - val_loss: 36859384856.5760\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 36838996180.99200\n",
      "Epoch 226/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 31824666460.1600 - val_loss: 36862026219.5200\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 36838996180.99200\n",
      "Epoch 227/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 31816235497.6996 - val_loss: 36823217733.6320\n",
      "\n",
      "Epoch 00227: val_loss improved from 36838996180.99200 to 36823217733.63200, saving model to best_model.h5\n",
      "Epoch 228/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 31806031262.1511 - val_loss: 36850191958.0160\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 36823217733.63200\n",
      "Epoch 229/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 31787117929.3582 - val_loss: 36816547053.5680\n",
      "\n",
      "Epoch 00229: val_loss improved from 36823217733.63200 to 36816547053.56800, saving model to best_model.h5\n",
      "Epoch 230/400\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 31766114152.9031 - val_loss: 36814727610.3680\n",
      "\n",
      "Epoch 00230: val_loss improved from 36816547053.56800 to 36814727610.36800, saving model to best_model.h5\n",
      "Epoch 231/400\n",
      "18000/18000 [==============================] - 1s 64us/step - loss: 31757652838.1724 - val_loss: 36765158473.7280\n",
      "\n",
      "Epoch 00231: val_loss improved from 36814727610.36800 to 36765158473.72800, saving model to best_model.h5\n",
      "Epoch 232/400\n",
      "18000/18000 [==============================] - 1s 59us/step - loss: 31740605198.7911 - val_loss: 36747704860.6720\n",
      "\n",
      "Epoch 00232: val_loss improved from 36765158473.72800 to 36747704860.67200, saving model to best_model.h5\n",
      "Epoch 233/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 31731440579.0151 - val_loss: 36745336356.8640\n",
      "\n",
      "Epoch 00233: val_loss improved from 36747704860.67200 to 36745336356.86400, saving model to best_model.h5\n",
      "Epoch 234/400\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 31713387362.5316 - val_loss: 36743014580.2240\n",
      "\n",
      "Epoch 00234: val_loss improved from 36745336356.86400 to 36743014580.22400, saving model to best_model.h5\n",
      "Epoch 235/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 31699588863.3173 - val_loss: 36732203204.6080\n",
      "\n",
      "Epoch 00235: val_loss improved from 36743014580.22400 to 36732203204.60800, saving model to best_model.h5\n",
      "Epoch 236/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 31678410482.5742 - val_loss: 36681068740.6080\n",
      "\n",
      "Epoch 00236: val_loss improved from 36732203204.60800 to 36681068740.60800, saving model to best_model.h5\n",
      "Epoch 237/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 31670218509.8809 - val_loss: 36676168122.3680\n",
      "\n",
      "Epoch 00237: val_loss improved from 36681068740.60800 to 36676168122.36800, saving model to best_model.h5\n",
      "Epoch 238/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 31655184072.7040 - val_loss: 36661178466.3040\n",
      "\n",
      "Epoch 00238: val_loss improved from 36676168122.36800 to 36661178466.30400, saving model to best_model.h5\n",
      "Epoch 239/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 31631932710.0018 - val_loss: 36656254484.4800\n",
      "\n",
      "Epoch 00239: val_loss improved from 36661178466.30400 to 36656254484.48000, saving model to best_model.h5\n",
      "Epoch 240/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 31626633656.5476 - val_loss: 36629744254.9760\n",
      "\n",
      "Epoch 00240: val_loss improved from 36656254484.48000 to 36629744254.97600, saving model to best_model.h5\n",
      "Epoch 241/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 31605153353.2729 - val_loss: 36621814005.7600\n",
      "\n",
      "Epoch 00241: val_loss improved from 36629744254.97600 to 36621814005.76000, saving model to best_model.h5\n",
      "Epoch 242/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 31605823591.7653 - val_loss: 36633182863.3600\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 36621814005.76000\n",
      "Epoch 243/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 31590333273.4293 - val_loss: 36614702694.4000\n",
      "\n",
      "Epoch 00243: val_loss improved from 36621814005.76000 to 36614702694.40000, saving model to best_model.h5\n",
      "Epoch 244/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 31571505446.9120 - val_loss: 36618101948.4160\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 36614702694.40000\n",
      "Epoch 245/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 31558577040.0427 - val_loss: 36592116924.4160\n",
      "\n",
      "Epoch 00245: val_loss improved from 36614702694.40000 to 36592116924.41600, saving model to best_model.h5\n",
      "Epoch 246/400\n",
      "18000/18000 [==============================] - 1s 39us/step - loss: 31544750859.1502 - val_loss: 36586302242.8160\n",
      "\n",
      "Epoch 00246: val_loss improved from 36592116924.41600 to 36586302242.81600, saving model to best_model.h5\n",
      "Epoch 247/400\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 31525130673.2658 - val_loss: 36547147399.1680\n",
      "\n",
      "Epoch 00247: val_loss improved from 36586302242.81600 to 36547147399.16800, saving model to best_model.h5\n",
      "Epoch 248/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 31512735372.6293 - val_loss: 36526568865.7920\n",
      "\n",
      "Epoch 00248: val_loss improved from 36547147399.16800 to 36526568865.79200, saving model to best_model.h5\n",
      "Epoch 249/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 31500005751.0116 - val_loss: 36523007213.5680\n",
      "\n",
      "Epoch 00249: val_loss improved from 36526568865.79200 to 36523007213.56800, saving model to best_model.h5\n",
      "Epoch 250/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 31489480582.0302 - val_loss: 36488400338.9440\n",
      "\n",
      "Epoch 00250: val_loss improved from 36523007213.56800 to 36488400338.94400, saving model to best_model.h5\n",
      "Epoch 251/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 31462804297.0453 - val_loss: 36496707354.6240\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 36488400338.94400\n",
      "Epoch 252/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 31456202369.7067 - val_loss: 36490625253.3760\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 36488400338.94400\n",
      "Epoch 253/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 31436065061.0916 - val_loss: 36462682800.1280\n",
      "\n",
      "Epoch 00253: val_loss improved from 36488400338.94400 to 36462682800.12800, saving model to best_model.h5\n",
      "Epoch 254/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 31425151979.0649 - val_loss: 36441182273.5360\n",
      "\n",
      "Epoch 00254: val_loss improved from 36462682800.12800 to 36441182273.53600, saving model to best_model.h5\n",
      "Epoch 255/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 31407648087.1538 - val_loss: 36433441849.3440\n",
      "\n",
      "Epoch 00255: val_loss improved from 36441182273.53600 to 36433441849.34400, saving model to best_model.h5\n",
      "Epoch 256/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 31400567377.4649 - val_loss: 36424686927.8720\n",
      "\n",
      "Epoch 00256: val_loss improved from 36433441849.34400 to 36424686927.87200, saving model to best_model.h5\n",
      "Epoch 257/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 31386028430.6773 - val_loss: 36445117612.0320\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 36424686927.87200\n",
      "Epoch 258/400\n",
      "18000/18000 [==============================] - 1s 66us/step - loss: 31366815514.6240 - val_loss: 36394383769.6000\n",
      "\n",
      "Epoch 00258: val_loss improved from 36424686927.87200 to 36394383769.60000, saving model to best_model.h5\n",
      "Epoch 259/400\n",
      "18000/18000 [==============================] - 1s 74us/step - loss: 31379215322.6809 - val_loss: 36365538295.8080\n",
      "\n",
      "Epoch 00259: val_loss improved from 36394383769.60000 to 36365538295.80800, saving model to best_model.h5\n",
      "Epoch 260/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 31340146032.6400 - val_loss: 36365515489.2800\n",
      "\n",
      "Epoch 00260: val_loss improved from 36365538295.80800 to 36365515489.28000, saving model to best_model.h5\n",
      "Epoch 261/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 31331032241.4933 - val_loss: 36358770458.6240\n",
      "\n",
      "Epoch 00261: val_loss improved from 36365515489.28000 to 36358770458.62400, saving model to best_model.h5\n",
      "Epoch 262/400\n",
      "18000/18000 [==============================] - 1s 59us/step - loss: 31317652830.4356 - val_loss: 36330879451.1360\n",
      "\n",
      "Epoch 00262: val_loss improved from 36358770458.62400 to 36330879451.13600, saving model to best_model.h5\n",
      "Epoch 263/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 31296119976.3911 - val_loss: 36335805988.8640\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 36330879451.13600\n",
      "Epoch 264/400\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 31283491154.6027 - val_loss: 36322100805.6320\n",
      "\n",
      "Epoch 00264: val_loss improved from 36330879451.13600 to 36322100805.63200, saving model to best_model.h5\n",
      "Epoch 265/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 31282344416.5973 - val_loss: 36300193202.1760\n",
      "\n",
      "Epoch 00265: val_loss improved from 36322100805.63200 to 36300193202.17600, saving model to best_model.h5\n",
      "Epoch 266/400\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 31266257429.3902 - val_loss: 36271672885.2480\n",
      "\n",
      "Epoch 00266: val_loss improved from 36300193202.17600 to 36271672885.24800, saving model to best_model.h5\n",
      "Epoch 267/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 31250019778.5600 - val_loss: 36256078069.7600\n",
      "\n",
      "Epoch 00267: val_loss improved from 36271672885.24800 to 36256078069.76000, saving model to best_model.h5\n",
      "Epoch 268/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 31232010440.2489 - val_loss: 36266413162.4960\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 36256078069.76000\n",
      "Epoch 269/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 31216689428.7076 - val_loss: 36263435632.6400\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 36256078069.76000\n",
      "Epoch 270/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 31206588874.7520 - val_loss: 36269499285.5040\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 36256078069.76000\n",
      "Epoch 271/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 31205506482.1760 - val_loss: 36213286240.2560\n",
      "\n",
      "Epoch 00271: val_loss improved from 36256078069.76000 to 36213286240.25600, saving model to best_model.h5\n",
      "Epoch 272/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 31182315215.9858 - val_loss: 36219400650.7520\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 36213286240.25600\n",
      "Epoch 273/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 31173912250.1404 - val_loss: 36206979973.1200\n",
      "\n",
      "Epoch 00273: val_loss improved from 36213286240.25600 to 36206979973.12000, saving model to best_model.h5\n",
      "Epoch 274/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 31166880006.1440 - val_loss: 36170306125.8240\n",
      "\n",
      "Epoch 00274: val_loss improved from 36206979973.12000 to 36170306125.82400, saving model to best_model.h5\n",
      "Epoch 275/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 31146362383.0187 - val_loss: 36195888988.1600\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 36170306125.82400\n",
      "Epoch 276/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 31142428375.2676 - val_loss: 36165664145.4080\n",
      "\n",
      "Epoch 00276: val_loss improved from 36170306125.82400 to 36165664145.40800, saving model to best_model.h5\n",
      "Epoch 277/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 31124615845.2053 - val_loss: 36169707782.1440\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 36165664145.40800\n",
      "Epoch 278/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 31100163336.8747 - val_loss: 36131608035.3280\n",
      "\n",
      "Epoch 00278: val_loss improved from 36165664145.40800 to 36131608035.32800, saving model to best_model.h5\n",
      "Epoch 279/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 31089171908.3804 - val_loss: 36140763807.7440\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 36131608035.32800\n",
      "Epoch 280/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 31079429992.9031 - val_loss: 36141871988.7360\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 36131608035.32800\n",
      "Epoch 281/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 31066960437.2480 - val_loss: 36105098133.5040\n",
      "\n",
      "Epoch 00281: val_loss improved from 36131608035.32800 to 36105098133.50400, saving model to best_model.h5\n",
      "Epoch 282/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 31057614793.3867 - val_loss: 36110745534.4640\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 36105098133.50400\n",
      "Epoch 283/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 31036260210.0053 - val_loss: 36085244133.3760\n",
      "\n",
      "Epoch 00283: val_loss improved from 36105098133.50400 to 36085244133.37600, saving model to best_model.h5\n",
      "Epoch 284/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 31028746780.6720 - val_loss: 36086799663.1040\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 36085244133.37600\n",
      "Epoch 285/400\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 31012319033.5716 - val_loss: 36074516742.1440\n",
      "\n",
      "Epoch 00285: val_loss improved from 36085244133.37600 to 36074516742.14400, saving model to best_model.h5\n",
      "Epoch 286/400\n",
      "18000/18000 [==============================] - 1s 39us/step - loss: 31011528225.2231 - val_loss: 36052159496.1920\n",
      "\n",
      "Epoch 00286: val_loss improved from 36074516742.14400 to 36052159496.19200, saving model to best_model.h5\n",
      "Epoch 287/400\n",
      "18000/18000 [==============================] - 1s 39us/step - loss: 31000685656.2916 - val_loss: 36031472205.8240\n",
      "\n",
      "Epoch 00287: val_loss improved from 36052159496.19200 to 36031472205.82400, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 288/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30988646835.0862 - val_loss: 36013876576.2560\n",
      "\n",
      "Epoch 00288: val_loss improved from 36031472205.82400 to 36013876576.25600, saving model to best_model.h5\n",
      "Epoch 289/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 30969067481.7707 - val_loss: 36004972625.9200\n",
      "\n",
      "Epoch 00289: val_loss improved from 36013876576.25600 to 36004972625.92000, saving model to best_model.h5\n",
      "Epoch 290/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 30956659987.7973 - val_loss: 36003181821.9520\n",
      "\n",
      "Epoch 00290: val_loss improved from 36004972625.92000 to 36003181821.95200, saving model to best_model.h5\n",
      "Epoch 291/400\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 30937490618.5956 - val_loss: 35994889388.0320\n",
      "\n",
      "Epoch 00291: val_loss improved from 36003181821.95200 to 35994889388.03200, saving model to best_model.h5\n",
      "Epoch 292/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 30939410779.7049 - val_loss: 36014300987.3920\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 35994889388.03200\n",
      "Epoch 293/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 30914949549.6249 - val_loss: 35962321829.8880\n",
      "\n",
      "Epoch 00293: val_loss improved from 35994889388.03200 to 35962321829.88800, saving model to best_model.h5\n",
      "Epoch 294/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 30901831532.5440 - val_loss: 35939181592.5760\n",
      "\n",
      "Epoch 00294: val_loss improved from 35962321829.88800 to 35939181592.57600, saving model to best_model.h5\n",
      "Epoch 295/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 30892168495.1040 - val_loss: 35954100535.2960\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 35939181592.57600\n",
      "Epoch 296/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 30879724493.9378 - val_loss: 35933691510.7840\n",
      "\n",
      "Epoch 00296: val_loss improved from 35939181592.57600 to 35933691510.78400, saving model to best_model.h5\n",
      "Epoch 297/400\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 30867505221.1769 - val_loss: 35907763404.8000\n",
      "\n",
      "Epoch 00297: val_loss improved from 35933691510.78400 to 35907763404.80000, saving model to best_model.h5\n",
      "Epoch 298/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 30850268208.2418 - val_loss: 35916411404.2880\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 35907763404.80000\n",
      "Epoch 299/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30840671953.8062 - val_loss: 35895010426.8800\n",
      "\n",
      "Epoch 00299: val_loss improved from 35907763404.80000 to 35895010426.88000, saving model to best_model.h5\n",
      "Epoch 300/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 30832298917.8880 - val_loss: 35905051951.1040\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 35895010426.88000\n",
      "Epoch 301/400\n",
      "18000/18000 [==============================] - 1s 62us/step - loss: 30819702614.6987 - val_loss: 35861322596.3520\n",
      "\n",
      "Epoch 00301: val_loss improved from 35895010426.88000 to 35861322596.35200, saving model to best_model.h5\n",
      "Epoch 302/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 30809013610.2684 - val_loss: 35848004370.4320\n",
      "\n",
      "Epoch 00302: val_loss improved from 35861322596.35200 to 35848004370.43200, saving model to best_model.h5\n",
      "Epoch 303/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 30805141001.5573 - val_loss: 35830478733.3120\n",
      "\n",
      "Epoch 00303: val_loss improved from 35848004370.43200 to 35830478733.31200, saving model to best_model.h5\n",
      "Epoch 304/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 30785445412.8640 - val_loss: 35806505500.6720\n",
      "\n",
      "Epoch 00304: val_loss improved from 35830478733.31200 to 35806505500.67200, saving model to best_model.h5\n",
      "Epoch 305/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 30772118749.1840 - val_loss: 35830345433.0880\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 35806505500.67200\n",
      "Epoch 306/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 30762816565.7031 - val_loss: 35802365263.8720\n",
      "\n",
      "Epoch 00306: val_loss improved from 35806505500.67200 to 35802365263.87200, saving model to best_model.h5\n",
      "Epoch 307/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30748818644.0818 - val_loss: 35788917702.6560\n",
      "\n",
      "Epoch 00307: val_loss improved from 35802365263.87200 to 35788917702.65600, saving model to best_model.h5\n",
      "Epoch 308/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 30740448156.7858 - val_loss: 35777588166.6560\n",
      "\n",
      "Epoch 00308: val_loss improved from 35788917702.65600 to 35777588166.65600, saving model to best_model.h5\n",
      "Epoch 309/400\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 30721635410.8302 - val_loss: 35760634331.1360\n",
      "\n",
      "Epoch 00309: val_loss improved from 35777588166.65600 to 35760634331.13600, saving model to best_model.h5\n",
      "Epoch 310/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 30710855046.4853 - val_loss: 35738324500.4800\n",
      "\n",
      "Epoch 00310: val_loss improved from 35760634331.13600 to 35738324500.48000, saving model to best_model.h5\n",
      "Epoch 311/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 30701429398.6418 - val_loss: 35733868183.5520\n",
      "\n",
      "Epoch 00311: val_loss improved from 35738324500.48000 to 35733868183.55200, saving model to best_model.h5\n",
      "Epoch 312/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 30689093932.3733 - val_loss: 35723935907.8400\n",
      "\n",
      "Epoch 00312: val_loss improved from 35733868183.55200 to 35723935907.84000, saving model to best_model.h5\n",
      "Epoch 313/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30679519679.8293 - val_loss: 35707708276.7360\n",
      "\n",
      "Epoch 00313: val_loss improved from 35723935907.84000 to 35707708276.73600, saving model to best_model.h5\n",
      "Epoch 314/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 30667599049.1591 - val_loss: 35704586960.8960\n",
      "\n",
      "Epoch 00314: val_loss improved from 35707708276.73600 to 35704586960.89600, saving model to best_model.h5\n",
      "Epoch 315/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 30654091562.5529 - val_loss: 35702940434.4320\n",
      "\n",
      "Epoch 00315: val_loss improved from 35704586960.89600 to 35702940434.43200, saving model to best_model.h5\n",
      "Epoch 316/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30635524210.6880 - val_loss: 35690823319.5520\n",
      "\n",
      "Epoch 00316: val_loss improved from 35702940434.43200 to 35690823319.55200, saving model to best_model.h5\n",
      "Epoch 317/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30636343388.8427 - val_loss: 35694366523.3920\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 35690823319.55200\n",
      "Epoch 318/400\n",
      "18000/18000 [==============================] - 1s 39us/step - loss: 30618975629.7671 - val_loss: 35675014496.2560\n",
      "\n",
      "Epoch 00318: val_loss improved from 35690823319.55200 to 35675014496.25600, saving model to best_model.h5\n",
      "Epoch 319/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30608550168.3484 - val_loss: 35657420210.1760\n",
      "\n",
      "Epoch 00319: val_loss improved from 35675014496.25600 to 35657420210.17600, saving model to best_model.h5\n",
      "Epoch 320/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 30592677988.1244 - val_loss: 35644005908.4800\n",
      "\n",
      "Epoch 00320: val_loss improved from 35657420210.17600 to 35644005908.48000, saving model to best_model.h5\n",
      "Epoch 321/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 30594313173.2196 - val_loss: 35636143489.0240\n",
      "\n",
      "Epoch 00321: val_loss improved from 35644005908.48000 to 35636143489.02400, saving model to best_model.h5\n",
      "Epoch 322/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30572102301.9236 - val_loss: 35627690917.8880\n",
      "\n",
      "Epoch 00322: val_loss improved from 35636143489.02400 to 35627690917.88800, saving model to best_model.h5\n",
      "Epoch 323/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 30564800866.9867 - val_loss: 35587345711.1040\n",
      "\n",
      "Epoch 00323: val_loss improved from 35627690917.88800 to 35587345711.10400, saving model to best_model.h5\n",
      "Epoch 324/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 47us/step - loss: 30554882730.6667 - val_loss: 35575017570.3040\n",
      "\n",
      "Epoch 00324: val_loss improved from 35587345711.10400 to 35575017570.30400, saving model to best_model.h5\n",
      "Epoch 325/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30537830861.4827 - val_loss: 35585016168.4480\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 35575017570.30400\n",
      "Epoch 326/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 30530092886.6987 - val_loss: 35569878368.2560\n",
      "\n",
      "Epoch 00326: val_loss improved from 35575017570.30400 to 35569878368.25600, saving model to best_model.h5\n",
      "Epoch 327/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30516127121.4080 - val_loss: 35591999946.7520\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 35569878368.25600\n",
      "Epoch 328/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30514390080.6258 - val_loss: 35568368189.4400\n",
      "\n",
      "Epoch 00328: val_loss improved from 35569878368.25600 to 35568368189.44000, saving model to best_model.h5\n",
      "Epoch 329/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30495993373.5822 - val_loss: 35548152397.8240\n",
      "\n",
      "Epoch 00329: val_loss improved from 35568368189.44000 to 35548152397.82400, saving model to best_model.h5\n",
      "Epoch 330/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30476560902.8267 - val_loss: 35516302622.7200\n",
      "\n",
      "Epoch 00330: val_loss improved from 35548152397.82400 to 35516302622.72000, saving model to best_model.h5\n",
      "Epoch 331/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30481813835.3209 - val_loss: 35518094409.7280\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 35516302622.72000\n",
      "Epoch 332/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30461238891.8613 - val_loss: 35499839619.0720\n",
      "\n",
      "Epoch 00332: val_loss improved from 35516302622.72000 to 35499839619.07200, saving model to best_model.h5\n",
      "Epoch 333/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30456196093.2693 - val_loss: 35514405355.5200\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 35499839619.07200\n",
      "Epoch 334/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30442760532.4231 - val_loss: 35488580304.8960\n",
      "\n",
      "Epoch 00334: val_loss improved from 35499839619.07200 to 35488580304.89600, saving model to best_model.h5\n",
      "Epoch 335/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30434869300.7929 - val_loss: 35477035843.5840\n",
      "\n",
      "Epoch 00335: val_loss improved from 35488580304.89600 to 35477035843.58400, saving model to best_model.h5\n",
      "Epoch 336/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30420854454.4996 - val_loss: 35459008954.3680\n",
      "\n",
      "Epoch 00336: val_loss improved from 35477035843.58400 to 35459008954.36800, saving model to best_model.h5\n",
      "Epoch 337/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30399696222.4356 - val_loss: 35463389085.6960\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 35459008954.36800\n",
      "Epoch 338/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30391954442.0124 - val_loss: 35453361586.1760\n",
      "\n",
      "Epoch 00338: val_loss improved from 35459008954.36800 to 35453361586.17600, saving model to best_model.h5\n",
      "Epoch 339/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30379755849.5004 - val_loss: 35449332629.5040\n",
      "\n",
      "Epoch 00339: val_loss improved from 35453361586.17600 to 35449332629.50400, saving model to best_model.h5\n",
      "Epoch 340/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 30376568284.0462 - val_loss: 35441894653.9520\n",
      "\n",
      "Epoch 00340: val_loss improved from 35449332629.50400 to 35441894653.95200, saving model to best_model.h5\n",
      "Epoch 341/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30361936330.7520 - val_loss: 35407701311.4880\n",
      "\n",
      "Epoch 00341: val_loss improved from 35441894653.95200 to 35407701311.48800, saving model to best_model.h5\n",
      "Epoch 342/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30345900430.6773 - val_loss: 35428090478.5920\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 35407701311.48800\n",
      "Epoch 343/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 30344359695.7013 - val_loss: 35402642227.2000\n",
      "\n",
      "Epoch 00343: val_loss improved from 35407701311.48800 to 35402642227.20000, saving model to best_model.h5\n",
      "Epoch 344/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 30327811524.3804 - val_loss: 35374384185.3440\n",
      "\n",
      "Epoch 00344: val_loss improved from 35402642227.20000 to 35374384185.34400, saving model to best_model.h5\n",
      "Epoch 345/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 30316676794.1404 - val_loss: 35368942305.2800\n",
      "\n",
      "Epoch 00345: val_loss improved from 35374384185.34400 to 35368942305.28000, saving model to best_model.h5\n",
      "Epoch 346/400\n",
      "18000/18000 [==============================] - 1s 61us/step - loss: 30312112291.8400 - val_loss: 35348974665.7280\n",
      "\n",
      "Epoch 00346: val_loss improved from 35368942305.28000 to 35348974665.72800, saving model to best_model.h5\n",
      "Epoch 347/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 30298328842.2400 - val_loss: 35351982080.0000\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 35348974665.72800\n",
      "Epoch 348/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 30296980094.9760 - val_loss: 35341099106.3040\n",
      "\n",
      "Epoch 00348: val_loss improved from 35348974665.72800 to 35341099106.30400, saving model to best_model.h5\n",
      "Epoch 349/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 30282037792.3129 - val_loss: 35364069900.2880\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 35341099106.30400\n",
      "Epoch 350/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 30267946607.5022 - val_loss: 35333255462.9120\n",
      "\n",
      "Epoch 00350: val_loss improved from 35341099106.30400 to 35333255462.91200, saving model to best_model.h5\n",
      "Epoch 351/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30256030629.8880 - val_loss: 35307365564.4160\n",
      "\n",
      "Epoch 00351: val_loss improved from 35333255462.91200 to 35307365564.41600, saving model to best_model.h5\n",
      "Epoch 352/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 30253091873.6782 - val_loss: 35293578330.1120\n",
      "\n",
      "Epoch 00352: val_loss improved from 35307365564.41600 to 35293578330.11200, saving model to best_model.h5\n",
      "Epoch 353/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 30231672768.2844 - val_loss: 35283245531.1360\n",
      "\n",
      "Epoch 00353: val_loss improved from 35293578330.11200 to 35283245531.13600, saving model to best_model.h5\n",
      "Epoch 354/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30219834137.7138 - val_loss: 35283831390.2080\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 35283245531.13600\n",
      "Epoch 355/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 30210993047.3244 - val_loss: 35273904947.2000\n",
      "\n",
      "Epoch 00355: val_loss improved from 35283245531.13600 to 35273904947.20000, saving model to best_model.h5\n",
      "Epoch 356/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 30208400787.2284 - val_loss: 35264711294.9760\n",
      "\n",
      "Epoch 00356: val_loss improved from 35273904947.20000 to 35264711294.97600, saving model to best_model.h5\n",
      "Epoch 357/400\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 30192333793.9627 - val_loss: 35246008893.4400\n",
      "\n",
      "Epoch 00357: val_loss improved from 35264711294.97600 to 35246008893.44000, saving model to best_model.h5\n",
      "Epoch 358/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30191091189.5324 - val_loss: 35216056975.3600\n",
      "\n",
      "Epoch 00358: val_loss improved from 35246008893.44000 to 35216056975.36000, saving model to best_model.h5\n",
      "Epoch 359/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 30176771710.9760 - val_loss: 35206169985.0240\n",
      "\n",
      "Epoch 00359: val_loss improved from 35216056975.36000 to 35206169985.02400, saving model to best_model.h5\n",
      "Epoch 360/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30162339958.3289 - val_loss: 35199309119.4880\n",
      "\n",
      "Epoch 00360: val_loss improved from 35206169985.02400 to 35199309119.48800, saving model to best_model.h5\n",
      "Epoch 361/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 43us/step - loss: 30152967640.4053 - val_loss: 35193901350.9120\n",
      "\n",
      "Epoch 00361: val_loss improved from 35199309119.48800 to 35193901350.91200, saving model to best_model.h5\n",
      "Epoch 362/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30139642995.5982 - val_loss: 35197942824.9600\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 35193901350.91200\n",
      "Epoch 363/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30128694460.4160 - val_loss: 35189572599.8080\n",
      "\n",
      "Epoch 00363: val_loss improved from 35193901350.91200 to 35189572599.80800, saving model to best_model.h5\n",
      "Epoch 364/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30123290600.3342 - val_loss: 35177944580.0960\n",
      "\n",
      "Epoch 00364: val_loss improved from 35189572599.80800 to 35177944580.09600, saving model to best_model.h5\n",
      "Epoch 365/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30118028900.5796 - val_loss: 35179561418.7520\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 35177944580.09600\n",
      "Epoch 366/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30099817840.6400 - val_loss: 35147152588.8000\n",
      "\n",
      "Epoch 00366: val_loss improved from 35177944580.09600 to 35147152588.80000, saving model to best_model.h5\n",
      "Epoch 367/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30101348291.9253 - val_loss: 35153502502.9120\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 35147152588.80000\n",
      "Epoch 368/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 30086162189.8809 - val_loss: 35156997111.8080\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 35147152588.80000\n",
      "Epoch 369/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30085005824.4551 - val_loss: 35126371090.4320\n",
      "\n",
      "Epoch 00369: val_loss improved from 35147152588.80000 to 35126371090.43200, saving model to best_model.h5\n",
      "Epoch 370/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30063486551.8364 - val_loss: 35130953367.5520\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 35126371090.43200\n",
      "Epoch 371/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30047530849.6213 - val_loss: 35087920594.9440\n",
      "\n",
      "Epoch 00371: val_loss improved from 35126371090.43200 to 35087920594.94400, saving model to best_model.h5\n",
      "Epoch 372/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30048522400.1991 - val_loss: 35088450650.1120\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 35087920594.94400\n",
      "Epoch 373/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 30051494130.1191 - val_loss: 35059838222.3360\n",
      "\n",
      "Epoch 00373: val_loss improved from 35087920594.94400 to 35059838222.33600, saving model to best_model.h5\n",
      "Epoch 374/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30025816741.2053 - val_loss: 35052700401.6640\n",
      "\n",
      "Epoch 00374: val_loss improved from 35059838222.33600 to 35052700401.66400, saving model to best_model.h5\n",
      "Epoch 375/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30011144670.7769 - val_loss: 35059193348.0960\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 35052700401.66400\n",
      "Epoch 376/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 30001709564.8142 - val_loss: 35045699944.4480\n",
      "\n",
      "Epoch 00376: val_loss improved from 35052700401.66400 to 35045699944.44800, saving model to best_model.h5\n",
      "Epoch 377/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 29994223519.5164 - val_loss: 35032854003.7120\n",
      "\n",
      "Epoch 00377: val_loss improved from 35045699944.44800 to 35032854003.71200, saving model to best_model.h5\n",
      "Epoch 378/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 29985945174.0160 - val_loss: 35029241495.5520\n",
      "\n",
      "Epoch 00378: val_loss improved from 35032854003.71200 to 35029241495.55200, saving model to best_model.h5\n",
      "Epoch 379/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 29985638463.7156 - val_loss: 35025131110.4000\n",
      "\n",
      "Epoch 00379: val_loss improved from 35029241495.55200 to 35025131110.40000, saving model to best_model.h5\n",
      "Epoch 380/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 29968314204.1600 - val_loss: 35003656110.0800\n",
      "\n",
      "Epoch 00380: val_loss improved from 35025131110.40000 to 35003656110.08000, saving model to best_model.h5\n",
      "Epoch 381/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 29969023465.6996 - val_loss: 34983852834.8160\n",
      "\n",
      "Epoch 00381: val_loss improved from 35003656110.08000 to 34983852834.81600, saving model to best_model.h5\n",
      "Epoch 382/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 29949909381.5751 - val_loss: 34972298936.3200\n",
      "\n",
      "Epoch 00382: val_loss improved from 34983852834.81600 to 34972298936.32000, saving model to best_model.h5\n",
      "Epoch 383/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 29938800787.4560 - val_loss: 34982526484.4800\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 34972298936.32000\n",
      "Epoch 384/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 29930644444.5013 - val_loss: 34969992364.0320\n",
      "\n",
      "Epoch 00384: val_loss improved from 34972298936.32000 to 34969992364.03200, saving model to best_model.h5\n",
      "Epoch 385/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 29926263983.6729 - val_loss: 34951936507.9040\n",
      "\n",
      "Epoch 00385: val_loss improved from 34969992364.03200 to 34951936507.90400, saving model to best_model.h5\n",
      "Epoch 386/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 29913018054.8836 - val_loss: 34954905485.3120\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 34951936507.90400\n",
      "Epoch 387/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 29901476444.3876 - val_loss: 34941263675.3920\n",
      "\n",
      "Epoch 00387: val_loss improved from 34951936507.90400 to 34941263675.39200, saving model to best_model.h5\n",
      "Epoch 388/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 29894713626.1689 - val_loss: 34931351224.3200\n",
      "\n",
      "Epoch 00388: val_loss improved from 34941263675.39200 to 34931351224.32000, saving model to best_model.h5\n",
      "Epoch 389/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 29892421113.1733 - val_loss: 34915141091.3280\n",
      "\n",
      "Epoch 00389: val_loss improved from 34931351224.32000 to 34915141091.32800, saving model to best_model.h5\n",
      "Epoch 390/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 29875464767.2604 - val_loss: 34929495441.4080\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 34915141091.32800\n",
      "Epoch 391/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 29880819313.3227 - val_loss: 34900234731.5200\n",
      "\n",
      "Epoch 00391: val_loss improved from 34915141091.32800 to 34900234731.52000, saving model to best_model.h5\n",
      "Epoch 392/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 29861296963.5840 - val_loss: 34897185505.2800\n",
      "\n",
      "Epoch 00392: val_loss improved from 34900234731.52000 to 34897185505.28000, saving model to best_model.h5\n",
      "Epoch 393/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 29851920405.8453 - val_loss: 34903928111.1040\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 34897185505.28000\n",
      "Epoch 394/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 29837986234.3680 - val_loss: 34894097350.6560\n",
      "\n",
      "Epoch 00394: val_loss improved from 34897185505.28000 to 34894097350.65600, saving model to best_model.h5\n",
      "Epoch 395/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 29828995596.2880 - val_loss: 34874919124.9920\n",
      "\n",
      "Epoch 00395: val_loss improved from 34894097350.65600 to 34874919124.99200, saving model to best_model.h5\n",
      "Epoch 396/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 29829302089.9556 - val_loss: 34860350537.7280\n",
      "\n",
      "Epoch 00396: val_loss improved from 34874919124.99200 to 34860350537.72800, saving model to best_model.h5\n",
      "Epoch 397/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 29809017804.1173 - val_loss: 34849582579.7120\n",
      "\n",
      "Epoch 00397: val_loss improved from 34860350537.72800 to 34849582579.71200, saving model to best_model.h5\n",
      "Epoch 398/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 48us/step - loss: 29800239515.4204 - val_loss: 34851138633.7280\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 34849582579.71200\n",
      "Epoch 399/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 29814908483.8116 - val_loss: 34851638149.1200\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 34849582579.71200\n",
      "Epoch 400/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 29797488394.2400 - val_loss: 34813784293.3760\n",
      "\n",
      "Epoch 00400: val_loss improved from 34849582579.71200 to 34813784293.37600, saving model to best_model.h5\n",
      "Model score: 0.6566940217993732\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set callback functions to early stop training and save the \n",
    "# best model so far\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=5),\n",
    "            ModelCheckpoint(filepath='best_model.h5',\n",
    "                            monitor='val_loss',\n",
    "                            save_best_only=True,\n",
    "                           verbose=1)]\n",
    "\n",
    "model = nn_model(layers=[20,20,20])\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "history = model.fit(selected_feature_train, price_train,\n",
    "            epochs=400, callbacks=callbacks, batch_size=128,\n",
    "            validation_data=(selected_feature_val, price_val))\n",
    "\n",
    "model_score = score(model.predict(selected_feature_val), price_val)\n",
    "print(f\"Model score: {model_score}\")\n",
    "plot_loss(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You don't have to remember these resources because they're here when you need them\n",
    "https://www.tensorflow.org/api_docs\n",
    "\n",
    "https://keras.io/\n",
    "\n",
    "https://www.tensorflow.org/tutorials/\n",
    "\n",
    "https://www.google.com\n",
    "\n",
    "### Don't trust me, trust your validation errors\n",
    "### Don't look at your test set until you're actually going to test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
