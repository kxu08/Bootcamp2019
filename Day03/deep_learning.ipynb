{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow & Keras - Basics of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most importantly... resources\n",
    "\n",
    "https://www.tensorflow.org/api_docs\n",
    "\n",
    "https://keras.io/\n",
    "\n",
    "https://www.tensorflow.org/tutorials/\n",
    "\n",
    "https://www.google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF overview\n",
    "\n",
    "* #### \"End-to-end machine learning platform\" \n",
    "\n",
    "    - Not the only one! Check out PyTorch, Theano, Cognitive Toolkit.\n",
    "   \n",
    "* #### Integrates with high-level APIs like Keras\n",
    "* #### Plays nice with Pandas\n",
    "* #### Makes deep learning *fast* and *easy* *\n",
    "    *<sup>\"easy\"</sup>\n",
    "\n",
    "## Tasks for TensorFlow:\n",
    "\n",
    "* #### Regression\n",
    "    - Predict house prices\n",
    "    - Predict drug metabolic rates\n",
    "    - Predict stock trends *\n",
    "    \n",
    "    *<sup>this is super hard</sup>\n",
    "    \n",
    "    \n",
    "\n",
    "* #### Classification\n",
    "    - Cat or dog?\n",
    "    - Malignant or benign cancer from images\n",
    "    ![](media/dr.png)\n",
    "    <span style=\"font-size:0.75em;\">Google AI Blog: Diabetic Retinopathy</span>\n",
    "\n",
    "\n",
    "\n",
    "* #### Dimensionality reduction\n",
    "    - Visualize high-dimensional data in 2 or 3-D space\n",
    "    - Compress representations for successive ML\n",
    "\n",
    "\n",
    "\n",
    "* #### Generative models\n",
    "    - Create new molecules with desirable properties\n",
    "    - Artificially enhance image resolution\n",
    "    ![](media/molecular_gan.png)\n",
    "    <span style=\"font-size:0.75em;\">Kadurin et al., 2017</span>\n",
    "\n",
    "\n",
    "* #### Reinforcement learning\n",
    "    - Can't beat your friends at chess? Make your computer do it\n",
    "\n",
    "\n",
    "\n",
    "* #### Much more...\n",
    "    - Generic math\n",
    "    - Probabilistic programming with TFP\n",
    "    - Automatic differentiation\n",
    "    - ...\n",
    "\n",
    "\n",
    "## Let's Regress\n",
    "\n",
    "### Imports!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name a more iconic duo, I'll wait\n",
    "\n",
    "#### New imports -- TF and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check our versions for good measure -- these programs may have very different behavior version-to-version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading in housing data as with SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7237550310</td>\n",
       "      <td>20140512T000000</td>\n",
       "      <td>1225000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.50</td>\n",
       "      <td>5420</td>\n",
       "      <td>101930</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>3890</td>\n",
       "      <td>1530</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>98053</td>\n",
       "      <td>47.6561</td>\n",
       "      <td>-122.005</td>\n",
       "      <td>4760</td>\n",
       "      <td>101930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1321400060</td>\n",
       "      <td>20140627T000000</td>\n",
       "      <td>257500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1715</td>\n",
       "      <td>6819</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1715</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>98003</td>\n",
       "      <td>47.3097</td>\n",
       "      <td>-122.327</td>\n",
       "      <td>2238</td>\n",
       "      <td>6819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2008000270</td>\n",
       "      <td>20150115T000000</td>\n",
       "      <td>291850.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1060</td>\n",
       "      <td>9711</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1060</td>\n",
       "      <td>0</td>\n",
       "      <td>1963</td>\n",
       "      <td>0</td>\n",
       "      <td>98198</td>\n",
       "      <td>47.4095</td>\n",
       "      <td>-122.315</td>\n",
       "      <td>1650</td>\n",
       "      <td>9711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2414600126</td>\n",
       "      <td>20150415T000000</td>\n",
       "      <td>229500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1780</td>\n",
       "      <td>7470</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>730</td>\n",
       "      <td>1960</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5123</td>\n",
       "      <td>-122.337</td>\n",
       "      <td>1780</td>\n",
       "      <td>8113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3793500160</td>\n",
       "      <td>20150312T000000</td>\n",
       "      <td>323000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1890</td>\n",
       "      <td>6560</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1890</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98038</td>\n",
       "      <td>47.3684</td>\n",
       "      <td>-122.031</td>\n",
       "      <td>2390</td>\n",
       "      <td>7570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1736800520</td>\n",
       "      <td>20150403T000000</td>\n",
       "      <td>662500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3560</td>\n",
       "      <td>9796</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1860</td>\n",
       "      <td>1700</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98007</td>\n",
       "      <td>47.6007</td>\n",
       "      <td>-122.145</td>\n",
       "      <td>2210</td>\n",
       "      <td>8925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9212900260</td>\n",
       "      <td>20140527T000000</td>\n",
       "      <td>468000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1160</td>\n",
       "      <td>6000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>860</td>\n",
       "      <td>300</td>\n",
       "      <td>1942</td>\n",
       "      <td>0</td>\n",
       "      <td>98115</td>\n",
       "      <td>47.6900</td>\n",
       "      <td>-122.292</td>\n",
       "      <td>1330</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>114101516</td>\n",
       "      <td>20140528T000000</td>\n",
       "      <td>310000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1430</td>\n",
       "      <td>19901</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1430</td>\n",
       "      <td>0</td>\n",
       "      <td>1927</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7558</td>\n",
       "      <td>-122.229</td>\n",
       "      <td>1780</td>\n",
       "      <td>12697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6054650070</td>\n",
       "      <td>20141007T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1370</td>\n",
       "      <td>9680</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1370</td>\n",
       "      <td>0</td>\n",
       "      <td>1977</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6127</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1370</td>\n",
       "      <td>10208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1175000570</td>\n",
       "      <td>20150312T000000</td>\n",
       "      <td>530000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1810</td>\n",
       "      <td>4850</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1810</td>\n",
       "      <td>0</td>\n",
       "      <td>1900</td>\n",
       "      <td>0</td>\n",
       "      <td>98107</td>\n",
       "      <td>47.6700</td>\n",
       "      <td>-122.394</td>\n",
       "      <td>1360</td>\n",
       "      <td>4850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9297300055</td>\n",
       "      <td>20150124T000000</td>\n",
       "      <td>650000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2950</td>\n",
       "      <td>5000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1980</td>\n",
       "      <td>970</td>\n",
       "      <td>1979</td>\n",
       "      <td>0</td>\n",
       "      <td>98126</td>\n",
       "      <td>47.5714</td>\n",
       "      <td>-122.375</td>\n",
       "      <td>2140</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1875500060</td>\n",
       "      <td>20140731T000000</td>\n",
       "      <td>395000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1890</td>\n",
       "      <td>14040</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1890</td>\n",
       "      <td>0</td>\n",
       "      <td>1994</td>\n",
       "      <td>0</td>\n",
       "      <td>98019</td>\n",
       "      <td>47.7277</td>\n",
       "      <td>-121.962</td>\n",
       "      <td>1890</td>\n",
       "      <td>14018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6865200140</td>\n",
       "      <td>20140529T000000</td>\n",
       "      <td>485000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1600</td>\n",
       "      <td>4300</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>1916</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6648</td>\n",
       "      <td>-122.343</td>\n",
       "      <td>1610</td>\n",
       "      <td>4300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>16000397</td>\n",
       "      <td>20141205T000000</td>\n",
       "      <td>189000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1200</td>\n",
       "      <td>9850</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1200</td>\n",
       "      <td>0</td>\n",
       "      <td>1921</td>\n",
       "      <td>0</td>\n",
       "      <td>98002</td>\n",
       "      <td>47.3089</td>\n",
       "      <td>-122.210</td>\n",
       "      <td>1060</td>\n",
       "      <td>5095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7983200060</td>\n",
       "      <td>20150424T000000</td>\n",
       "      <td>230000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1250</td>\n",
       "      <td>9774</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1250</td>\n",
       "      <td>0</td>\n",
       "      <td>1969</td>\n",
       "      <td>0</td>\n",
       "      <td>98003</td>\n",
       "      <td>47.3343</td>\n",
       "      <td>-122.306</td>\n",
       "      <td>1280</td>\n",
       "      <td>8850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6300500875</td>\n",
       "      <td>20140514T000000</td>\n",
       "      <td>385000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1620</td>\n",
       "      <td>4980</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>860</td>\n",
       "      <td>760</td>\n",
       "      <td>1947</td>\n",
       "      <td>0</td>\n",
       "      <td>98133</td>\n",
       "      <td>47.7025</td>\n",
       "      <td>-122.341</td>\n",
       "      <td>1400</td>\n",
       "      <td>4980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2524049179</td>\n",
       "      <td>20140826T000000</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3050</td>\n",
       "      <td>44867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2330</td>\n",
       "      <td>720</td>\n",
       "      <td>1968</td>\n",
       "      <td>0</td>\n",
       "      <td>98040</td>\n",
       "      <td>47.5316</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>4110</td>\n",
       "      <td>20336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7137970340</td>\n",
       "      <td>20140703T000000</td>\n",
       "      <td>285000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2270</td>\n",
       "      <td>6300</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2270</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>98092</td>\n",
       "      <td>47.3266</td>\n",
       "      <td>-122.169</td>\n",
       "      <td>2240</td>\n",
       "      <td>7005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8091400200</td>\n",
       "      <td>20140516T000000</td>\n",
       "      <td>252700.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1070</td>\n",
       "      <td>9643</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1070</td>\n",
       "      <td>0</td>\n",
       "      <td>1985</td>\n",
       "      <td>0</td>\n",
       "      <td>98030</td>\n",
       "      <td>47.3533</td>\n",
       "      <td>-122.166</td>\n",
       "      <td>1220</td>\n",
       "      <td>8386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3814700200</td>\n",
       "      <td>20141120T000000</td>\n",
       "      <td>329000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2450</td>\n",
       "      <td>6500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2450</td>\n",
       "      <td>0</td>\n",
       "      <td>1985</td>\n",
       "      <td>0</td>\n",
       "      <td>98030</td>\n",
       "      <td>47.3739</td>\n",
       "      <td>-122.172</td>\n",
       "      <td>2200</td>\n",
       "      <td>6865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1202000200</td>\n",
       "      <td>20141103T000000</td>\n",
       "      <td>233000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1710</td>\n",
       "      <td>4697</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1710</td>\n",
       "      <td>0</td>\n",
       "      <td>1941</td>\n",
       "      <td>0</td>\n",
       "      <td>98002</td>\n",
       "      <td>47.3048</td>\n",
       "      <td>-122.218</td>\n",
       "      <td>1030</td>\n",
       "      <td>4705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1794500383</td>\n",
       "      <td>20140626T000000</td>\n",
       "      <td>937000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2450</td>\n",
       "      <td>2691</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1750</td>\n",
       "      <td>700</td>\n",
       "      <td>1915</td>\n",
       "      <td>0</td>\n",
       "      <td>98119</td>\n",
       "      <td>47.6386</td>\n",
       "      <td>-122.360</td>\n",
       "      <td>1760</td>\n",
       "      <td>3573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3303700376</td>\n",
       "      <td>20141201T000000</td>\n",
       "      <td>667000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1400</td>\n",
       "      <td>1581</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1400</td>\n",
       "      <td>0</td>\n",
       "      <td>1909</td>\n",
       "      <td>0</td>\n",
       "      <td>98112</td>\n",
       "      <td>47.6221</td>\n",
       "      <td>-122.314</td>\n",
       "      <td>1860</td>\n",
       "      <td>3861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5101402488</td>\n",
       "      <td>20140624T000000</td>\n",
       "      <td>438000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1520</td>\n",
       "      <td>6380</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>790</td>\n",
       "      <td>730</td>\n",
       "      <td>1948</td>\n",
       "      <td>0</td>\n",
       "      <td>98115</td>\n",
       "      <td>47.6950</td>\n",
       "      <td>-122.304</td>\n",
       "      <td>1520</td>\n",
       "      <td>6235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1873100390</td>\n",
       "      <td>20150302T000000</td>\n",
       "      <td>719000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2570</td>\n",
       "      <td>7173</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2570</td>\n",
       "      <td>0</td>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>98052</td>\n",
       "      <td>47.7073</td>\n",
       "      <td>-122.110</td>\n",
       "      <td>2630</td>\n",
       "      <td>6026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21583</th>\n",
       "      <td>2025049203</td>\n",
       "      <td>20140610T000000</td>\n",
       "      <td>399950.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>710</td>\n",
       "      <td>1157</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>710</td>\n",
       "      <td>0</td>\n",
       "      <td>1943</td>\n",
       "      <td>0</td>\n",
       "      <td>98102</td>\n",
       "      <td>47.6413</td>\n",
       "      <td>-122.329</td>\n",
       "      <td>1370</td>\n",
       "      <td>1173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21584</th>\n",
       "      <td>952006823</td>\n",
       "      <td>20141202T000000</td>\n",
       "      <td>380000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1260</td>\n",
       "      <td>900</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>940</td>\n",
       "      <td>320</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98116</td>\n",
       "      <td>47.5621</td>\n",
       "      <td>-122.384</td>\n",
       "      <td>1310</td>\n",
       "      <td>1415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21585</th>\n",
       "      <td>3832050760</td>\n",
       "      <td>20140828T000000</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1870</td>\n",
       "      <td>5000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1870</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98042</td>\n",
       "      <td>47.3339</td>\n",
       "      <td>-122.055</td>\n",
       "      <td>2170</td>\n",
       "      <td>5399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21586</th>\n",
       "      <td>2767604724</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>505000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1430</td>\n",
       "      <td>1201</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1430</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98107</td>\n",
       "      <td>47.6707</td>\n",
       "      <td>-122.381</td>\n",
       "      <td>1430</td>\n",
       "      <td>1249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21587</th>\n",
       "      <td>6632300207</td>\n",
       "      <td>20150305T000000</td>\n",
       "      <td>385000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1520</td>\n",
       "      <td>1488</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1520</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7337</td>\n",
       "      <td>-122.309</td>\n",
       "      <td>1520</td>\n",
       "      <td>1497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21588</th>\n",
       "      <td>2767600688</td>\n",
       "      <td>20141113T000000</td>\n",
       "      <td>414500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1210</td>\n",
       "      <td>1278</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1020</td>\n",
       "      <td>190</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98117</td>\n",
       "      <td>47.6756</td>\n",
       "      <td>-122.375</td>\n",
       "      <td>1210</td>\n",
       "      <td>1118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21589</th>\n",
       "      <td>7570050450</td>\n",
       "      <td>20140910T000000</td>\n",
       "      <td>347500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2540</td>\n",
       "      <td>4760</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2540</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>98038</td>\n",
       "      <td>47.3452</td>\n",
       "      <td>-122.022</td>\n",
       "      <td>2540</td>\n",
       "      <td>4571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21590</th>\n",
       "      <td>7430200100</td>\n",
       "      <td>20140514T000000</td>\n",
       "      <td>1222500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4910</td>\n",
       "      <td>9444</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>3110</td>\n",
       "      <td>1800</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6502</td>\n",
       "      <td>-122.066</td>\n",
       "      <td>4560</td>\n",
       "      <td>11063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21591</th>\n",
       "      <td>4140940150</td>\n",
       "      <td>20141002T000000</td>\n",
       "      <td>572000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2770</td>\n",
       "      <td>3852</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2770</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5001</td>\n",
       "      <td>-122.232</td>\n",
       "      <td>1810</td>\n",
       "      <td>5641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21592</th>\n",
       "      <td>1931300412</td>\n",
       "      <td>20150416T000000</td>\n",
       "      <td>475000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1190</td>\n",
       "      <td>1200</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1190</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6542</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1180</td>\n",
       "      <td>1224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21593</th>\n",
       "      <td>8672200110</td>\n",
       "      <td>20150317T000000</td>\n",
       "      <td>1088000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4170</td>\n",
       "      <td>8142</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>4170</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98056</td>\n",
       "      <td>47.5354</td>\n",
       "      <td>-122.181</td>\n",
       "      <td>3030</td>\n",
       "      <td>7980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21594</th>\n",
       "      <td>5087900040</td>\n",
       "      <td>20141017T000000</td>\n",
       "      <td>350000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2500</td>\n",
       "      <td>5995</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98042</td>\n",
       "      <td>47.3749</td>\n",
       "      <td>-122.107</td>\n",
       "      <td>2530</td>\n",
       "      <td>5988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21595</th>\n",
       "      <td>1972201967</td>\n",
       "      <td>20141031T000000</td>\n",
       "      <td>520000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1530</td>\n",
       "      <td>981</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1480</td>\n",
       "      <td>50</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6533</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "      <td>1282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21596</th>\n",
       "      <td>7502800100</td>\n",
       "      <td>20140813T000000</td>\n",
       "      <td>679950.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3600</td>\n",
       "      <td>9437</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3600</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98059</td>\n",
       "      <td>47.4822</td>\n",
       "      <td>-122.131</td>\n",
       "      <td>3550</td>\n",
       "      <td>9421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21597</th>\n",
       "      <td>191100405</td>\n",
       "      <td>20150421T000000</td>\n",
       "      <td>1575000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3410</td>\n",
       "      <td>10125</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>3410</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98040</td>\n",
       "      <td>47.5653</td>\n",
       "      <td>-122.223</td>\n",
       "      <td>2290</td>\n",
       "      <td>10125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21598</th>\n",
       "      <td>8956200760</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>541800.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3118</td>\n",
       "      <td>7866</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3118</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98001</td>\n",
       "      <td>47.2931</td>\n",
       "      <td>-122.264</td>\n",
       "      <td>2673</td>\n",
       "      <td>6500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21599</th>\n",
       "      <td>7202300110</td>\n",
       "      <td>20140915T000000</td>\n",
       "      <td>810000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3990</td>\n",
       "      <td>7838</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3990</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98053</td>\n",
       "      <td>47.6857</td>\n",
       "      <td>-122.046</td>\n",
       "      <td>3370</td>\n",
       "      <td>6814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21600</th>\n",
       "      <td>249000205</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>1537000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4470</td>\n",
       "      <td>8088</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>4470</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98004</td>\n",
       "      <td>47.6321</td>\n",
       "      <td>-122.200</td>\n",
       "      <td>2780</td>\n",
       "      <td>8964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21601</th>\n",
       "      <td>5100403806</td>\n",
       "      <td>20150407T000000</td>\n",
       "      <td>467000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1425</td>\n",
       "      <td>1179</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1425</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.6963</td>\n",
       "      <td>-122.318</td>\n",
       "      <td>1285</td>\n",
       "      <td>1253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21602</th>\n",
       "      <td>844000965</td>\n",
       "      <td>20140626T000000</td>\n",
       "      <td>224000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1500</td>\n",
       "      <td>11968</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98010</td>\n",
       "      <td>47.3095</td>\n",
       "      <td>-122.002</td>\n",
       "      <td>1320</td>\n",
       "      <td>11303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21603</th>\n",
       "      <td>7852140040</td>\n",
       "      <td>20140825T000000</td>\n",
       "      <td>507250.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2270</td>\n",
       "      <td>5536</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2270</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98065</td>\n",
       "      <td>47.5389</td>\n",
       "      <td>-121.881</td>\n",
       "      <td>2270</td>\n",
       "      <td>5731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21604</th>\n",
       "      <td>9834201367</td>\n",
       "      <td>20150126T000000</td>\n",
       "      <td>429000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1490</td>\n",
       "      <td>1126</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1490</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5699</td>\n",
       "      <td>-122.288</td>\n",
       "      <td>1400</td>\n",
       "      <td>1230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21605</th>\n",
       "      <td>3448900210</td>\n",
       "      <td>20141014T000000</td>\n",
       "      <td>610685.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2520</td>\n",
       "      <td>6023</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2520</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98056</td>\n",
       "      <td>47.5137</td>\n",
       "      <td>-122.167</td>\n",
       "      <td>2520</td>\n",
       "      <td>6023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21606</th>\n",
       "      <td>7936000429</td>\n",
       "      <td>20150326T000000</td>\n",
       "      <td>1007500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3510</td>\n",
       "      <td>7200</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2600</td>\n",
       "      <td>910</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5537</td>\n",
       "      <td>-122.398</td>\n",
       "      <td>2050</td>\n",
       "      <td>6200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21607</th>\n",
       "      <td>2997800021</td>\n",
       "      <td>20150219T000000</td>\n",
       "      <td>475000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1310</td>\n",
       "      <td>1294</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1180</td>\n",
       "      <td>130</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98116</td>\n",
       "      <td>47.5773</td>\n",
       "      <td>-122.409</td>\n",
       "      <td>1330</td>\n",
       "      <td>1265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21608</th>\n",
       "      <td>263000018</td>\n",
       "      <td>20140521T000000</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1530</td>\n",
       "      <td>1131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1530</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6993</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21609</th>\n",
       "      <td>6600060120</td>\n",
       "      <td>20150223T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2310</td>\n",
       "      <td>5813</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2310</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5107</td>\n",
       "      <td>-122.362</td>\n",
       "      <td>1830</td>\n",
       "      <td>7200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21610</th>\n",
       "      <td>1523300141</td>\n",
       "      <td>20140623T000000</td>\n",
       "      <td>402101.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1350</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5944</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21611</th>\n",
       "      <td>291310100</td>\n",
       "      <td>20150116T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1600</td>\n",
       "      <td>2388</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>98027</td>\n",
       "      <td>47.5345</td>\n",
       "      <td>-122.069</td>\n",
       "      <td>1410</td>\n",
       "      <td>1287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21612</th>\n",
       "      <td>1523300157</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1076</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5941</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>1357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21613 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id             date      price  bedrooms  bathrooms  \\\n",
       "0      7129300520  20141013T000000   221900.0         3       1.00   \n",
       "1      6414100192  20141209T000000   538000.0         3       2.25   \n",
       "2      5631500400  20150225T000000   180000.0         2       1.00   \n",
       "3      2487200875  20141209T000000   604000.0         4       3.00   \n",
       "4      1954400510  20150218T000000   510000.0         3       2.00   \n",
       "5      7237550310  20140512T000000  1225000.0         4       4.50   \n",
       "6      1321400060  20140627T000000   257500.0         3       2.25   \n",
       "7      2008000270  20150115T000000   291850.0         3       1.50   \n",
       "8      2414600126  20150415T000000   229500.0         3       1.00   \n",
       "9      3793500160  20150312T000000   323000.0         3       2.50   \n",
       "10     1736800520  20150403T000000   662500.0         3       2.50   \n",
       "11     9212900260  20140527T000000   468000.0         2       1.00   \n",
       "12      114101516  20140528T000000   310000.0         3       1.00   \n",
       "13     6054650070  20141007T000000   400000.0         3       1.75   \n",
       "14     1175000570  20150312T000000   530000.0         5       2.00   \n",
       "15     9297300055  20150124T000000   650000.0         4       3.00   \n",
       "16     1875500060  20140731T000000   395000.0         3       2.00   \n",
       "17     6865200140  20140529T000000   485000.0         4       1.00   \n",
       "18       16000397  20141205T000000   189000.0         2       1.00   \n",
       "19     7983200060  20150424T000000   230000.0         3       1.00   \n",
       "20     6300500875  20140514T000000   385000.0         4       1.75   \n",
       "21     2524049179  20140826T000000  2000000.0         3       2.75   \n",
       "22     7137970340  20140703T000000   285000.0         5       2.50   \n",
       "23     8091400200  20140516T000000   252700.0         2       1.50   \n",
       "24     3814700200  20141120T000000   329000.0         3       2.25   \n",
       "25     1202000200  20141103T000000   233000.0         3       2.00   \n",
       "26     1794500383  20140626T000000   937000.0         3       1.75   \n",
       "27     3303700376  20141201T000000   667000.0         3       1.00   \n",
       "28     5101402488  20140624T000000   438000.0         3       1.75   \n",
       "29     1873100390  20150302T000000   719000.0         4       2.50   \n",
       "...           ...              ...        ...       ...        ...   \n",
       "21583  2025049203  20140610T000000   399950.0         2       1.00   \n",
       "21584   952006823  20141202T000000   380000.0         3       2.50   \n",
       "21585  3832050760  20140828T000000   270000.0         3       2.50   \n",
       "21586  2767604724  20141015T000000   505000.0         2       2.50   \n",
       "21587  6632300207  20150305T000000   385000.0         3       2.50   \n",
       "21588  2767600688  20141113T000000   414500.0         2       1.50   \n",
       "21589  7570050450  20140910T000000   347500.0         3       2.50   \n",
       "21590  7430200100  20140514T000000  1222500.0         4       3.50   \n",
       "21591  4140940150  20141002T000000   572000.0         4       2.75   \n",
       "21592  1931300412  20150416T000000   475000.0         3       2.25   \n",
       "21593  8672200110  20150317T000000  1088000.0         5       3.75   \n",
       "21594  5087900040  20141017T000000   350000.0         4       2.75   \n",
       "21595  1972201967  20141031T000000   520000.0         2       2.25   \n",
       "21596  7502800100  20140813T000000   679950.0         5       2.75   \n",
       "21597   191100405  20150421T000000  1575000.0         4       3.25   \n",
       "21598  8956200760  20141013T000000   541800.0         4       2.50   \n",
       "21599  7202300110  20140915T000000   810000.0         4       3.00   \n",
       "21600   249000205  20141015T000000  1537000.0         5       3.75   \n",
       "21601  5100403806  20150407T000000   467000.0         3       2.50   \n",
       "21602   844000965  20140626T000000   224000.0         3       1.75   \n",
       "21603  7852140040  20140825T000000   507250.0         3       2.50   \n",
       "21604  9834201367  20150126T000000   429000.0         3       2.00   \n",
       "21605  3448900210  20141014T000000   610685.0         4       2.50   \n",
       "21606  7936000429  20150326T000000  1007500.0         4       3.50   \n",
       "21607  2997800021  20150219T000000   475000.0         3       2.50   \n",
       "21608   263000018  20140521T000000   360000.0         3       2.50   \n",
       "21609  6600060120  20150223T000000   400000.0         4       2.50   \n",
       "21610  1523300141  20140623T000000   402101.0         2       0.75   \n",
       "21611   291310100  20150116T000000   400000.0         3       2.50   \n",
       "21612  1523300157  20141015T000000   325000.0         2       0.75   \n",
       "\n",
       "       sqft_living  sqft_lot  floors  waterfront  view  ...  grade  \\\n",
       "0             1180      5650     1.0           0     0  ...      7   \n",
       "1             2570      7242     2.0           0     0  ...      7   \n",
       "2              770     10000     1.0           0     0  ...      6   \n",
       "3             1960      5000     1.0           0     0  ...      7   \n",
       "4             1680      8080     1.0           0     0  ...      8   \n",
       "5             5420    101930     1.0           0     0  ...     11   \n",
       "6             1715      6819     2.0           0     0  ...      7   \n",
       "7             1060      9711     1.0           0     0  ...      7   \n",
       "8             1780      7470     1.0           0     0  ...      7   \n",
       "9             1890      6560     2.0           0     0  ...      7   \n",
       "10            3560      9796     1.0           0     0  ...      8   \n",
       "11            1160      6000     1.0           0     0  ...      7   \n",
       "12            1430     19901     1.5           0     0  ...      7   \n",
       "13            1370      9680     1.0           0     0  ...      7   \n",
       "14            1810      4850     1.5           0     0  ...      7   \n",
       "15            2950      5000     2.0           0     3  ...      9   \n",
       "16            1890     14040     2.0           0     0  ...      7   \n",
       "17            1600      4300     1.5           0     0  ...      7   \n",
       "18            1200      9850     1.0           0     0  ...      7   \n",
       "19            1250      9774     1.0           0     0  ...      7   \n",
       "20            1620      4980     1.0           0     0  ...      7   \n",
       "21            3050     44867     1.0           0     4  ...      9   \n",
       "22            2270      6300     2.0           0     0  ...      8   \n",
       "23            1070      9643     1.0           0     0  ...      7   \n",
       "24            2450      6500     2.0           0     0  ...      8   \n",
       "25            1710      4697     1.5           0     0  ...      6   \n",
       "26            2450      2691     2.0           0     0  ...      8   \n",
       "27            1400      1581     1.5           0     0  ...      8   \n",
       "28            1520      6380     1.0           0     0  ...      7   \n",
       "29            2570      7173     2.0           0     0  ...      8   \n",
       "...            ...       ...     ...         ...   ...  ...    ...   \n",
       "21583          710      1157     2.0           0     0  ...      7   \n",
       "21584         1260       900     2.0           0     0  ...      7   \n",
       "21585         1870      5000     2.0           0     0  ...      7   \n",
       "21586         1430      1201     3.0           0     0  ...      8   \n",
       "21587         1520      1488     3.0           0     0  ...      8   \n",
       "21588         1210      1278     2.0           0     0  ...      8   \n",
       "21589         2540      4760     2.0           0     0  ...      8   \n",
       "21590         4910      9444     1.5           0     0  ...     11   \n",
       "21591         2770      3852     2.0           0     0  ...      8   \n",
       "21592         1190      1200     3.0           0     0  ...      8   \n",
       "21593         4170      8142     2.0           0     2  ...     10   \n",
       "21594         2500      5995     2.0           0     0  ...      8   \n",
       "21595         1530       981     3.0           0     0  ...      8   \n",
       "21596         3600      9437     2.0           0     0  ...      9   \n",
       "21597         3410     10125     2.0           0     0  ...     10   \n",
       "21598         3118      7866     2.0           0     2  ...      9   \n",
       "21599         3990      7838     2.0           0     0  ...      9   \n",
       "21600         4470      8088     2.0           0     0  ...     11   \n",
       "21601         1425      1179     3.0           0     0  ...      8   \n",
       "21602         1500     11968     1.0           0     0  ...      6   \n",
       "21603         2270      5536     2.0           0     0  ...      8   \n",
       "21604         1490      1126     3.0           0     0  ...      8   \n",
       "21605         2520      6023     2.0           0     0  ...      9   \n",
       "21606         3510      7200     2.0           0     0  ...      9   \n",
       "21607         1310      1294     2.0           0     0  ...      8   \n",
       "21608         1530      1131     3.0           0     0  ...      8   \n",
       "21609         2310      5813     2.0           0     0  ...      8   \n",
       "21610         1020      1350     2.0           0     0  ...      7   \n",
       "21611         1600      2388     2.0           0     0  ...      8   \n",
       "21612         1020      1076     2.0           0     0  ...      7   \n",
       "\n",
       "       sqft_above  sqft_basement  yr_built  yr_renovated  zipcode      lat  \\\n",
       "0            1180              0      1955             0    98178  47.5112   \n",
       "1            2170            400      1951          1991    98125  47.7210   \n",
       "2             770              0      1933             0    98028  47.7379   \n",
       "3            1050            910      1965             0    98136  47.5208   \n",
       "4            1680              0      1987             0    98074  47.6168   \n",
       "5            3890           1530      2001             0    98053  47.6561   \n",
       "6            1715              0      1995             0    98003  47.3097   \n",
       "7            1060              0      1963             0    98198  47.4095   \n",
       "8            1050            730      1960             0    98146  47.5123   \n",
       "9            1890              0      2003             0    98038  47.3684   \n",
       "10           1860           1700      1965             0    98007  47.6007   \n",
       "11            860            300      1942             0    98115  47.6900   \n",
       "12           1430              0      1927             0    98028  47.7558   \n",
       "13           1370              0      1977             0    98074  47.6127   \n",
       "14           1810              0      1900             0    98107  47.6700   \n",
       "15           1980            970      1979             0    98126  47.5714   \n",
       "16           1890              0      1994             0    98019  47.7277   \n",
       "17           1600              0      1916             0    98103  47.6648   \n",
       "18           1200              0      1921             0    98002  47.3089   \n",
       "19           1250              0      1969             0    98003  47.3343   \n",
       "20            860            760      1947             0    98133  47.7025   \n",
       "21           2330            720      1968             0    98040  47.5316   \n",
       "22           2270              0      1995             0    98092  47.3266   \n",
       "23           1070              0      1985             0    98030  47.3533   \n",
       "24           2450              0      1985             0    98030  47.3739   \n",
       "25           1710              0      1941             0    98002  47.3048   \n",
       "26           1750            700      1915             0    98119  47.6386   \n",
       "27           1400              0      1909             0    98112  47.6221   \n",
       "28            790            730      1948             0    98115  47.6950   \n",
       "29           2570              0      2005             0    98052  47.7073   \n",
       "...           ...            ...       ...           ...      ...      ...   \n",
       "21583         710              0      1943             0    98102  47.6413   \n",
       "21584         940            320      2007             0    98116  47.5621   \n",
       "21585        1870              0      2009             0    98042  47.3339   \n",
       "21586        1430              0      2009             0    98107  47.6707   \n",
       "21587        1520              0      2006             0    98125  47.7337   \n",
       "21588        1020            190      2007             0    98117  47.6756   \n",
       "21589        2540              0      2010             0    98038  47.3452   \n",
       "21590        3110           1800      2007             0    98074  47.6502   \n",
       "21591        2770              0      2014             0    98178  47.5001   \n",
       "21592        1190              0      2008             0    98103  47.6542   \n",
       "21593        4170              0      2006             0    98056  47.5354   \n",
       "21594        2500              0      2008             0    98042  47.3749   \n",
       "21595        1480             50      2006             0    98103  47.6533   \n",
       "21596        3600              0      2014             0    98059  47.4822   \n",
       "21597        3410              0      2007             0    98040  47.5653   \n",
       "21598        3118              0      2014             0    98001  47.2931   \n",
       "21599        3990              0      2003             0    98053  47.6857   \n",
       "21600        4470              0      2008             0    98004  47.6321   \n",
       "21601        1425              0      2008             0    98125  47.6963   \n",
       "21602        1500              0      2014             0    98010  47.3095   \n",
       "21603        2270              0      2003             0    98065  47.5389   \n",
       "21604        1490              0      2014             0    98144  47.5699   \n",
       "21605        2520              0      2014             0    98056  47.5137   \n",
       "21606        2600            910      2009             0    98136  47.5537   \n",
       "21607        1180            130      2008             0    98116  47.5773   \n",
       "21608        1530              0      2009             0    98103  47.6993   \n",
       "21609        2310              0      2014             0    98146  47.5107   \n",
       "21610        1020              0      2009             0    98144  47.5944   \n",
       "21611        1600              0      2004             0    98027  47.5345   \n",
       "21612        1020              0      2008             0    98144  47.5941   \n",
       "\n",
       "          long  sqft_living15  sqft_lot15  \n",
       "0     -122.257           1340        5650  \n",
       "1     -122.319           1690        7639  \n",
       "2     -122.233           2720        8062  \n",
       "3     -122.393           1360        5000  \n",
       "4     -122.045           1800        7503  \n",
       "5     -122.005           4760      101930  \n",
       "6     -122.327           2238        6819  \n",
       "7     -122.315           1650        9711  \n",
       "8     -122.337           1780        8113  \n",
       "9     -122.031           2390        7570  \n",
       "10    -122.145           2210        8925  \n",
       "11    -122.292           1330        6000  \n",
       "12    -122.229           1780       12697  \n",
       "13    -122.045           1370       10208  \n",
       "14    -122.394           1360        4850  \n",
       "15    -122.375           2140        4000  \n",
       "16    -121.962           1890       14018  \n",
       "17    -122.343           1610        4300  \n",
       "18    -122.210           1060        5095  \n",
       "19    -122.306           1280        8850  \n",
       "20    -122.341           1400        4980  \n",
       "21    -122.233           4110       20336  \n",
       "22    -122.169           2240        7005  \n",
       "23    -122.166           1220        8386  \n",
       "24    -122.172           2200        6865  \n",
       "25    -122.218           1030        4705  \n",
       "26    -122.360           1760        3573  \n",
       "27    -122.314           1860        3861  \n",
       "28    -122.304           1520        6235  \n",
       "29    -122.110           2630        6026  \n",
       "...        ...            ...         ...  \n",
       "21583 -122.329           1370        1173  \n",
       "21584 -122.384           1310        1415  \n",
       "21585 -122.055           2170        5399  \n",
       "21586 -122.381           1430        1249  \n",
       "21587 -122.309           1520        1497  \n",
       "21588 -122.375           1210        1118  \n",
       "21589 -122.022           2540        4571  \n",
       "21590 -122.066           4560       11063  \n",
       "21591 -122.232           1810        5641  \n",
       "21592 -122.346           1180        1224  \n",
       "21593 -122.181           3030        7980  \n",
       "21594 -122.107           2530        5988  \n",
       "21595 -122.346           1530        1282  \n",
       "21596 -122.131           3550        9421  \n",
       "21597 -122.223           2290       10125  \n",
       "21598 -122.264           2673        6500  \n",
       "21599 -122.046           3370        6814  \n",
       "21600 -122.200           2780        8964  \n",
       "21601 -122.318           1285        1253  \n",
       "21602 -122.002           1320       11303  \n",
       "21603 -121.881           2270        5731  \n",
       "21604 -122.288           1400        1230  \n",
       "21605 -122.167           2520        6023  \n",
       "21606 -122.398           2050        6200  \n",
       "21607 -122.409           1330        1265  \n",
       "21608 -122.346           1530        1509  \n",
       "21609 -122.362           1830        7200  \n",
       "21610 -122.299           1020        2007  \n",
       "21611 -122.069           1410        1287  \n",
       "21612 -122.299           1020        1357  \n",
       "\n",
       "[21613 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('kc_house_data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_selection = [\"bedrooms\",\"bathrooms\",\"sqft_living\",\"sqft_lot\",\n",
    "                    \"floors\",\"condition\",\"grade\",\"sqft_above\",\n",
    "                    \"sqft_basement\",\"sqft_living15\",\"sqft_lot15\",\n",
    "                    \"lat\", \"long\",\"yr_built\",\"yr_renovated\",\"waterfront\"]\n",
    "\n",
    "selected_feature = np.array(data[column_selection])\n",
    "price = np.array(data[\"price\"])\n",
    "selected_feature_train = selected_feature[:20000]\n",
    "price_train = price[:20000]\n",
    "\n",
    "selected_feature_test = selected_feature[20000:]\n",
    "price_test = price[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y,y_pred):\n",
    "    return np.mean(np.abs(y-y_pred)/y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_len = len(column_selection)\n",
    "model.add(keras.layers.Dense(50, input_dim=input_len, activation='relu'))\n",
    "model.add(keras.layers.Dense(50, activation='relu'))\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "20000/20000 [==============================] - 2s 98us/step - loss: 365669033495.7568\n",
      "Epoch 2/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 256892456638.8736\n",
      "Epoch 3/50\n",
      "20000/20000 [==============================] - 1s 46us/step - loss: 118581314053.7344\n",
      "Epoch 4/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 77825994561.9456\n",
      "Epoch 5/50\n",
      "20000/20000 [==============================] - 1s 46us/step - loss: 74435019171.4304\n",
      "Epoch 6/50\n",
      "20000/20000 [==============================] - 1s 42us/step - loss: 72377106844.8768\n",
      "Epoch 7/50\n",
      "20000/20000 [==============================] - 1s 43us/step - loss: 70845085161.8816\n",
      "Epoch 8/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 69536943413.6576\n",
      "Epoch 9/50\n",
      "20000/20000 [==============================] - 1s 43us/step - loss: 68377267208.1920\n",
      "Epoch 10/50\n",
      "20000/20000 [==============================] - 1s 43us/step - loss: 67398507482.3168\n",
      "Epoch 11/50\n",
      "20000/20000 [==============================] - 1s 41us/step - loss: 66449516580.0448\n",
      "Epoch 12/50\n",
      "20000/20000 [==============================] - 1s 43us/step - loss: 65753233070.4896\n",
      "Epoch 13/50\n",
      "20000/20000 [==============================] - 1s 45us/step - loss: 65246045116.8256\n",
      "Epoch 14/50\n",
      "20000/20000 [==============================] - 1s 47us/step - loss: 64865689862.1440\n",
      "Epoch 15/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 64658290678.1696\n",
      "Epoch 16/50\n",
      "20000/20000 [==============================] - 1s 55us/step - loss: 64555758970.4704\n",
      "Epoch 17/50\n",
      "20000/20000 [==============================] - 1s 57us/step - loss: 64348194576.7936\n",
      "Epoch 18/50\n",
      "20000/20000 [==============================] - 1s 47us/step - loss: 64161415351.5008\n",
      "Epoch 19/50\n",
      "20000/20000 [==============================] - 1s 49us/step - loss: 64153920097.4848\n",
      "Epoch 20/50\n",
      "20000/20000 [==============================] - 1s 56us/step - loss: 64164327273.2672\n",
      "Epoch 21/50\n",
      "20000/20000 [==============================] - 1s 43us/step - loss: 64090946404.3520\n",
      "Epoch 22/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 63971375454.6176\n",
      "Epoch 23/50\n",
      "20000/20000 [==============================] - 1s 50us/step - loss: 63800793130.5984\n",
      "Epoch 24/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 63907617269.3504\n",
      "Epoch 25/50\n",
      "20000/20000 [==============================] - 1s 42us/step - loss: 63738145996.8000\n",
      "Epoch 26/50\n",
      "20000/20000 [==============================] - 1s 43us/step - loss: 63752756081.4592\n",
      "Epoch 27/50\n",
      "20000/20000 [==============================] - 1s 45us/step - loss: 63687096870.5024\n",
      "Epoch 28/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 63496693121.0240\n",
      "Epoch 29/50\n",
      "20000/20000 [==============================] - 1s 42us/step - loss: 63599389573.1200\n",
      "Epoch 30/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 63432194706.6368\n",
      "Epoch 31/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 63428794292.6336\n",
      "Epoch 32/50\n",
      "20000/20000 [==============================] - 1s 45us/step - loss: 63189229558.1696\n",
      "Epoch 33/50\n",
      "20000/20000 [==============================] - 1s 54us/step - loss: 63168384650.4448\n",
      "Epoch 34/50\n",
      "20000/20000 [==============================] - 1s 61us/step - loss: 63209628644.1472\n",
      "Epoch 35/50\n",
      "20000/20000 [==============================] - 1s 49us/step - loss: 63039056320.9216\n",
      "Epoch 36/50\n",
      "20000/20000 [==============================] - 1s 49us/step - loss: 63310884844.3392\n",
      "Epoch 37/50\n",
      "20000/20000 [==============================] - 1s 43us/step - loss: 62910145744.0768\n",
      "Epoch 38/50\n",
      "20000/20000 [==============================] - 1s 41us/step - loss: 63133198516.2240\n",
      "Epoch 39/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 62568853700.6080\n",
      "Epoch 40/50\n",
      "20000/20000 [==============================] - 1s 46us/step - loss: 62554151891.7632\n",
      "Epoch 41/50\n",
      "20000/20000 [==============================] - 1s 43us/step - loss: 62506338392.4736\n",
      "Epoch 42/50\n",
      "20000/20000 [==============================] - 1s 42us/step - loss: 62140409171.1488\n",
      "Epoch 43/50\n",
      "20000/20000 [==============================] - 1s 46us/step - loss: 62015432962.8672\n",
      "Epoch 44/50\n",
      "20000/20000 [==============================] - 1s 52us/step - loss: 61866165347.9424\n",
      "Epoch 45/50\n",
      "20000/20000 [==============================] - 1s 46us/step - loss: 61890855593.5744\n",
      "Epoch 46/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 61635160932.3520\n",
      "Epoch 47/50\n",
      "20000/20000 [==============================] - 1s 43us/step - loss: 61604617453.5680\n",
      "Epoch 48/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 61802675044.3520\n",
      "Epoch 49/50\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 61439805371.1872\n",
      "Epoch 50/50\n",
      "20000/20000 [==============================] - 1s 45us/step - loss: 61255870644.2240\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(selected_feature_train, price_train, epochs=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5347612664268558"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(selected_feature_test)\n",
    "score(preds, price_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Like SKLearn, it's easy to train and evaluate simple models.\n",
    "#### ... but we should try to do better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Deep Learning -- What you need to know\n",
    "### Train, Validation, Test:\n",
    "   * Optimize parameters with Train (weights, biases)\n",
    "   * Optimize hyperparameters with Validation (layer width & depth, activation functions, etc.)\n",
    "   * Optimize NOTHING with Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split out a validation set for hyperparameter optimization\n",
    "\n",
    "selected_feature_train = selected_feature[:18000]\n",
    "price_train = price[:18000]\n",
    "\n",
    "selected_feature_val = selected_feature[18000:20000]\n",
    "price_val = price[18000:20000]\n",
    "\n",
    "selected_feature_test = selected_feature[20000:]\n",
    "price_test = price[20000:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the future, try better validation schemes like [k-fold cross validation](https://chrisalbon.com/deep_learning/keras/k-fold_cross-validating_neural_networks/), though 80/20 or 90/10 train/val like this works in a pinch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a hyperparameter optimization:\n",
    "\n",
    "### Try three activation functions to use for dense layers in the neural network above. Save the model that achieves the best validation loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint: [activation functions](http://letmegooglethat.com/?q=keras+activation+functions)\n",
    "\n",
    "#### Hint: `model.fit` has argument \"`validation_data`\" which takes a tuple of features and targets\n",
    "\n",
    "#### Hint: Use `model.save(\"filename.h5\")` to save a model locally. If you want to use it later, just call `keras.models.load_model(\"filename.h5\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For easy looping, define neural network model as a function\n",
    "def nn_model(optimizer='adam',\n",
    "             activation='relu',\n",
    "             layers=[20,20],\n",
    "             loss='mean_squared_error'):\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(50, input_dim=input_len, activation=activ))\n",
    "    model.add(keras.layers.Dense(50, activation=activ))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 2s 103us/step - loss: 532976.1402 - val_loss: 557917.1435\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 532970.0954 - val_loss: 557911.8150\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532965.0778 - val_loss: 557906.8585\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532959.7826 - val_loss: 557901.4790\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532953.9452 - val_loss: 557895.5335\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532948.0422 - val_loss: 557889.6670\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532942.2711 - val_loss: 557883.6970\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 532936.3595 - val_loss: 557877.8125\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532930.7372 - val_loss: 557872.4470\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532925.2140 - val_loss: 557866.8460\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 1s 62us/step - loss: 532919.7753 - val_loss: 557861.6330\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 1s 61us/step - loss: 532914.3888 - val_loss: 557855.9605\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 532909.0208 - val_loss: 557850.7880\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 532903.6846 - val_loss: 557845.5335\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 532898.3763 - val_loss: 557839.9765\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532893.0706 - val_loss: 557834.8360\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 532887.7813 - val_loss: 557829.6650\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 532882.5183 - val_loss: 557824.4250\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 532877.2416 - val_loss: 557818.9855\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532871.9762 - val_loss: 557813.7380\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532866.7403 - val_loss: 557808.6030\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532861.4883 - val_loss: 557803.4790\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 532856.2354 - val_loss: 557797.9335\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532851.0037 - val_loss: 557792.8160\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 532845.7629 - val_loss: 557787.6650\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532840.3614 - val_loss: 557781.7820\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532834.5491 - val_loss: 557775.9805\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 1s 64us/step - loss: 532828.9726 - val_loss: 557770.6450\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 1s 64us/step - loss: 532823.4922 - val_loss: 557765.0485\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 532818.0286 - val_loss: 557759.7380\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532812.6159 - val_loss: 557754.4430\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 532807.1844 - val_loss: 557748.8460\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 532801.7806 - val_loss: 557743.6650\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 532796.3886 - val_loss: 557737.9525\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532790.9913 - val_loss: 557732.6900\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532785.6103 - val_loss: 557727.4930\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532780.2281 - val_loss: 557721.8150\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 532774.8531 - val_loss: 557716.6110\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 532769.4810 - val_loss: 557711.0550\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 532764.0931 - val_loss: 557705.7920\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532758.7429 - val_loss: 557700.5060\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 532753.3740 - val_loss: 557695.0085\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 532747.9944 - val_loss: 557689.7320\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 532742.6497 - val_loss: 557684.4470\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532737.2771 - val_loss: 557678.8970\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 1s 59us/step - loss: 532731.9018 - val_loss: 557673.6890\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 1s 63us/step - loss: 532726.5591 - val_loss: 557668.4250\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532721.1828 - val_loss: 557662.8625\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532715.8219 - val_loss: 557657.6650\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 532710.4750 - val_loss: 557651.9945\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 2s 102us/step - loss: 532969.5734 - val_loss: 557908.7160\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 532960.3154 - val_loss: 557900.8600\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 532952.7059 - val_loss: 557893.5195\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532945.2786 - val_loss: 557885.8530\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532937.9344 - val_loss: 557878.6530\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 532930.6183 - val_loss: 557871.5010\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532923.3429 - val_loss: 557863.9545\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 532916.0685 - val_loss: 557856.8360\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532908.8241 - val_loss: 557849.6850\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 1s 63us/step - loss: 532901.6033 - val_loss: 557842.4510\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 1s 79us/step - loss: 532894.3654 - val_loss: 557835.0485\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 532887.1322 - val_loss: 557827.8125\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 532879.9171 - val_loss: 557820.6900\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532872.7098 - val_loss: 557813.6650\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 532865.4984 - val_loss: 557806.4290\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 532858.2976 - val_loss: 557799.0165\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 532851.0801 - val_loss: 557791.7980\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532843.8895 - val_loss: 557784.6670\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532836.6774 - val_loss: 557777.6350\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 532829.4903 - val_loss: 557770.4290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532822.2826 - val_loss: 557763.0125\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 532815.0768 - val_loss: 557755.7980\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 532807.8961 - val_loss: 557748.6670\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 532800.6868 - val_loss: 557741.6650\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 532793.5059 - val_loss: 557734.4250\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532786.3022 - val_loss: 557727.0365\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532779.1036 - val_loss: 557719.8125\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 1s 62us/step - loss: 532771.9182 - val_loss: 557712.6900\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 1s 67us/step - loss: 532764.7254 - val_loss: 557705.6650\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 532757.5402 - val_loss: 557698.4430\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 1s 60us/step - loss: 532750.3334 - val_loss: 557691.0485\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532743.1393 - val_loss: 557683.8150\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 532735.9523 - val_loss: 557676.7680\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 1s 60us/step - loss: 532728.7599 - val_loss: 557669.6670\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 532721.5713 - val_loss: 557662.4470\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 532714.3762 - val_loss: 557655.0550\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 532707.1782 - val_loss: 557647.8230\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 532699.9929 - val_loss: 557640.8160\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532692.7960 - val_loss: 557633.6850\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 532685.6186 - val_loss: 557626.4755\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 532678.4242 - val_loss: 557619.3645\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 532671.2374 - val_loss: 557611.9335\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 532664.0459 - val_loss: 557604.8360\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 1s 60us/step - loss: 532656.8513 - val_loss: 557597.6930\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 1s 75us/step - loss: 532649.6699 - val_loss: 557590.5060\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 1s 62us/step - loss: 532642.4763 - val_loss: 557583.4790\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 532635.2810 - val_loss: 557575.9525\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 532628.0936 - val_loss: 557568.8460\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 1s 59us/step - loss: 532620.8944 - val_loss: 557561.7030\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532613.7123 - val_loss: 557554.6030\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 2s 110us/step - loss: 455952.1304 - val_loss: 373286.8225\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 278361.6762 - val_loss: 209940.7698\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 172223.8001 - val_loss: 175337.5301\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 1s 65us/step - loss: 164821.2789 - val_loss: 172687.7000\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 1s 62us/step - loss: 162513.3667 - val_loss: 170515.4134\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 160748.9832 - val_loss: 170039.3686\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 159272.3799 - val_loss: 169398.3594\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 158261.9577 - val_loss: 168648.4468\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 1s 61us/step - loss: 157660.4394 - val_loss: 168290.7511\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 157267.5227 - val_loss: 167622.9265\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 156906.5776 - val_loss: 167625.4138\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 1s 59us/step - loss: 156582.3795 - val_loss: 167245.5091\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 156339.0880 - val_loss: 167515.9761\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 156025.3955 - val_loss: 167448.9193\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 155635.8357 - val_loss: 167442.4140\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 155318.0477 - val_loss: 166780.1139\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 155498.3728 - val_loss: 166488.3240\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 154990.8275 - val_loss: 167272.1128\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 154861.7708 - val_loss: 166666.6418\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 154911.8777 - val_loss: 166157.9350\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 154519.0990 - val_loss: 166551.2291\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 154394.3246 - val_loss: 167430.2441\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 154454.1467 - val_loss: 166013.1382\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 154075.3205 - val_loss: 166521.1909\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 1s 68us/step - loss: 153963.1720 - val_loss: 165924.7779\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 1s 64us/step - loss: 153759.0765 - val_loss: 165467.6439\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 1s 61us/step - loss: 153948.8865 - val_loss: 165642.9401\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 153631.7364 - val_loss: 166592.4932\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 153527.1088 - val_loss: 165423.6541\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 153324.8746 - val_loss: 166680.7668\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 153479.5514 - val_loss: 165848.0201\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 153115.5983 - val_loss: 165045.5928\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 1s 59us/step - loss: 153115.9632 - val_loss: 165410.7117\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 1s 68us/step - loss: 152911.3349 - val_loss: 164909.6011\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 152913.0666 - val_loss: 165842.8438\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 1s 74us/step - loss: 152737.9939 - val_loss: 164852.0232\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 1s 79us/step - loss: 152654.7912 - val_loss: 165400.8479\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 1s 77us/step - loss: 152503.0845 - val_loss: 164554.0540\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 1s 69us/step - loss: 152456.0341 - val_loss: 165519.3701\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 1s 79us/step - loss: 152538.6772 - val_loss: 164977.9030\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 42us/step - loss: 152274.3146 - val_loss: 164840.1689\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 1s 35us/step - loss: 152403.5622 - val_loss: 165427.3961\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 151953.8458 - val_loss: 164312.8211\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 1s 33us/step - loss: 152042.8247 - val_loss: 164806.8318\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 151854.0993 - val_loss: 164258.8999\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 151732.5698 - val_loss: 164033.2630\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 151733.9967 - val_loss: 163808.9901\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 151463.0350 - val_loss: 163963.4001\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 151592.0935 - val_loss: 164032.9958\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 151199.7310 - val_loss: 164281.4720\n",
      "BEST ACTIVATION FUNCTION relu WITH SCORE 0.484392117276886\n"
     ]
    }
   ],
   "source": [
    "best_score = 1000.0 # bad\n",
    "\n",
    "# loop over chosen activation functions, train, evaluate on validation\n",
    "for activ in ['sigmoid', 'tanh', 'relu']:\n",
    "    model = nn_model(activation=activ)\n",
    "\n",
    "    history = model.fit(selected_feature_train, price_train,\n",
    "                epochs=50, batch_size=128,\n",
    "                validation_data=(selected_feature_val, price_val))\n",
    "    model_score = score(model.predict(selected_feature_test), price_test)\n",
    "\n",
    "    if model_score < best_score:\n",
    "        best_score = model_score\n",
    "        best_activ = activ\n",
    "        best_model = model\n",
    "        best_train = history\n",
    "\n",
    "print(f\"BEST ACTIVATION FUNCTION {best_activ} WITH SCORE {best_score}\")\n",
    "best_model.save(\"awesome_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize your training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot loss during training\n",
    "def plot_loss(hist):\n",
    "    %matplotlib inline\n",
    "    plt.title('Training Curve')\n",
    "    plt.plot(hist.history['loss'], label='train')\n",
    "    plt.plot(hist.history['val_loss'], label='validation')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Mean squared error\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(best_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Standardize your features:\n",
    "* Typically assumes normally distributed feature, shifting mean to 0 and standard deviation to 1\n",
    "* In theory does not matter for neural networks\n",
    "* In practice tends to matter for neural networks\n",
    "* Scale if using:\n",
    "    - Logistic regression\n",
    "    - Support vector machines\n",
    "    - Perceptrons\n",
    "    - Neural networks\n",
    "    - Principle component analysis\n",
    "* Don't bother if using:\n",
    "    - \"Forest\" methods\n",
    "    - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.158106372019069e-17\n",
      "1.0\n",
      "0.016119707489855875\n",
      "0.9970087526331801\n",
      "0.09330835574898895\n",
      "0.9414925643047213\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate StandardScaler\n",
    "in_scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler to the training set and perform the transformation\n",
    "selected_feature_train = in_scaler.fit_transform(selected_feature_train)\n",
    "\n",
    "# Use the fitted scaler to transform validation and test features\n",
    "selected_feature_val = in_scaler.transform(selected_feature_val)\n",
    "selected_feature_test = in_scaler.transform(selected_feature_test)\n",
    "\n",
    "# Check appropriate scaling\n",
    "print(np.mean(selected_feature_train[:,0]))\n",
    "print(np.std(selected_feature_train[:,0]))\n",
    "\n",
    "print(np.mean(selected_feature_val[:,0]))\n",
    "print(np.std(selected_feature_val[:,0]))\n",
    "\n",
    "print(np.mean(selected_feature_test[:,0]))\n",
    "print(np.std(selected_feature_test[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "18000/18000 [==============================] - 2s 110us/step - loss: 416645830992.7822 - val_loss: 456241971200.0000\n",
      "Epoch 2/100\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 416218113288.8747 - val_loss: 455213074612.2240\n",
      "Epoch 3/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 414424552672.8249 - val_loss: 452046622818.3040\n",
      "Epoch 4/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 410226954396.5582 - val_loss: 445697031667.7120\n",
      "Epoch 5/100\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 402827681289.5574 - val_loss: 435398948421.6320\n",
      "Epoch 6/100\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 391654131164.0463 - val_loss: 420623121121.2800\n",
      "Epoch 7/100\n",
      "18000/18000 [==============================] - 1s 67us/step - loss: 376345412210.2329 - val_loss: 401105718345.7280\n",
      "Epoch 8/100\n",
      "18000/18000 [==============================] - 1s 67us/step - loss: 357004951362.6738 - val_loss: 377348272160.7680\n",
      "Epoch 9/100\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 334074275721.6711 - val_loss: 349949095575.5520\n",
      "Epoch 10/100\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 308316349486.4213 - val_loss: 319859888488.4480\n",
      "Epoch 11/100\n",
      "18000/18000 [==============================] - 1s 60us/step - loss: 280407697857.6498 - val_loss: 287908633772.0320\n",
      "Epoch 12/100\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 251621454686.8907 - val_loss: 255755065360.3840\n",
      "Epoch 13/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 223088187452.0747 - val_loss: 224489321529.3440\n",
      "Epoch 14/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 195986871433.4435 - val_loss: 195616544915.4560\n",
      "Epoch 15/100\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 171263805358.0800 - val_loss: 169729525678.0800\n",
      "Epoch 16/100\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 149628494967.2391 - val_loss: 147810999992.3200\n",
      "Epoch 17/100\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 131402249338.8800 - val_loss: 129783624171.5200\n",
      "Epoch 18/100\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 116576160317.4400 - val_loss: 115628117196.8000\n",
      "Epoch 19/100\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 104910532029.0987 - val_loss: 104911923183.6160\n",
      "Epoch 20/100\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 95997541380.5511 - val_loss: 96762260094.9760\n",
      "Epoch 21/100\n",
      "18000/18000 [==============================] - 1s 62us/step - loss: 89379547319.8649 - val_loss: 90947912531.9680\n",
      "Epoch 22/100\n",
      "14080/18000 [======================>.......] - ETA: 0s - loss: 85828827657.3091"
     ]
    }
   ],
   "source": [
    "model = nn_model()\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "history = model.fit(selected_feature_train, price_train,\n",
    "            epochs=100, batch_size=128,\n",
    "            validation_data=(selected_feature_val, price_val))\n",
    "model_score = score(model.predict(selected_feature_test), price_test)\n",
    "print(model_score)\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the future, consider standardizing outputs as well\n",
    "\n",
    "### Regularize:\n",
    "* Heavily parameterized models like neural networks are prone to overfitting\n",
    "* Popular off-the-shelf tools exist to regularize models and prevent overfitting:\n",
    "    - L2 regularization (weight decay)\n",
    "    - Dropout\n",
    "    - Batch normalization\n",
    "    \n",
    "#### These tools come as standard Keras/TF layers!\n",
    "`model.add(keras.layers.Dropout(rate)`\n",
    "`model.add(keras.layers.ActivityRegularization(l1=0.0, l2=0.0)`\n",
    "`model.add(keras.layers.BatchNormalization())`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping and model checkpointing:\n",
    "#### It's unlikely the last iteration is the best, and who knows how long until the thing is converged. Just grab the best validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/400\n",
      "18000/18000 [==============================] - 2s 122us/step - loss: 416645088222.3218 - val_loss: 456236671434.7521\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 456236671434.75201, saving model to best_model.h5\n",
      "Epoch 2/400\n",
      "18000/18000 [==============================] - 1s 62us/step - loss: 416197657056.5974 - val_loss: 455164537864.1920\n",
      "\n",
      "Epoch 00002: val_loss improved from 456236671434.75201 to 455164537864.19202, saving model to best_model.h5\n",
      "Epoch 3/400\n",
      "18000/18000 [==============================] - 1s 66us/step - loss: 414400001401.7422 - val_loss: 452025910820.8641\n",
      "\n",
      "Epoch 00003: val_loss improved from 455164537864.19202 to 452025910820.86401, saving model to best_model.h5\n",
      "Epoch 4/400\n",
      "18000/18000 [==============================] - 1s 63us/step - loss: 410371744945.4933 - val_loss: 445906115887.1040\n",
      "\n",
      "Epoch 00004: val_loss improved from 452025910820.86401 to 445906115887.10400, saving model to best_model.h5\n",
      "Epoch 5/400\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 403276284967.1395 - val_loss: 435968490078.2080\n",
      "\n",
      "Epoch 00005: val_loss improved from 445906115887.10400 to 435968490078.20801, saving model to best_model.h5\n",
      "Epoch 6/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 392518803273.9555 - val_loss: 421799506477.0560\n",
      "\n",
      "Epoch 00006: val_loss improved from 435968490078.20801 to 421799506477.05603, saving model to best_model.h5\n",
      "Epoch 7/400\n",
      "18000/18000 [==============================] - 1s 65us/step - loss: 377905444203.1786 - val_loss: 403042142257.1520\n",
      "\n",
      "Epoch 00007: val_loss improved from 421799506477.05603 to 403042142257.15198, saving model to best_model.h5\n",
      "Epoch 8/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 359439171758.7627 - val_loss: 380294197673.9840\n",
      "\n",
      "Epoch 00008: val_loss improved from 403042142257.15198 to 380294197673.98401, saving model to best_model.h5\n",
      "Epoch 9/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 337293403342.6205 - val_loss: 353580240863.2320\n",
      "\n",
      "Epoch 00009: val_loss improved from 380294197673.98401 to 353580240863.23199, saving model to best_model.h5\n",
      "Epoch 10/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 312120224048.0142 - val_loss: 323927445864.4480\n",
      "\n",
      "Epoch 00010: val_loss improved from 353580240863.23199 to 323927445864.44800, saving model to best_model.h5\n",
      "Epoch 11/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 284900121611.8329 - val_loss: 292531108839.4240\n",
      "\n",
      "Epoch 00011: val_loss improved from 323927445864.44800 to 292531108839.42401, saving model to best_model.h5\n",
      "Epoch 12/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 256797564250.7947 - val_loss: 261136424370.1760\n",
      "\n",
      "Epoch 00012: val_loss improved from 292531108839.42401 to 261136424370.17599, saving model to best_model.h5\n",
      "Epoch 13/400\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 228902969555.1715 - val_loss: 230564901683.2000\n",
      "\n",
      "Epoch 00013: val_loss improved from 261136424370.17599 to 230564901683.20001, saving model to best_model.h5\n",
      "Epoch 14/400\n",
      "18000/18000 [==============================] - 1s 64us/step - loss: 202254983204.4089 - val_loss: 201884320661.5040\n",
      "\n",
      "Epoch 00014: val_loss improved from 230564901683.20001 to 201884320661.50400, saving model to best_model.h5\n",
      "Epoch 15/400\n",
      "18000/18000 [==============================] - 1s 61us/step - loss: 177782115015.7938 - val_loss: 176104090370.0480\n",
      "\n",
      "Epoch 00015: val_loss improved from 201884320661.50400 to 176104090370.04800, saving model to best_model.h5\n",
      "Epoch 16/400\n",
      "18000/18000 [==============================] - 1s 62us/step - loss: 156188070830.9902 - val_loss: 154209125662.7200\n",
      "\n",
      "Epoch 00016: val_loss improved from 176104090370.04800 to 154209125662.72000, saving model to best_model.h5\n",
      "Epoch 17/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 137806850788.9209 - val_loss: 136010958635.0080\n",
      "\n",
      "Epoch 00017: val_loss improved from 154209125662.72000 to 136010958635.00800, saving model to best_model.h5\n",
      "Epoch 18/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 122601081201.5502 - val_loss: 121301015920.6400\n",
      "\n",
      "Epoch 00018: val_loss improved from 136010958635.00800 to 121301015920.64000, saving model to best_model.h5\n",
      "Epoch 19/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 110464811663.3600 - val_loss: 109919576457.2160\n",
      "\n",
      "Epoch 00019: val_loss improved from 121301015920.64000 to 109919576457.21600, saving model to best_model.h5\n",
      "Epoch 20/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 101121743585.2800 - val_loss: 101571065085.9520\n",
      "\n",
      "Epoch 00020: val_loss improved from 109919576457.21600 to 101571065085.95200, saving model to best_model.h5\n",
      "Epoch 21/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 94068384865.3938 - val_loss: 95244408913.9200\n",
      "\n",
      "Epoch 00021: val_loss improved from 101571065085.95200 to 95244408913.92000, saving model to best_model.h5\n",
      "Epoch 22/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 88756543400.6187 - val_loss: 90569775710.2080\n",
      "\n",
      "Epoch 00022: val_loss improved from 95244408913.92000 to 90569775710.20799, saving model to best_model.h5\n",
      "Epoch 23/400\n",
      "18000/18000 [==============================] - 1s 59us/step - loss: 84712594633.1591 - val_loss: 87002215809.0240\n",
      "\n",
      "Epoch 00023: val_loss improved from 90569775710.20799 to 87002215809.02400, saving model to best_model.h5\n",
      "Epoch 24/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 81603228612.8356 - val_loss: 84256221429.7600\n",
      "\n",
      "Epoch 00024: val_loss improved from 87002215809.02400 to 84256221429.75999, saving model to best_model.h5\n",
      "Epoch 25/400\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 79173733173.0204 - val_loss: 82061945143.2960\n",
      "\n",
      "Epoch 00025: val_loss improved from 84256221429.75999 to 82061945143.29601, saving model to best_model.h5\n",
      "Epoch 26/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 77199145748.2524 - val_loss: 80246319415.2960\n",
      "\n",
      "Epoch 00026: val_loss improved from 82061945143.29601 to 80246319415.29601, saving model to best_model.h5\n",
      "Epoch 27/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 75571666361.4578 - val_loss: 78672987553.7920\n",
      "\n",
      "Epoch 00027: val_loss improved from 80246319415.29601 to 78672987553.79201, saving model to best_model.h5\n",
      "Epoch 28/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 74170761594.6525 - val_loss: 77334466396.1600\n",
      "\n",
      "Epoch 00028: val_loss improved from 78672987553.79201 to 77334466396.16000, saving model to best_model.h5\n",
      "Epoch 29/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 72970862060.4302 - val_loss: 76142404042.7520\n",
      "\n",
      "Epoch 00029: val_loss improved from 77334466396.16000 to 76142404042.75200, saving model to best_model.h5\n",
      "Epoch 30/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 71886676237.4258 - val_loss: 75062966616.0640\n",
      "\n",
      "Epoch 00030: val_loss improved from 76142404042.75200 to 75062966616.06400, saving model to best_model.h5\n",
      "Epoch 31/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 70911220005.0916 - val_loss: 74074677116.9280\n",
      "\n",
      "Epoch 00031: val_loss improved from 75062966616.06400 to 74074677116.92799, saving model to best_model.h5\n",
      "Epoch 32/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 69992349397.4471 - val_loss: 73150938775.5520\n",
      "\n",
      "Epoch 00032: val_loss improved from 74074677116.92799 to 73150938775.55200, saving model to best_model.h5\n",
      "Epoch 33/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 69135029010.4320 - val_loss: 72271365406.7200\n",
      "\n",
      "Epoch 00033: val_loss improved from 73150938775.55200 to 72271365406.72000, saving model to best_model.h5\n",
      "Epoch 34/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 68318729768.5049 - val_loss: 71427720773.6320\n",
      "\n",
      "Epoch 00034: val_loss improved from 72271365406.72000 to 71427720773.63200, saving model to best_model.h5\n",
      "Epoch 35/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 67522541114.7093 - val_loss: 70637041188.8640\n",
      "\n",
      "Epoch 00035: val_loss improved from 71427720773.63200 to 70637041188.86400, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/400\n",
      "18000/18000 [==============================] - 2s 98us/step - loss: 66751103209.0169 - val_loss: 69857330593.7920\n",
      "\n",
      "Epoch 00036: val_loss improved from 70637041188.86400 to 69857330593.79201, saving model to best_model.h5\n",
      "Epoch 37/400\n",
      "18000/18000 [==============================] - 1s 65us/step - loss: 66001783550.4071 - val_loss: 69088201703.4240\n",
      "\n",
      "Epoch 00037: val_loss improved from 69857330593.79201 to 69088201703.42400, saving model to best_model.h5\n",
      "Epoch 38/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 65259026411.9751 - val_loss: 68354475753.4720\n",
      "\n",
      "Epoch 00038: val_loss improved from 69088201703.42400 to 68354475753.47200, saving model to best_model.h5\n",
      "Epoch 39/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 64521657971.1431 - val_loss: 67613030645.7600\n",
      "\n",
      "Epoch 00039: val_loss improved from 68354475753.47200 to 67613030645.76000, saving model to best_model.h5\n",
      "Epoch 40/400\n",
      "18000/18000 [==============================] - 1s 59us/step - loss: 63805681441.9058 - val_loss: 66886318161.9200\n",
      "\n",
      "Epoch 00040: val_loss improved from 67613030645.76000 to 66886318161.92000, saving model to best_model.h5\n",
      "Epoch 41/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 63085259157.9591 - val_loss: 66176570851.3280\n",
      "\n",
      "Epoch 00041: val_loss improved from 66886318161.92000 to 66176570851.32800, saving model to best_model.h5\n",
      "Epoch 42/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 62352147873.7920 - val_loss: 65469578805.2480\n",
      "\n",
      "Epoch 00042: val_loss improved from 66176570851.32800 to 65469578805.24800, saving model to best_model.h5\n",
      "Epoch 43/400\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 61642574128.0142 - val_loss: 64760455593.9840\n",
      "\n",
      "Epoch 00043: val_loss improved from 65469578805.24800 to 64760455593.98400, saving model to best_model.h5\n",
      "Epoch 44/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 60922253053.4969 - val_loss: 64051841368.0640\n",
      "\n",
      "Epoch 00044: val_loss improved from 64760455593.98400 to 64051841368.06400, saving model to best_model.h5\n",
      "Epoch 45/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 60212032700.4160 - val_loss: 63356539469.8240\n",
      "\n",
      "Epoch 00045: val_loss improved from 64051841368.06400 to 63356539469.82400, saving model to best_model.h5\n",
      "Epoch 46/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 59512277110.3289 - val_loss: 62666123214.8480\n",
      "\n",
      "Epoch 00046: val_loss improved from 63356539469.82400 to 62666123214.84800, saving model to best_model.h5\n",
      "Epoch 47/400\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 58790632022.9262 - val_loss: 61966838890.4960\n",
      "\n",
      "Epoch 00047: val_loss improved from 62666123214.84800 to 61966838890.49600, saving model to best_model.h5\n",
      "Epoch 48/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 58078336268.5156 - val_loss: 61275309015.0400\n",
      "\n",
      "Epoch 00048: val_loss improved from 61966838890.49600 to 61275309015.04000, saving model to best_model.h5\n",
      "Epoch 49/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 57367972988.7004 - val_loss: 60577946206.2080\n",
      "\n",
      "Epoch 00049: val_loss improved from 61275309015.04000 to 60577946206.20800, saving model to best_model.h5\n",
      "Epoch 50/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 56665332933.5182 - val_loss: 59889384423.4240\n",
      "\n",
      "Epoch 00050: val_loss improved from 60577946206.20800 to 59889384423.42400, saving model to best_model.h5\n",
      "Epoch 51/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 55965308481.9911 - val_loss: 59207354744.8320\n",
      "\n",
      "Epoch 00051: val_loss improved from 59889384423.42400 to 59207354744.83200, saving model to best_model.h5\n",
      "Epoch 52/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 55272883413.9022 - val_loss: 58513943822.3360\n",
      "\n",
      "Epoch 00052: val_loss improved from 59207354744.83200 to 58513943822.33600, saving model to best_model.h5\n",
      "Epoch 53/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 54556523415.3244 - val_loss: 57838029209.6000\n",
      "\n",
      "Epoch 00053: val_loss improved from 58513943822.33600 to 57838029209.60000, saving model to best_model.h5\n",
      "Epoch 54/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 53875024891.4489 - val_loss: 57163240898.5600\n",
      "\n",
      "Epoch 00054: val_loss improved from 57838029209.60000 to 57163240898.56000, saving model to best_model.h5\n",
      "Epoch 55/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 53175751162.0835 - val_loss: 56490331766.7840\n",
      "\n",
      "Epoch 00055: val_loss improved from 57163240898.56000 to 56490331766.78400, saving model to best_model.h5\n",
      "Epoch 56/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 52504312232.1636 - val_loss: 55827264897.0240\n",
      "\n",
      "Epoch 00056: val_loss improved from 56490331766.78400 to 55827264897.02400, saving model to best_model.h5\n",
      "Epoch 57/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 51812337684.0249 - val_loss: 55167151210.4960\n",
      "\n",
      "Epoch 00057: val_loss improved from 55827264897.02400 to 55167151210.49600, saving model to best_model.h5\n",
      "Epoch 58/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 51132023271.8791 - val_loss: 54514120097.7920\n",
      "\n",
      "Epoch 00058: val_loss improved from 55167151210.49600 to 54514120097.79200, saving model to best_model.h5\n",
      "Epoch 59/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 50464173471.9716 - val_loss: 53875745554.4320\n",
      "\n",
      "Epoch 00059: val_loss improved from 54514120097.79200 to 53875745554.43200, saving model to best_model.h5\n",
      "Epoch 60/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 49799531772.1316 - val_loss: 53240766070.7840\n",
      "\n",
      "Epoch 00060: val_loss improved from 53875745554.43200 to 53240766070.78400, saving model to best_model.h5\n",
      "Epoch 61/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 49152544068.9493 - val_loss: 52635786477.5680\n",
      "\n",
      "Epoch 00061: val_loss improved from 53240766070.78400 to 52635786477.56800, saving model to best_model.h5\n",
      "Epoch 62/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 48499499022.5636 - val_loss: 52019728154.6240\n",
      "\n",
      "Epoch 00062: val_loss improved from 52635786477.56800 to 52019728154.62400, saving model to best_model.h5\n",
      "Epoch 63/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 47870956175.3600 - val_loss: 51429909331.9680\n",
      "\n",
      "Epoch 00063: val_loss improved from 52019728154.62400 to 51429909331.96800, saving model to best_model.h5\n",
      "Epoch 64/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 47250644325.7173 - val_loss: 50849696153.6000\n",
      "\n",
      "Epoch 00064: val_loss improved from 51429909331.96800 to 50849696153.60000, saving model to best_model.h5\n",
      "Epoch 65/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 46627214440.6756 - val_loss: 50274833858.5600\n",
      "\n",
      "Epoch 00065: val_loss improved from 50849696153.60000 to 50274833858.56000, saving model to best_model.h5\n",
      "Epoch 66/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 46028059985.6924 - val_loss: 49711005827.0720\n",
      "\n",
      "Epoch 00066: val_loss improved from 50274833858.56000 to 49711005827.07200, saving model to best_model.h5\n",
      "Epoch 67/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 45431022849.5929 - val_loss: 49183650480.1280\n",
      "\n",
      "Epoch 00067: val_loss improved from 49711005827.07200 to 49183650480.12800, saving model to best_model.h5\n",
      "Epoch 68/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 44862129751.8364 - val_loss: 48653637189.6320\n",
      "\n",
      "Epoch 00068: val_loss improved from 49183650480.12800 to 48653637189.63200, saving model to best_model.h5\n",
      "Epoch 69/400\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 44295542971.5058 - val_loss: 48132611080.1920\n",
      "\n",
      "Epoch 00069: val_loss improved from 48653637189.63200 to 48132611080.19200, saving model to best_model.h5\n",
      "Epoch 70/400\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 43738164989.4969 - val_loss: 47630259355.6480\n",
      "\n",
      "Epoch 00070: val_loss improved from 48132611080.19200 to 47630259355.64800, saving model to best_model.h5\n",
      "Epoch 71/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 48us/step - loss: 43203405716.5938 - val_loss: 47153690738.6880\n",
      "\n",
      "Epoch 00071: val_loss improved from 47630259355.64800 to 47153690738.68800, saving model to best_model.h5\n",
      "Epoch 72/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 42679820593.8347 - val_loss: 46684520611.8400\n",
      "\n",
      "Epoch 00072: val_loss improved from 47153690738.68800 to 46684520611.84000, saving model to best_model.h5\n",
      "Epoch 73/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 42188650075.4773 - val_loss: 46244600971.2640\n",
      "\n",
      "Epoch 00073: val_loss improved from 46684520611.84000 to 46244600971.26400, saving model to best_model.h5\n",
      "Epoch 74/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 41699817934.3929 - val_loss: 45842708004.8640\n",
      "\n",
      "Epoch 00074: val_loss improved from 46244600971.26400 to 45842708004.86400, saving model to best_model.h5\n",
      "Epoch 75/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 41247992497.9484 - val_loss: 45455436611.5840\n",
      "\n",
      "Epoch 00075: val_loss improved from 45842708004.86400 to 45455436611.58400, saving model to best_model.h5\n",
      "Epoch 76/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 40832375728.8107 - val_loss: 45075864322.0480\n",
      "\n",
      "Epoch 00076: val_loss improved from 45455436611.58400 to 45075864322.04800, saving model to best_model.h5\n",
      "Epoch 77/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 40425152253.4969 - val_loss: 44754962612.2240\n",
      "\n",
      "Epoch 00077: val_loss improved from 45075864322.04800 to 44754962612.22400, saving model to best_model.h5\n",
      "Epoch 78/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 40050075356.7289 - val_loss: 44456154431.4880\n",
      "\n",
      "Epoch 00078: val_loss improved from 44754962612.22400 to 44456154431.48800, saving model to best_model.h5\n",
      "Epoch 79/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 39703157855.5733 - val_loss: 44172761563.1360\n",
      "\n",
      "Epoch 00079: val_loss improved from 44456154431.48800 to 44172761563.13600, saving model to best_model.h5\n",
      "Epoch 80/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 39387283319.4667 - val_loss: 43907886809.0880\n",
      "\n",
      "Epoch 00080: val_loss improved from 44172761563.13600 to 43907886809.08800, saving model to best_model.h5\n",
      "Epoch 81/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 39083220001.6782 - val_loss: 43669057470.4640\n",
      "\n",
      "Epoch 00081: val_loss improved from 43907886809.08800 to 43669057470.46400, saving model to best_model.h5\n",
      "Epoch 82/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 38810774277.6889 - val_loss: 43458989686.7840\n",
      "\n",
      "Epoch 00082: val_loss improved from 43669057470.46400 to 43458989686.78400, saving model to best_model.h5\n",
      "Epoch 83/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 38558436786.1760 - val_loss: 43272881373.1840\n",
      "\n",
      "Epoch 00083: val_loss improved from 43458989686.78400 to 43272881373.18400, saving model to best_model.h5\n",
      "Epoch 84/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 38332917643.4916 - val_loss: 43102741364.7360\n",
      "\n",
      "Epoch 00084: val_loss improved from 43272881373.18400 to 43102741364.73600, saving model to best_model.h5\n",
      "Epoch 85/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 38134555435.9182 - val_loss: 42940679946.2400\n",
      "\n",
      "Epoch 00085: val_loss improved from 43102741364.73600 to 42940679946.24000, saving model to best_model.h5\n",
      "Epoch 86/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 37955865983.2036 - val_loss: 42818599026.6880\n",
      "\n",
      "Epoch 00086: val_loss improved from 42940679946.24000 to 42818599026.68800, saving model to best_model.h5\n",
      "Epoch 87/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 37772205965.3120 - val_loss: 42683706572.8000\n",
      "\n",
      "Epoch 00087: val_loss improved from 42818599026.68800 to 42683706572.80000, saving model to best_model.h5\n",
      "Epoch 88/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 37610136460.4018 - val_loss: 42555996372.9920\n",
      "\n",
      "Epoch 00088: val_loss improved from 42683706572.80000 to 42555996372.99200, saving model to best_model.h5\n",
      "Epoch 89/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 37465811721.3298 - val_loss: 42423765237.7600\n",
      "\n",
      "Epoch 00089: val_loss improved from 42555996372.99200 to 42423765237.76000, saving model to best_model.h5\n",
      "Epoch 90/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 37325928704.6827 - val_loss: 42354548867.0720\n",
      "\n",
      "Epoch 00090: val_loss improved from 42423765237.76000 to 42354548867.07200, saving model to best_model.h5\n",
      "Epoch 91/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 37198905689.8844 - val_loss: 42212640423.9360\n",
      "\n",
      "Epoch 00091: val_loss improved from 42354548867.07200 to 42212640423.93600, saving model to best_model.h5\n",
      "Epoch 92/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 37075291034.9653 - val_loss: 42115949166.5920\n",
      "\n",
      "Epoch 00092: val_loss improved from 42212640423.93600 to 42115949166.59200, saving model to best_model.h5\n",
      "Epoch 93/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 36960614470.9973 - val_loss: 42047881707.5200\n",
      "\n",
      "Epoch 00093: val_loss improved from 42115949166.59200 to 42047881707.52000, saving model to best_model.h5\n",
      "Epoch 94/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 36847697748.8782 - val_loss: 41958313590.7840\n",
      "\n",
      "Epoch 00094: val_loss improved from 42047881707.52000 to 41958313590.78400, saving model to best_model.h5\n",
      "Epoch 95/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 36746772348.9280 - val_loss: 41892034248.7040\n",
      "\n",
      "Epoch 00095: val_loss improved from 41958313590.78400 to 41892034248.70400, saving model to best_model.h5\n",
      "Epoch 96/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 36648524440.4622 - val_loss: 41777164386.3040\n",
      "\n",
      "Epoch 00096: val_loss improved from 41892034248.70400 to 41777164386.30400, saving model to best_model.h5\n",
      "Epoch 97/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 36547636146.6311 - val_loss: 41662185406.4640\n",
      "\n",
      "Epoch 00097: val_loss improved from 41777164386.30400 to 41662185406.46400, saving model to best_model.h5\n",
      "Epoch 98/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 36446482890.7520 - val_loss: 41581772079.1040\n",
      "\n",
      "Epoch 00098: val_loss improved from 41662185406.46400 to 41581772079.10400, saving model to best_model.h5\n",
      "Epoch 99/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 36346986167.4098 - val_loss: 41517011468.2880\n",
      "\n",
      "Epoch 00099: val_loss improved from 41581772079.10400 to 41517011468.28800, saving model to best_model.h5\n",
      "Epoch 100/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 36259087999.8862 - val_loss: 41427063930.8800\n",
      "\n",
      "Epoch 00100: val_loss improved from 41517011468.28800 to 41427063930.88000, saving model to best_model.h5\n",
      "Epoch 101/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 36178341631.3173 - val_loss: 41330681839.6160\n",
      "\n",
      "Epoch 00101: val_loss improved from 41427063930.88000 to 41330681839.61600, saving model to best_model.h5\n",
      "Epoch 102/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 36082635148.8569 - val_loss: 41224239513.6000\n",
      "\n",
      "Epoch 00102: val_loss improved from 41330681839.61600 to 41224239513.60000, saving model to best_model.h5\n",
      "Epoch 103/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 35997897573.2622 - val_loss: 41157933662.2080\n",
      "\n",
      "Epoch 00103: val_loss improved from 41224239513.60000 to 41157933662.20800, saving model to best_model.h5\n",
      "Epoch 104/400\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 35904001887.8009 - val_loss: 41071589883.9040\n",
      "\n",
      "Epoch 00104: val_loss improved from 41157933662.20800 to 41071589883.90400, saving model to best_model.h5\n",
      "Epoch 105/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 35819676627.3991 - val_loss: 40975216181.2480\n",
      "\n",
      "Epoch 00105: val_loss improved from 41071589883.90400 to 40975216181.24800, saving model to best_model.h5\n",
      "Epoch 106/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 49us/step - loss: 35734134747.5911 - val_loss: 40943851732.9920\n",
      "\n",
      "Epoch 00106: val_loss improved from 40975216181.24800 to 40943851732.99200, saving model to best_model.h5\n",
      "Epoch 107/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 35662241264.0711 - val_loss: 40829336387.5840\n",
      "\n",
      "Epoch 00107: val_loss improved from 40943851732.99200 to 40829336387.58400, saving model to best_model.h5\n",
      "Epoch 108/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 35598459100.2738 - val_loss: 40757999697.9200\n",
      "\n",
      "Epoch 00108: val_loss improved from 40829336387.58400 to 40757999697.92000, saving model to best_model.h5\n",
      "Epoch 109/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 35503797390.9049 - val_loss: 40652122750.9760\n",
      "\n",
      "Epoch 00109: val_loss improved from 40757999697.92000 to 40652122750.97600, saving model to best_model.h5\n",
      "Epoch 110/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 35426803278.7342 - val_loss: 40556733726.7200\n",
      "\n",
      "Epoch 00110: val_loss improved from 40652122750.97600 to 40556733726.72000, saving model to best_model.h5\n",
      "Epoch 111/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 35356519759.8720 - val_loss: 40471838654.4640\n",
      "\n",
      "Epoch 00111: val_loss improved from 40556733726.72000 to 40471838654.46400, saving model to best_model.h5\n",
      "Epoch 112/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 35281241465.7422 - val_loss: 40392378253.3120\n",
      "\n",
      "Epoch 00112: val_loss improved from 40471838654.46400 to 40392378253.31200, saving model to best_model.h5\n",
      "Epoch 113/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 35210804501.6178 - val_loss: 40340045004.8000\n",
      "\n",
      "Epoch 00113: val_loss improved from 40392378253.31200 to 40340045004.80000, saving model to best_model.h5\n",
      "Epoch 114/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 35140929020.8142 - val_loss: 40258438103.0400\n",
      "\n",
      "Epoch 00114: val_loss improved from 40340045004.80000 to 40258438103.04000, saving model to best_model.h5\n",
      "Epoch 115/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 35077332802.6738 - val_loss: 40180619378.6880\n",
      "\n",
      "Epoch 00115: val_loss improved from 40258438103.04000 to 40180619378.68800, saving model to best_model.h5\n",
      "Epoch 116/400\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 35005742954.7236 - val_loss: 40121166626.8160\n",
      "\n",
      "Epoch 00116: val_loss improved from 40180619378.68800 to 40121166626.81600, saving model to best_model.h5\n",
      "Epoch 117/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 34944534608.0996 - val_loss: 40068653776.8960\n",
      "\n",
      "Epoch 00117: val_loss improved from 40121166626.81600 to 40068653776.89600, saving model to best_model.h5\n",
      "Epoch 118/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 34878449454.6489 - val_loss: 40017548607.4880\n",
      "\n",
      "Epoch 00118: val_loss improved from 40068653776.89600 to 40017548607.48800, saving model to best_model.h5\n",
      "Epoch 119/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 34823080516.7218 - val_loss: 39914759979.0080\n",
      "\n",
      "Epoch 00119: val_loss improved from 40017548607.48800 to 39914759979.00800, saving model to best_model.h5\n",
      "Epoch 120/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 34754789215.8009 - val_loss: 39857587814.4000\n",
      "\n",
      "Epoch 00120: val_loss improved from 39914759979.00800 to 39857587814.40000, saving model to best_model.h5\n",
      "Epoch 121/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 34694084513.3369 - val_loss: 39801243566.0800\n",
      "\n",
      "Epoch 00121: val_loss improved from 39857587814.40000 to 39801243566.08000, saving model to best_model.h5\n",
      "Epoch 122/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 34627529823.5733 - val_loss: 39761618239.4880\n",
      "\n",
      "Epoch 00122: val_loss improved from 39801243566.08000 to 39761618239.48800, saving model to best_model.h5\n",
      "Epoch 123/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 34572439009.5076 - val_loss: 39677768040.4480\n",
      "\n",
      "Epoch 00123: val_loss improved from 39761618239.48800 to 39677768040.44800, saving model to best_model.h5\n",
      "Epoch 124/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 34525818759.8507 - val_loss: 39604220788.7360\n",
      "\n",
      "Epoch 00124: val_loss improved from 39677768040.44800 to 39604220788.73600, saving model to best_model.h5\n",
      "Epoch 125/400\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 34456151676.2453 - val_loss: 39527747256.3200\n",
      "\n",
      "Epoch 00125: val_loss improved from 39604220788.73600 to 39527747256.32000, saving model to best_model.h5\n",
      "Epoch 126/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 34412487147.5200 - val_loss: 39495456587.7760\n",
      "\n",
      "Epoch 00126: val_loss improved from 39527747256.32000 to 39495456587.77600, saving model to best_model.h5\n",
      "Epoch 127/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 34360288342.4711 - val_loss: 39408326049.7920\n",
      "\n",
      "Epoch 00127: val_loss improved from 39495456587.77600 to 39408326049.79200, saving model to best_model.h5\n",
      "Epoch 128/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 34299272559.7298 - val_loss: 39382138880.0000\n",
      "\n",
      "Epoch 00128: val_loss improved from 39408326049.79200 to 39382138880.00000, saving model to best_model.h5\n",
      "Epoch 129/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 34249322011.7618 - val_loss: 39304483110.9120\n",
      "\n",
      "Epoch 00129: val_loss improved from 39382138880.00000 to 39304483110.91200, saving model to best_model.h5\n",
      "Epoch 130/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 34207225879.6658 - val_loss: 39256446763.0080\n",
      "\n",
      "Epoch 00130: val_loss improved from 39304483110.91200 to 39256446763.00800, saving model to best_model.h5\n",
      "Epoch 131/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 34147421315.9822 - val_loss: 39216078454.7840\n",
      "\n",
      "Epoch 00131: val_loss improved from 39256446763.00800 to 39216078454.78400, saving model to best_model.h5\n",
      "Epoch 132/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 34096112285.9236 - val_loss: 39169674608.6400\n",
      "\n",
      "Epoch 00132: val_loss improved from 39216078454.78400 to 39169674608.64000, saving model to best_model.h5\n",
      "Epoch 133/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 34042727195.5342 - val_loss: 39110353518.5920\n",
      "\n",
      "Epoch 00133: val_loss improved from 39169674608.64000 to 39110353518.59200, saving model to best_model.h5\n",
      "Epoch 134/400\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 34000066867.6551 - val_loss: 39095101784.0640\n",
      "\n",
      "Epoch 00134: val_loss improved from 39110353518.59200 to 39095101784.06400, saving model to best_model.h5\n",
      "Epoch 135/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 33945684069.9449 - val_loss: 39013146099.7120\n",
      "\n",
      "Epoch 00135: val_loss improved from 39095101784.06400 to 39013146099.71200, saving model to best_model.h5\n",
      "Epoch 136/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 33900133646.3360 - val_loss: 38977756069.8880\n",
      "\n",
      "Epoch 00136: val_loss improved from 39013146099.71200 to 38977756069.88800, saving model to best_model.h5\n",
      "Epoch 137/400\n",
      "18000/18000 [==============================] - 1s 37us/step - loss: 33863613846.8693 - val_loss: 38907818999.8080\n",
      "\n",
      "Epoch 00137: val_loss improved from 38977756069.88800 to 38907818999.80800, saving model to best_model.h5\n",
      "Epoch 138/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 33816919667.1431 - val_loss: 38869480570.8800\n",
      "\n",
      "Epoch 00138: val_loss improved from 38907818999.80800 to 38869480570.88000, saving model to best_model.h5\n",
      "Epoch 139/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 33769286493.0702 - val_loss: 38835417055.2320\n",
      "\n",
      "Epoch 00139: val_loss improved from 38869480570.88000 to 38835417055.23200, saving model to best_model.h5\n",
      "Epoch 140/400\n",
      "18000/18000 [==============================] - 1s 38us/step - loss: 33736138264.1209 - val_loss: 38772498497.5360\n",
      "\n",
      "Epoch 00140: val_loss improved from 38835417055.23200 to 38772498497.53600, saving model to best_model.h5\n",
      "Epoch 141/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 48us/step - loss: 33696992580.9493 - val_loss: 38764493045.7600\n",
      "\n",
      "Epoch 00141: val_loss improved from 38772498497.53600 to 38764493045.76000, saving model to best_model.h5\n",
      "Epoch 142/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 33650815842.5316 - val_loss: 38692661329.9200\n",
      "\n",
      "Epoch 00142: val_loss improved from 38764493045.76000 to 38692661329.92000, saving model to best_model.h5\n",
      "Epoch 143/400\n",
      "18000/18000 [==============================] - 1s 61us/step - loss: 33599817602.3893 - val_loss: 38661002297.3440\n",
      "\n",
      "Epoch 00143: val_loss improved from 38692661329.92000 to 38661002297.34400, saving model to best_model.h5\n",
      "Epoch 144/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 33556876561.9769 - val_loss: 38601671704.5760\n",
      "\n",
      "Epoch 00144: val_loss improved from 38661002297.34400 to 38601671704.57600, saving model to best_model.h5\n",
      "Epoch 145/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 33527061834.4107 - val_loss: 38563541614.5920\n",
      "\n",
      "Epoch 00145: val_loss improved from 38601671704.57600 to 38563541614.59200, saving model to best_model.h5\n",
      "Epoch 146/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 33482383301.7458 - val_loss: 38534326190.0800\n",
      "\n",
      "Epoch 00146: val_loss improved from 38563541614.59200 to 38534326190.08000, saving model to best_model.h5\n",
      "Epoch 147/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 33450574816.1422 - val_loss: 38526453383.1680\n",
      "\n",
      "Epoch 00147: val_loss improved from 38534326190.08000 to 38526453383.16800, saving model to best_model.h5\n",
      "Epoch 148/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 33417307979.7760 - val_loss: 38463937380.3520\n",
      "\n",
      "Epoch 00148: val_loss improved from 38526453383.16800 to 38463937380.35200, saving model to best_model.h5\n",
      "Epoch 149/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 33383097546.9796 - val_loss: 38405969543.1680\n",
      "\n",
      "Epoch 00149: val_loss improved from 38463937380.35200 to 38405969543.16800, saving model to best_model.h5\n",
      "Epoch 150/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 33343809164.6293 - val_loss: 38398706647.0400\n",
      "\n",
      "Epoch 00150: val_loss improved from 38405969543.16800 to 38398706647.04000, saving model to best_model.h5\n",
      "Epoch 151/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 33314583004.9564 - val_loss: 38365830545.4080\n",
      "\n",
      "Epoch 00151: val_loss improved from 38398706647.04000 to 38365830545.40800, saving model to best_model.h5\n",
      "Epoch 152/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 33269216420.7502 - val_loss: 38295932043.2640\n",
      "\n",
      "Epoch 00152: val_loss improved from 38365830545.40800 to 38295932043.26400, saving model to best_model.h5\n",
      "Epoch 153/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 33237643158.4142 - val_loss: 38252653740.0320\n",
      "\n",
      "Epoch 00153: val_loss improved from 38295932043.26400 to 38252653740.03200, saving model to best_model.h5\n",
      "Epoch 154/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 33207404207.2178 - val_loss: 38242748104.7040\n",
      "\n",
      "Epoch 00154: val_loss improved from 38252653740.03200 to 38242748104.70400, saving model to best_model.h5\n",
      "Epoch 155/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 33181476101.2338 - val_loss: 38207427149.8240\n",
      "\n",
      "Epoch 00155: val_loss improved from 38242748104.70400 to 38207427149.82400, saving model to best_model.h5\n",
      "Epoch 156/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 33148104620.2596 - val_loss: 38219259052.0320\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 38207427149.82400\n",
      "Epoch 157/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 33117534981.6889 - val_loss: 38169959399.4240\n",
      "\n",
      "Epoch 00157: val_loss improved from 38207427149.82400 to 38169959399.42400, saving model to best_model.h5\n",
      "Epoch 158/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 33083978980.4658 - val_loss: 38128672538.6240\n",
      "\n",
      "Epoch 00158: val_loss improved from 38169959399.42400 to 38128672538.62400, saving model to best_model.h5\n",
      "Epoch 159/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 33055623803.3351 - val_loss: 38104472518.6560\n",
      "\n",
      "Epoch 00159: val_loss improved from 38128672538.62400 to 38104472518.65600, saving model to best_model.h5\n",
      "Epoch 160/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 33030845819.5627 - val_loss: 38050051588.0960\n",
      "\n",
      "Epoch 00160: val_loss improved from 38104472518.65600 to 38050051588.09600, saving model to best_model.h5\n",
      "Epoch 161/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 33003038649.0027 - val_loss: 38024709996.5440\n",
      "\n",
      "Epoch 00161: val_loss improved from 38050051588.09600 to 38024709996.54400, saving model to best_model.h5\n",
      "Epoch 162/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 32978092068.4089 - val_loss: 37985452359.6800\n",
      "\n",
      "Epoch 00162: val_loss improved from 38024709996.54400 to 37985452359.68000, saving model to best_model.h5\n",
      "Epoch 163/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 32952904169.6996 - val_loss: 37950014291.9680\n",
      "\n",
      "Epoch 00163: val_loss improved from 37985452359.68000 to 37950014291.96800, saving model to best_model.h5\n",
      "Epoch 164/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 32922112342.2436 - val_loss: 37944827445.2480\n",
      "\n",
      "Epoch 00164: val_loss improved from 37950014291.96800 to 37944827445.24800, saving model to best_model.h5\n",
      "Epoch 165/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 32901939822.5920 - val_loss: 37927616806.9120\n",
      "\n",
      "Epoch 00165: val_loss improved from 37944827445.24800 to 37927616806.91200, saving model to best_model.h5\n",
      "Epoch 166/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 32874172237.5964 - val_loss: 37875793133.5680\n",
      "\n",
      "Epoch 00166: val_loss improved from 37927616806.91200 to 37875793133.56800, saving model to best_model.h5\n",
      "Epoch 167/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 32855672127.4880 - val_loss: 37850992934.9120\n",
      "\n",
      "Epoch 00167: val_loss improved from 37875793133.56800 to 37850992934.91200, saving model to best_model.h5\n",
      "Epoch 168/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 32833501797.4898 - val_loss: 37854491049.9840\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 37850992934.91200\n",
      "Epoch 169/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 32802107472.0996 - val_loss: 37792669990.9120\n",
      "\n",
      "Epoch 00169: val_loss improved from 37850992934.91200 to 37792669990.91200, saving model to best_model.h5\n",
      "Epoch 170/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 32793616982.9262 - val_loss: 37775277621.2480\n",
      "\n",
      "Epoch 00170: val_loss improved from 37792669990.91200 to 37775277621.24800, saving model to best_model.h5\n",
      "Epoch 171/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 32764825609.1022 - val_loss: 37755452555.2640\n",
      "\n",
      "Epoch 00171: val_loss improved from 37775277621.24800 to 37755452555.26400, saving model to best_model.h5\n",
      "Epoch 172/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 32735820910.1369 - val_loss: 37756339093.5040\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 37755452555.26400\n",
      "Epoch 173/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 32725035748.9209 - val_loss: 37703976288.2560\n",
      "\n",
      "Epoch 00173: val_loss improved from 37755452555.26400 to 37703976288.25600, saving model to best_model.h5\n",
      "Epoch 174/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 32696046611.1147 - val_loss: 37715366707.2000\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 37703976288.25600\n",
      "Epoch 175/400\n",
      "18000/18000 [==============================] - 1s 38us/step - loss: 32676534059.0080 - val_loss: 37698144731.1360\n",
      "\n",
      "Epoch 00175: val_loss improved from 37703976288.25600 to 37698144731.13600, saving model to best_model.h5\n",
      "Epoch 176/400\n",
      "18000/18000 [==============================] - 1s 39us/step - loss: 32661015862.3858 - val_loss: 37658896662.5280\n",
      "\n",
      "Epoch 00176: val_loss improved from 37698144731.13600 to 37658896662.52800, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 32635938470.1156 - val_loss: 37633564672.0000\n",
      "\n",
      "Epoch 00177: val_loss improved from 37658896662.52800 to 37633564672.00000, saving model to best_model.h5\n",
      "Epoch 178/400\n",
      "18000/18000 [==============================] - 1s 61us/step - loss: 32617037973.2764 - val_loss: 37613019136.0000\n",
      "\n",
      "Epoch 00178: val_loss improved from 37633564672.00000 to 37613019136.00000, saving model to best_model.h5\n",
      "Epoch 179/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 32604205929.8133 - val_loss: 37593009618.9440\n",
      "\n",
      "Epoch 00179: val_loss improved from 37613019136.00000 to 37593009618.94400, saving model to best_model.h5\n",
      "Epoch 180/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 32580118923.0364 - val_loss: 37583775039.4880\n",
      "\n",
      "Epoch 00180: val_loss improved from 37593009618.94400 to 37583775039.48800, saving model to best_model.h5\n",
      "Epoch 181/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 32562514065.6356 - val_loss: 37531538358.2720\n",
      "\n",
      "Epoch 00181: val_loss improved from 37583775039.48800 to 37531538358.27200, saving model to best_model.h5\n",
      "Epoch 182/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 32539511848.9600 - val_loss: 37535664275.4560\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 37531538358.27200\n",
      "Epoch 183/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 32516917669.4329 - val_loss: 37530230816.7680\n",
      "\n",
      "Epoch 00183: val_loss improved from 37531538358.27200 to 37530230816.76800, saving model to best_model.h5\n",
      "Epoch 184/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 32516181002.0124 - val_loss: 37478621970.4320\n",
      "\n",
      "Epoch 00184: val_loss improved from 37530230816.76800 to 37478621970.43200, saving model to best_model.h5\n",
      "Epoch 185/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 32489994611.3707 - val_loss: 37462110240.7680\n",
      "\n",
      "Epoch 00185: val_loss improved from 37478621970.43200 to 37462110240.76800, saving model to best_model.h5\n",
      "Epoch 186/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 32473291463.7938 - val_loss: 37428731379.7120\n",
      "\n",
      "Epoch 00186: val_loss improved from 37462110240.76800 to 37428731379.71200, saving model to best_model.h5\n",
      "Epoch 187/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 32459515437.9662 - val_loss: 37460495073.2800\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 37428731379.71200\n",
      "Epoch 188/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 32428426166.2720 - val_loss: 37445266735.1040\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 37428731379.71200\n",
      "Epoch 189/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 32415488225.7351 - val_loss: 37419267293.1840\n",
      "\n",
      "Epoch 00189: val_loss improved from 37428731379.71200 to 37419267293.18400, saving model to best_model.h5\n",
      "Epoch 190/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 32401736013.1413 - val_loss: 37395112722.4320\n",
      "\n",
      "Epoch 00190: val_loss improved from 37419267293.18400 to 37395112722.43200, saving model to best_model.h5\n",
      "Epoch 191/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 32382055048.0782 - val_loss: 37396381433.8560\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 37395112722.43200\n",
      "Epoch 192/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 32367028943.0756 - val_loss: 37372049457.1520\n",
      "\n",
      "Epoch 00192: val_loss improved from 37395112722.43200 to 37372049457.15200, saving model to best_model.h5\n",
      "Epoch 193/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 32348244435.8542 - val_loss: 37340409069.5680\n",
      "\n",
      "Epoch 00193: val_loss improved from 37372049457.15200 to 37340409069.56800, saving model to best_model.h5\n",
      "Epoch 194/400\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 32334587252.2809 - val_loss: 37352293957.6320\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 37340409069.56800\n",
      "Epoch 195/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 32309357840.1564 - val_loss: 37316448616.4480\n",
      "\n",
      "Epoch 00195: val_loss improved from 37340409069.56800 to 37316448616.44800, saving model to best_model.h5\n",
      "Epoch 196/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 32297495629.3689 - val_loss: 37289642000.3840\n",
      "\n",
      "Epoch 00196: val_loss improved from 37316448616.44800 to 37289642000.38400, saving model to best_model.h5\n",
      "Epoch 197/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 32276998524.4729 - val_loss: 37288914550.7840\n",
      "\n",
      "Epoch 00197: val_loss improved from 37289642000.38400 to 37288914550.78400, saving model to best_model.h5\n",
      "Epoch 198/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 32264187106.6453 - val_loss: 37284457676.8000\n",
      "\n",
      "Epoch 00198: val_loss improved from 37288914550.78400 to 37284457676.80000, saving model to best_model.h5\n",
      "Epoch 199/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 32245145574.5138 - val_loss: 37217404649.4720\n",
      "\n",
      "Epoch 00199: val_loss improved from 37284457676.80000 to 37217404649.47200, saving model to best_model.h5\n",
      "Epoch 200/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 32229146667.6907 - val_loss: 37223668711.4240\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 37217404649.47200\n",
      "Epoch 201/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 32218305704.3911 - val_loss: 37222694715.3920\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 37217404649.47200\n",
      "Epoch 202/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 32197559502.6204 - val_loss: 37184812810.2400\n",
      "\n",
      "Epoch 00202: val_loss improved from 37217404649.47200 to 37184812810.24000, saving model to best_model.h5\n",
      "Epoch 203/400\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 32177727078.4000 - val_loss: 37198403403.7760\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 37184812810.24000\n",
      "Epoch 204/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 32168662237.1840 - val_loss: 37165747732.4800\n",
      "\n",
      "Epoch 00204: val_loss improved from 37184812810.24000 to 37165747732.48000, saving model to best_model.h5\n",
      "Epoch 205/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 32151417207.0116 - val_loss: 37141246738.4320\n",
      "\n",
      "Epoch 00205: val_loss improved from 37165747732.48000 to 37141246738.43200, saving model to best_model.h5\n",
      "Epoch 206/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 32129678367.8578 - val_loss: 37141981790.2080\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 37141246738.43200\n",
      "Epoch 207/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 32116178041.0596 - val_loss: 37134686453.7600\n",
      "\n",
      "Epoch 00207: val_loss improved from 37141246738.43200 to 37134686453.76000, saving model to best_model.h5\n",
      "Epoch 208/400\n",
      "18000/18000 [==============================] - 1s 64us/step - loss: 32096660937.8418 - val_loss: 37101655097.3440\n",
      "\n",
      "Epoch 00208: val_loss improved from 37134686453.76000 to 37101655097.34400, saving model to best_model.h5\n",
      "Epoch 209/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 32091785536.3982 - val_loss: 37086656135.1680\n",
      "\n",
      "Epoch 00209: val_loss improved from 37101655097.34400 to 37086656135.16800, saving model to best_model.h5\n",
      "Epoch 210/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 32068059618.4178 - val_loss: 37059672178.6880\n",
      "\n",
      "Epoch 00210: val_loss improved from 37086656135.16800 to 37059672178.68800, saving model to best_model.h5\n",
      "Epoch 211/400\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 32055697000.2204 - val_loss: 37083339522.0480\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 37059672178.68800\n",
      "Epoch 212/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 32046330178.2187 - val_loss: 37048108417.0240\n",
      "\n",
      "Epoch 00212: val_loss improved from 37059672178.68800 to 37048108417.02400, saving model to best_model.h5\n",
      "Epoch 213/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 32022041417.0453 - val_loss: 37044407304.1920\n",
      "\n",
      "Epoch 00213: val_loss improved from 37048108417.02400 to 37044407304.19200, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 32003748646.4569 - val_loss: 37021387816.9600\n",
      "\n",
      "Epoch 00214: val_loss improved from 37044407304.19200 to 37021387816.96000, saving model to best_model.h5\n",
      "Epoch 215/400\n",
      "18000/18000 [==============================] - 1s 75us/step - loss: 31999111532.0889 - val_loss: 37007264350.2080\n",
      "\n",
      "Epoch 00215: val_loss improved from 37021387816.96000 to 37007264350.20800, saving model to best_model.h5\n",
      "Epoch 216/400\n",
      "18000/18000 [==============================] - 2s 84us/step - loss: 31977107976.6471 - val_loss: 36994526281.7280\n",
      "\n",
      "Epoch 00216: val_loss improved from 37007264350.20800 to 36994526281.72800, saving model to best_model.h5\n",
      "Epoch 217/400\n",
      "18000/18000 [==============================] - 1s 78us/step - loss: 31959212053.8453 - val_loss: 36972025118.7200\n",
      "\n",
      "Epoch 00217: val_loss improved from 36994526281.72800 to 36972025118.72000, saving model to best_model.h5\n",
      "Epoch 218/400\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 31954721705.5289 - val_loss: 36935423000.5760\n",
      "\n",
      "Epoch 00218: val_loss improved from 36972025118.72000 to 36935423000.57600, saving model to best_model.h5\n",
      "Epoch 219/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 31950323566.3644 - val_loss: 36941218086.9120\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 36935423000.57600\n",
      "Epoch 220/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 31939434417.7209 - val_loss: 36939945443.3280\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 36935423000.57600\n",
      "Epoch 221/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 31906310677.3902 - val_loss: 36920803098.6240\n",
      "\n",
      "Epoch 00221: val_loss improved from 36935423000.57600 to 36920803098.62400, saving model to best_model.h5\n",
      "Epoch 222/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 31881853155.5556 - val_loss: 36892125331.4560\n",
      "\n",
      "Epoch 00222: val_loss improved from 36920803098.62400 to 36892125331.45600, saving model to best_model.h5\n",
      "Epoch 223/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 31880552184.0356 - val_loss: 36838996180.9920\n",
      "\n",
      "Epoch 00223: val_loss improved from 36892125331.45600 to 36838996180.99200, saving model to best_model.h5\n",
      "Epoch 224/400\n",
      "18000/18000 [==============================] - 1s 60us/step - loss: 31862093484.4871 - val_loss: 36860079603.7120\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 36838996180.99200\n",
      "Epoch 225/400\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 31843552403.4560 - val_loss: 36859384856.5760\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 36838996180.99200\n",
      "Epoch 226/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 31824666460.1600 - val_loss: 36862026219.5200\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 36838996180.99200\n",
      "Epoch 227/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 31816235497.6996 - val_loss: 36823217733.6320\n",
      "\n",
      "Epoch 00227: val_loss improved from 36838996180.99200 to 36823217733.63200, saving model to best_model.h5\n",
      "Epoch 228/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 31806031262.1511 - val_loss: 36850191958.0160\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 36823217733.63200\n",
      "Epoch 229/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 31787117929.3582 - val_loss: 36816547053.5680\n",
      "\n",
      "Epoch 00229: val_loss improved from 36823217733.63200 to 36816547053.56800, saving model to best_model.h5\n",
      "Epoch 230/400\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 31766114152.9031 - val_loss: 36814727610.3680\n",
      "\n",
      "Epoch 00230: val_loss improved from 36816547053.56800 to 36814727610.36800, saving model to best_model.h5\n",
      "Epoch 231/400\n",
      "18000/18000 [==============================] - 1s 64us/step - loss: 31757652838.1724 - val_loss: 36765158473.7280\n",
      "\n",
      "Epoch 00231: val_loss improved from 36814727610.36800 to 36765158473.72800, saving model to best_model.h5\n",
      "Epoch 232/400\n",
      "18000/18000 [==============================] - 1s 59us/step - loss: 31740605198.7911 - val_loss: 36747704860.6720\n",
      "\n",
      "Epoch 00232: val_loss improved from 36765158473.72800 to 36747704860.67200, saving model to best_model.h5\n",
      "Epoch 233/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 31731440579.0151 - val_loss: 36745336356.8640\n",
      "\n",
      "Epoch 00233: val_loss improved from 36747704860.67200 to 36745336356.86400, saving model to best_model.h5\n",
      "Epoch 234/400\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 31713387362.5316 - val_loss: 36743014580.2240\n",
      "\n",
      "Epoch 00234: val_loss improved from 36745336356.86400 to 36743014580.22400, saving model to best_model.h5\n",
      "Epoch 235/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 31699588863.3173 - val_loss: 36732203204.6080\n",
      "\n",
      "Epoch 00235: val_loss improved from 36743014580.22400 to 36732203204.60800, saving model to best_model.h5\n",
      "Epoch 236/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 31678410482.5742 - val_loss: 36681068740.6080\n",
      "\n",
      "Epoch 00236: val_loss improved from 36732203204.60800 to 36681068740.60800, saving model to best_model.h5\n",
      "Epoch 237/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 31670218509.8809 - val_loss: 36676168122.3680\n",
      "\n",
      "Epoch 00237: val_loss improved from 36681068740.60800 to 36676168122.36800, saving model to best_model.h5\n",
      "Epoch 238/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 31655184072.7040 - val_loss: 36661178466.3040\n",
      "\n",
      "Epoch 00238: val_loss improved from 36676168122.36800 to 36661178466.30400, saving model to best_model.h5\n",
      "Epoch 239/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 31631932710.0018 - val_loss: 36656254484.4800\n",
      "\n",
      "Epoch 00239: val_loss improved from 36661178466.30400 to 36656254484.48000, saving model to best_model.h5\n",
      "Epoch 240/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 31626633656.5476 - val_loss: 36629744254.9760\n",
      "\n",
      "Epoch 00240: val_loss improved from 36656254484.48000 to 36629744254.97600, saving model to best_model.h5\n",
      "Epoch 241/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 31605153353.2729 - val_loss: 36621814005.7600\n",
      "\n",
      "Epoch 00241: val_loss improved from 36629744254.97600 to 36621814005.76000, saving model to best_model.h5\n",
      "Epoch 242/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 31605823591.7653 - val_loss: 36633182863.3600\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 36621814005.76000\n",
      "Epoch 243/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 31590333273.4293 - val_loss: 36614702694.4000\n",
      "\n",
      "Epoch 00243: val_loss improved from 36621814005.76000 to 36614702694.40000, saving model to best_model.h5\n",
      "Epoch 244/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 31571505446.9120 - val_loss: 36618101948.4160\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 36614702694.40000\n",
      "Epoch 245/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 31558577040.0427 - val_loss: 36592116924.4160\n",
      "\n",
      "Epoch 00245: val_loss improved from 36614702694.40000 to 36592116924.41600, saving model to best_model.h5\n",
      "Epoch 246/400\n",
      "18000/18000 [==============================] - 1s 39us/step - loss: 31544750859.1502 - val_loss: 36586302242.8160\n",
      "\n",
      "Epoch 00246: val_loss improved from 36592116924.41600 to 36586302242.81600, saving model to best_model.h5\n",
      "Epoch 247/400\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 31525130673.2658 - val_loss: 36547147399.1680\n",
      "\n",
      "Epoch 00247: val_loss improved from 36586302242.81600 to 36547147399.16800, saving model to best_model.h5\n",
      "Epoch 248/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 31512735372.6293 - val_loss: 36526568865.7920\n",
      "\n",
      "Epoch 00248: val_loss improved from 36547147399.16800 to 36526568865.79200, saving model to best_model.h5\n",
      "Epoch 249/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 31500005751.0116 - val_loss: 36523007213.5680\n",
      "\n",
      "Epoch 00249: val_loss improved from 36526568865.79200 to 36523007213.56800, saving model to best_model.h5\n",
      "Epoch 250/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 31489480582.0302 - val_loss: 36488400338.9440\n",
      "\n",
      "Epoch 00250: val_loss improved from 36523007213.56800 to 36488400338.94400, saving model to best_model.h5\n",
      "Epoch 251/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 31462804297.0453 - val_loss: 36496707354.6240\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 36488400338.94400\n",
      "Epoch 252/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 31456202369.7067 - val_loss: 36490625253.3760\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 36488400338.94400\n",
      "Epoch 253/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 31436065061.0916 - val_loss: 36462682800.1280\n",
      "\n",
      "Epoch 00253: val_loss improved from 36488400338.94400 to 36462682800.12800, saving model to best_model.h5\n",
      "Epoch 254/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 31425151979.0649 - val_loss: 36441182273.5360\n",
      "\n",
      "Epoch 00254: val_loss improved from 36462682800.12800 to 36441182273.53600, saving model to best_model.h5\n",
      "Epoch 255/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 31407648087.1538 - val_loss: 36433441849.3440\n",
      "\n",
      "Epoch 00255: val_loss improved from 36441182273.53600 to 36433441849.34400, saving model to best_model.h5\n",
      "Epoch 256/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 31400567377.4649 - val_loss: 36424686927.8720\n",
      "\n",
      "Epoch 00256: val_loss improved from 36433441849.34400 to 36424686927.87200, saving model to best_model.h5\n",
      "Epoch 257/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 31386028430.6773 - val_loss: 36445117612.0320\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 36424686927.87200\n",
      "Epoch 258/400\n",
      "18000/18000 [==============================] - 1s 66us/step - loss: 31366815514.6240 - val_loss: 36394383769.6000\n",
      "\n",
      "Epoch 00258: val_loss improved from 36424686927.87200 to 36394383769.60000, saving model to best_model.h5\n",
      "Epoch 259/400\n",
      "18000/18000 [==============================] - 1s 74us/step - loss: 31379215322.6809 - val_loss: 36365538295.8080\n",
      "\n",
      "Epoch 00259: val_loss improved from 36394383769.60000 to 36365538295.80800, saving model to best_model.h5\n",
      "Epoch 260/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 31340146032.6400 - val_loss: 36365515489.2800\n",
      "\n",
      "Epoch 00260: val_loss improved from 36365538295.80800 to 36365515489.28000, saving model to best_model.h5\n",
      "Epoch 261/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 31331032241.4933 - val_loss: 36358770458.6240\n",
      "\n",
      "Epoch 00261: val_loss improved from 36365515489.28000 to 36358770458.62400, saving model to best_model.h5\n",
      "Epoch 262/400\n",
      "18000/18000 [==============================] - 1s 59us/step - loss: 31317652830.4356 - val_loss: 36330879451.1360\n",
      "\n",
      "Epoch 00262: val_loss improved from 36358770458.62400 to 36330879451.13600, saving model to best_model.h5\n",
      "Epoch 263/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 31296119976.3911 - val_loss: 36335805988.8640\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 36330879451.13600\n",
      "Epoch 264/400\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 31283491154.6027 - val_loss: 36322100805.6320\n",
      "\n",
      "Epoch 00264: val_loss improved from 36330879451.13600 to 36322100805.63200, saving model to best_model.h5\n",
      "Epoch 265/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 31282344416.5973 - val_loss: 36300193202.1760\n",
      "\n",
      "Epoch 00265: val_loss improved from 36322100805.63200 to 36300193202.17600, saving model to best_model.h5\n",
      "Epoch 266/400\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 31266257429.3902 - val_loss: 36271672885.2480\n",
      "\n",
      "Epoch 00266: val_loss improved from 36300193202.17600 to 36271672885.24800, saving model to best_model.h5\n",
      "Epoch 267/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 31250019778.5600 - val_loss: 36256078069.7600\n",
      "\n",
      "Epoch 00267: val_loss improved from 36271672885.24800 to 36256078069.76000, saving model to best_model.h5\n",
      "Epoch 268/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 31232010440.2489 - val_loss: 36266413162.4960\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 36256078069.76000\n",
      "Epoch 269/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 31216689428.7076 - val_loss: 36263435632.6400\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 36256078069.76000\n",
      "Epoch 270/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 31206588874.7520 - val_loss: 36269499285.5040\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 36256078069.76000\n",
      "Epoch 271/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 31205506482.1760 - val_loss: 36213286240.2560\n",
      "\n",
      "Epoch 00271: val_loss improved from 36256078069.76000 to 36213286240.25600, saving model to best_model.h5\n",
      "Epoch 272/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 31182315215.9858 - val_loss: 36219400650.7520\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 36213286240.25600\n",
      "Epoch 273/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 31173912250.1404 - val_loss: 36206979973.1200\n",
      "\n",
      "Epoch 00273: val_loss improved from 36213286240.25600 to 36206979973.12000, saving model to best_model.h5\n",
      "Epoch 274/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 31166880006.1440 - val_loss: 36170306125.8240\n",
      "\n",
      "Epoch 00274: val_loss improved from 36206979973.12000 to 36170306125.82400, saving model to best_model.h5\n",
      "Epoch 275/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 31146362383.0187 - val_loss: 36195888988.1600\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 36170306125.82400\n",
      "Epoch 276/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 31142428375.2676 - val_loss: 36165664145.4080\n",
      "\n",
      "Epoch 00276: val_loss improved from 36170306125.82400 to 36165664145.40800, saving model to best_model.h5\n",
      "Epoch 277/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 31124615845.2053 - val_loss: 36169707782.1440\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 36165664145.40800\n",
      "Epoch 278/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 31100163336.8747 - val_loss: 36131608035.3280\n",
      "\n",
      "Epoch 00278: val_loss improved from 36165664145.40800 to 36131608035.32800, saving model to best_model.h5\n",
      "Epoch 279/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 31089171908.3804 - val_loss: 36140763807.7440\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 36131608035.32800\n",
      "Epoch 280/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 31079429992.9031 - val_loss: 36141871988.7360\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 36131608035.32800\n",
      "Epoch 281/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 31066960437.2480 - val_loss: 36105098133.5040\n",
      "\n",
      "Epoch 00281: val_loss improved from 36131608035.32800 to 36105098133.50400, saving model to best_model.h5\n",
      "Epoch 282/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 31057614793.3867 - val_loss: 36110745534.4640\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 36105098133.50400\n",
      "Epoch 283/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 31036260210.0053 - val_loss: 36085244133.3760\n",
      "\n",
      "Epoch 00283: val_loss improved from 36105098133.50400 to 36085244133.37600, saving model to best_model.h5\n",
      "Epoch 284/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 31028746780.6720 - val_loss: 36086799663.1040\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 36085244133.37600\n",
      "Epoch 285/400\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 31012319033.5716 - val_loss: 36074516742.1440\n",
      "\n",
      "Epoch 00285: val_loss improved from 36085244133.37600 to 36074516742.14400, saving model to best_model.h5\n",
      "Epoch 286/400\n",
      "18000/18000 [==============================] - 1s 39us/step - loss: 31011528225.2231 - val_loss: 36052159496.1920\n",
      "\n",
      "Epoch 00286: val_loss improved from 36074516742.14400 to 36052159496.19200, saving model to best_model.h5\n",
      "Epoch 287/400\n",
      "18000/18000 [==============================] - 1s 39us/step - loss: 31000685656.2916 - val_loss: 36031472205.8240\n",
      "\n",
      "Epoch 00287: val_loss improved from 36052159496.19200 to 36031472205.82400, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 288/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30988646835.0862 - val_loss: 36013876576.2560\n",
      "\n",
      "Epoch 00288: val_loss improved from 36031472205.82400 to 36013876576.25600, saving model to best_model.h5\n",
      "Epoch 289/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 30969067481.7707 - val_loss: 36004972625.9200\n",
      "\n",
      "Epoch 00289: val_loss improved from 36013876576.25600 to 36004972625.92000, saving model to best_model.h5\n",
      "Epoch 290/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 30956659987.7973 - val_loss: 36003181821.9520\n",
      "\n",
      "Epoch 00290: val_loss improved from 36004972625.92000 to 36003181821.95200, saving model to best_model.h5\n",
      "Epoch 291/400\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 30937490618.5956 - val_loss: 35994889388.0320\n",
      "\n",
      "Epoch 00291: val_loss improved from 36003181821.95200 to 35994889388.03200, saving model to best_model.h5\n",
      "Epoch 292/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 30939410779.7049 - val_loss: 36014300987.3920\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 35994889388.03200\n",
      "Epoch 293/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 30914949549.6249 - val_loss: 35962321829.8880\n",
      "\n",
      "Epoch 00293: val_loss improved from 35994889388.03200 to 35962321829.88800, saving model to best_model.h5\n",
      "Epoch 294/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 30901831532.5440 - val_loss: 35939181592.5760\n",
      "\n",
      "Epoch 00294: val_loss improved from 35962321829.88800 to 35939181592.57600, saving model to best_model.h5\n",
      "Epoch 295/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 30892168495.1040 - val_loss: 35954100535.2960\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 35939181592.57600\n",
      "Epoch 296/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 30879724493.9378 - val_loss: 35933691510.7840\n",
      "\n",
      "Epoch 00296: val_loss improved from 35939181592.57600 to 35933691510.78400, saving model to best_model.h5\n",
      "Epoch 297/400\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 30867505221.1769 - val_loss: 35907763404.8000\n",
      "\n",
      "Epoch 00297: val_loss improved from 35933691510.78400 to 35907763404.80000, saving model to best_model.h5\n",
      "Epoch 298/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 30850268208.2418 - val_loss: 35916411404.2880\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 35907763404.80000\n",
      "Epoch 299/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30840671953.8062 - val_loss: 35895010426.8800\n",
      "\n",
      "Epoch 00299: val_loss improved from 35907763404.80000 to 35895010426.88000, saving model to best_model.h5\n",
      "Epoch 300/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 30832298917.8880 - val_loss: 35905051951.1040\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 35895010426.88000\n",
      "Epoch 301/400\n",
      "18000/18000 [==============================] - 1s 62us/step - loss: 30819702614.6987 - val_loss: 35861322596.3520\n",
      "\n",
      "Epoch 00301: val_loss improved from 35895010426.88000 to 35861322596.35200, saving model to best_model.h5\n",
      "Epoch 302/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 30809013610.2684 - val_loss: 35848004370.4320\n",
      "\n",
      "Epoch 00302: val_loss improved from 35861322596.35200 to 35848004370.43200, saving model to best_model.h5\n",
      "Epoch 303/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 30805141001.5573 - val_loss: 35830478733.3120\n",
      "\n",
      "Epoch 00303: val_loss improved from 35848004370.43200 to 35830478733.31200, saving model to best_model.h5\n",
      "Epoch 304/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 30785445412.8640 - val_loss: 35806505500.6720\n",
      "\n",
      "Epoch 00304: val_loss improved from 35830478733.31200 to 35806505500.67200, saving model to best_model.h5\n",
      "Epoch 305/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 30772118749.1840 - val_loss: 35830345433.0880\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 35806505500.67200\n",
      "Epoch 306/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 30762816565.7031 - val_loss: 35802365263.8720\n",
      "\n",
      "Epoch 00306: val_loss improved from 35806505500.67200 to 35802365263.87200, saving model to best_model.h5\n",
      "Epoch 307/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30748818644.0818 - val_loss: 35788917702.6560\n",
      "\n",
      "Epoch 00307: val_loss improved from 35802365263.87200 to 35788917702.65600, saving model to best_model.h5\n",
      "Epoch 308/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 30740448156.7858 - val_loss: 35777588166.6560\n",
      "\n",
      "Epoch 00308: val_loss improved from 35788917702.65600 to 35777588166.65600, saving model to best_model.h5\n",
      "Epoch 309/400\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 30721635410.8302 - val_loss: 35760634331.1360\n",
      "\n",
      "Epoch 00309: val_loss improved from 35777588166.65600 to 35760634331.13600, saving model to best_model.h5\n",
      "Epoch 310/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 30710855046.4853 - val_loss: 35738324500.4800\n",
      "\n",
      "Epoch 00310: val_loss improved from 35760634331.13600 to 35738324500.48000, saving model to best_model.h5\n",
      "Epoch 311/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 30701429398.6418 - val_loss: 35733868183.5520\n",
      "\n",
      "Epoch 00311: val_loss improved from 35738324500.48000 to 35733868183.55200, saving model to best_model.h5\n",
      "Epoch 312/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 30689093932.3733 - val_loss: 35723935907.8400\n",
      "\n",
      "Epoch 00312: val_loss improved from 35733868183.55200 to 35723935907.84000, saving model to best_model.h5\n",
      "Epoch 313/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30679519679.8293 - val_loss: 35707708276.7360\n",
      "\n",
      "Epoch 00313: val_loss improved from 35723935907.84000 to 35707708276.73600, saving model to best_model.h5\n",
      "Epoch 314/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 30667599049.1591 - val_loss: 35704586960.8960\n",
      "\n",
      "Epoch 00314: val_loss improved from 35707708276.73600 to 35704586960.89600, saving model to best_model.h5\n",
      "Epoch 315/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 30654091562.5529 - val_loss: 35702940434.4320\n",
      "\n",
      "Epoch 00315: val_loss improved from 35704586960.89600 to 35702940434.43200, saving model to best_model.h5\n",
      "Epoch 316/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30635524210.6880 - val_loss: 35690823319.5520\n",
      "\n",
      "Epoch 00316: val_loss improved from 35702940434.43200 to 35690823319.55200, saving model to best_model.h5\n",
      "Epoch 317/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30636343388.8427 - val_loss: 35694366523.3920\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 35690823319.55200\n",
      "Epoch 318/400\n",
      "18000/18000 [==============================] - 1s 39us/step - loss: 30618975629.7671 - val_loss: 35675014496.2560\n",
      "\n",
      "Epoch 00318: val_loss improved from 35690823319.55200 to 35675014496.25600, saving model to best_model.h5\n",
      "Epoch 319/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30608550168.3484 - val_loss: 35657420210.1760\n",
      "\n",
      "Epoch 00319: val_loss improved from 35675014496.25600 to 35657420210.17600, saving model to best_model.h5\n",
      "Epoch 320/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 30592677988.1244 - val_loss: 35644005908.4800\n",
      "\n",
      "Epoch 00320: val_loss improved from 35657420210.17600 to 35644005908.48000, saving model to best_model.h5\n",
      "Epoch 321/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 30594313173.2196 - val_loss: 35636143489.0240\n",
      "\n",
      "Epoch 00321: val_loss improved from 35644005908.48000 to 35636143489.02400, saving model to best_model.h5\n",
      "Epoch 322/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30572102301.9236 - val_loss: 35627690917.8880\n",
      "\n",
      "Epoch 00322: val_loss improved from 35636143489.02400 to 35627690917.88800, saving model to best_model.h5\n",
      "Epoch 323/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 30564800866.9867 - val_loss: 35587345711.1040\n",
      "\n",
      "Epoch 00323: val_loss improved from 35627690917.88800 to 35587345711.10400, saving model to best_model.h5\n",
      "Epoch 324/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 47us/step - loss: 30554882730.6667 - val_loss: 35575017570.3040\n",
      "\n",
      "Epoch 00324: val_loss improved from 35587345711.10400 to 35575017570.30400, saving model to best_model.h5\n",
      "Epoch 325/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30537830861.4827 - val_loss: 35585016168.4480\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 35575017570.30400\n",
      "Epoch 326/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 30530092886.6987 - val_loss: 35569878368.2560\n",
      "\n",
      "Epoch 00326: val_loss improved from 35575017570.30400 to 35569878368.25600, saving model to best_model.h5\n",
      "Epoch 327/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30516127121.4080 - val_loss: 35591999946.7520\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 35569878368.25600\n",
      "Epoch 328/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30514390080.6258 - val_loss: 35568368189.4400\n",
      "\n",
      "Epoch 00328: val_loss improved from 35569878368.25600 to 35568368189.44000, saving model to best_model.h5\n",
      "Epoch 329/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30495993373.5822 - val_loss: 35548152397.8240\n",
      "\n",
      "Epoch 00329: val_loss improved from 35568368189.44000 to 35548152397.82400, saving model to best_model.h5\n",
      "Epoch 330/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30476560902.8267 - val_loss: 35516302622.7200\n",
      "\n",
      "Epoch 00330: val_loss improved from 35548152397.82400 to 35516302622.72000, saving model to best_model.h5\n",
      "Epoch 331/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30481813835.3209 - val_loss: 35518094409.7280\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 35516302622.72000\n",
      "Epoch 332/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30461238891.8613 - val_loss: 35499839619.0720\n",
      "\n",
      "Epoch 00332: val_loss improved from 35516302622.72000 to 35499839619.07200, saving model to best_model.h5\n",
      "Epoch 333/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30456196093.2693 - val_loss: 35514405355.5200\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 35499839619.07200\n",
      "Epoch 334/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30442760532.4231 - val_loss: 35488580304.8960\n",
      "\n",
      "Epoch 00334: val_loss improved from 35499839619.07200 to 35488580304.89600, saving model to best_model.h5\n",
      "Epoch 335/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30434869300.7929 - val_loss: 35477035843.5840\n",
      "\n",
      "Epoch 00335: val_loss improved from 35488580304.89600 to 35477035843.58400, saving model to best_model.h5\n",
      "Epoch 336/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30420854454.4996 - val_loss: 35459008954.3680\n",
      "\n",
      "Epoch 00336: val_loss improved from 35477035843.58400 to 35459008954.36800, saving model to best_model.h5\n",
      "Epoch 337/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30399696222.4356 - val_loss: 35463389085.6960\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 35459008954.36800\n",
      "Epoch 338/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30391954442.0124 - val_loss: 35453361586.1760\n",
      "\n",
      "Epoch 00338: val_loss improved from 35459008954.36800 to 35453361586.17600, saving model to best_model.h5\n",
      "Epoch 339/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30379755849.5004 - val_loss: 35449332629.5040\n",
      "\n",
      "Epoch 00339: val_loss improved from 35453361586.17600 to 35449332629.50400, saving model to best_model.h5\n",
      "Epoch 340/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 30376568284.0462 - val_loss: 35441894653.9520\n",
      "\n",
      "Epoch 00340: val_loss improved from 35449332629.50400 to 35441894653.95200, saving model to best_model.h5\n",
      "Epoch 341/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30361936330.7520 - val_loss: 35407701311.4880\n",
      "\n",
      "Epoch 00341: val_loss improved from 35441894653.95200 to 35407701311.48800, saving model to best_model.h5\n",
      "Epoch 342/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30345900430.6773 - val_loss: 35428090478.5920\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 35407701311.48800\n",
      "Epoch 343/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 30344359695.7013 - val_loss: 35402642227.2000\n",
      "\n",
      "Epoch 00343: val_loss improved from 35407701311.48800 to 35402642227.20000, saving model to best_model.h5\n",
      "Epoch 344/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 30327811524.3804 - val_loss: 35374384185.3440\n",
      "\n",
      "Epoch 00344: val_loss improved from 35402642227.20000 to 35374384185.34400, saving model to best_model.h5\n",
      "Epoch 345/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 30316676794.1404 - val_loss: 35368942305.2800\n",
      "\n",
      "Epoch 00345: val_loss improved from 35374384185.34400 to 35368942305.28000, saving model to best_model.h5\n",
      "Epoch 346/400\n",
      "18000/18000 [==============================] - 1s 61us/step - loss: 30312112291.8400 - val_loss: 35348974665.7280\n",
      "\n",
      "Epoch 00346: val_loss improved from 35368942305.28000 to 35348974665.72800, saving model to best_model.h5\n",
      "Epoch 347/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 30298328842.2400 - val_loss: 35351982080.0000\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 35348974665.72800\n",
      "Epoch 348/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 30296980094.9760 - val_loss: 35341099106.3040\n",
      "\n",
      "Epoch 00348: val_loss improved from 35348974665.72800 to 35341099106.30400, saving model to best_model.h5\n",
      "Epoch 349/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 30282037792.3129 - val_loss: 35364069900.2880\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 35341099106.30400\n",
      "Epoch 350/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 30267946607.5022 - val_loss: 35333255462.9120\n",
      "\n",
      "Epoch 00350: val_loss improved from 35341099106.30400 to 35333255462.91200, saving model to best_model.h5\n",
      "Epoch 351/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30256030629.8880 - val_loss: 35307365564.4160\n",
      "\n",
      "Epoch 00351: val_loss improved from 35333255462.91200 to 35307365564.41600, saving model to best_model.h5\n",
      "Epoch 352/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 30253091873.6782 - val_loss: 35293578330.1120\n",
      "\n",
      "Epoch 00352: val_loss improved from 35307365564.41600 to 35293578330.11200, saving model to best_model.h5\n",
      "Epoch 353/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 30231672768.2844 - val_loss: 35283245531.1360\n",
      "\n",
      "Epoch 00353: val_loss improved from 35293578330.11200 to 35283245531.13600, saving model to best_model.h5\n",
      "Epoch 354/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30219834137.7138 - val_loss: 35283831390.2080\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 35283245531.13600\n",
      "Epoch 355/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 30210993047.3244 - val_loss: 35273904947.2000\n",
      "\n",
      "Epoch 00355: val_loss improved from 35283245531.13600 to 35273904947.20000, saving model to best_model.h5\n",
      "Epoch 356/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 30208400787.2284 - val_loss: 35264711294.9760\n",
      "\n",
      "Epoch 00356: val_loss improved from 35273904947.20000 to 35264711294.97600, saving model to best_model.h5\n",
      "Epoch 357/400\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 30192333793.9627 - val_loss: 35246008893.4400\n",
      "\n",
      "Epoch 00357: val_loss improved from 35264711294.97600 to 35246008893.44000, saving model to best_model.h5\n",
      "Epoch 358/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30191091189.5324 - val_loss: 35216056975.3600\n",
      "\n",
      "Epoch 00358: val_loss improved from 35246008893.44000 to 35216056975.36000, saving model to best_model.h5\n",
      "Epoch 359/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 30176771710.9760 - val_loss: 35206169985.0240\n",
      "\n",
      "Epoch 00359: val_loss improved from 35216056975.36000 to 35206169985.02400, saving model to best_model.h5\n",
      "Epoch 360/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30162339958.3289 - val_loss: 35199309119.4880\n",
      "\n",
      "Epoch 00360: val_loss improved from 35206169985.02400 to 35199309119.48800, saving model to best_model.h5\n",
      "Epoch 361/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 43us/step - loss: 30152967640.4053 - val_loss: 35193901350.9120\n",
      "\n",
      "Epoch 00361: val_loss improved from 35199309119.48800 to 35193901350.91200, saving model to best_model.h5\n",
      "Epoch 362/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30139642995.5982 - val_loss: 35197942824.9600\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 35193901350.91200\n",
      "Epoch 363/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30128694460.4160 - val_loss: 35189572599.8080\n",
      "\n",
      "Epoch 00363: val_loss improved from 35193901350.91200 to 35189572599.80800, saving model to best_model.h5\n",
      "Epoch 364/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30123290600.3342 - val_loss: 35177944580.0960\n",
      "\n",
      "Epoch 00364: val_loss improved from 35189572599.80800 to 35177944580.09600, saving model to best_model.h5\n",
      "Epoch 365/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30118028900.5796 - val_loss: 35179561418.7520\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 35177944580.09600\n",
      "Epoch 366/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30099817840.6400 - val_loss: 35147152588.8000\n",
      "\n",
      "Epoch 00366: val_loss improved from 35177944580.09600 to 35147152588.80000, saving model to best_model.h5\n",
      "Epoch 367/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30101348291.9253 - val_loss: 35153502502.9120\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 35147152588.80000\n",
      "Epoch 368/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 30086162189.8809 - val_loss: 35156997111.8080\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 35147152588.80000\n",
      "Epoch 369/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30085005824.4551 - val_loss: 35126371090.4320\n",
      "\n",
      "Epoch 00369: val_loss improved from 35147152588.80000 to 35126371090.43200, saving model to best_model.h5\n",
      "Epoch 370/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30063486551.8364 - val_loss: 35130953367.5520\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 35126371090.43200\n",
      "Epoch 371/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30047530849.6213 - val_loss: 35087920594.9440\n",
      "\n",
      "Epoch 00371: val_loss improved from 35126371090.43200 to 35087920594.94400, saving model to best_model.h5\n",
      "Epoch 372/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30048522400.1991 - val_loss: 35088450650.1120\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 35087920594.94400\n",
      "Epoch 373/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 30051494130.1191 - val_loss: 35059838222.3360\n",
      "\n",
      "Epoch 00373: val_loss improved from 35087920594.94400 to 35059838222.33600, saving model to best_model.h5\n",
      "Epoch 374/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30025816741.2053 - val_loss: 35052700401.6640\n",
      "\n",
      "Epoch 00374: val_loss improved from 35059838222.33600 to 35052700401.66400, saving model to best_model.h5\n",
      "Epoch 375/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30011144670.7769 - val_loss: 35059193348.0960\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 35052700401.66400\n",
      "Epoch 376/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 30001709564.8142 - val_loss: 35045699944.4480\n",
      "\n",
      "Epoch 00376: val_loss improved from 35052700401.66400 to 35045699944.44800, saving model to best_model.h5\n",
      "Epoch 377/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 29994223519.5164 - val_loss: 35032854003.7120\n",
      "\n",
      "Epoch 00377: val_loss improved from 35045699944.44800 to 35032854003.71200, saving model to best_model.h5\n",
      "Epoch 378/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 29985945174.0160 - val_loss: 35029241495.5520\n",
      "\n",
      "Epoch 00378: val_loss improved from 35032854003.71200 to 35029241495.55200, saving model to best_model.h5\n",
      "Epoch 379/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 29985638463.7156 - val_loss: 35025131110.4000\n",
      "\n",
      "Epoch 00379: val_loss improved from 35029241495.55200 to 35025131110.40000, saving model to best_model.h5\n",
      "Epoch 380/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 29968314204.1600 - val_loss: 35003656110.0800\n",
      "\n",
      "Epoch 00380: val_loss improved from 35025131110.40000 to 35003656110.08000, saving model to best_model.h5\n",
      "Epoch 381/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 29969023465.6996 - val_loss: 34983852834.8160\n",
      "\n",
      "Epoch 00381: val_loss improved from 35003656110.08000 to 34983852834.81600, saving model to best_model.h5\n",
      "Epoch 382/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 29949909381.5751 - val_loss: 34972298936.3200\n",
      "\n",
      "Epoch 00382: val_loss improved from 34983852834.81600 to 34972298936.32000, saving model to best_model.h5\n",
      "Epoch 383/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 29938800787.4560 - val_loss: 34982526484.4800\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 34972298936.32000\n",
      "Epoch 384/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 29930644444.5013 - val_loss: 34969992364.0320\n",
      "\n",
      "Epoch 00384: val_loss improved from 34972298936.32000 to 34969992364.03200, saving model to best_model.h5\n",
      "Epoch 385/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 29926263983.6729 - val_loss: 34951936507.9040\n",
      "\n",
      "Epoch 00385: val_loss improved from 34969992364.03200 to 34951936507.90400, saving model to best_model.h5\n",
      "Epoch 386/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 29913018054.8836 - val_loss: 34954905485.3120\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 34951936507.90400\n",
      "Epoch 387/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 29901476444.3876 - val_loss: 34941263675.3920\n",
      "\n",
      "Epoch 00387: val_loss improved from 34951936507.90400 to 34941263675.39200, saving model to best_model.h5\n",
      "Epoch 388/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 29894713626.1689 - val_loss: 34931351224.3200\n",
      "\n",
      "Epoch 00388: val_loss improved from 34941263675.39200 to 34931351224.32000, saving model to best_model.h5\n",
      "Epoch 389/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 29892421113.1733 - val_loss: 34915141091.3280\n",
      "\n",
      "Epoch 00389: val_loss improved from 34931351224.32000 to 34915141091.32800, saving model to best_model.h5\n",
      "Epoch 390/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 29875464767.2604 - val_loss: 34929495441.4080\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 34915141091.32800\n",
      "Epoch 391/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 29880819313.3227 - val_loss: 34900234731.5200\n",
      "\n",
      "Epoch 00391: val_loss improved from 34915141091.32800 to 34900234731.52000, saving model to best_model.h5\n",
      "Epoch 392/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 29861296963.5840 - val_loss: 34897185505.2800\n",
      "\n",
      "Epoch 00392: val_loss improved from 34900234731.52000 to 34897185505.28000, saving model to best_model.h5\n",
      "Epoch 393/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 29851920405.8453 - val_loss: 34903928111.1040\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 34897185505.28000\n",
      "Epoch 394/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 29837986234.3680 - val_loss: 34894097350.6560\n",
      "\n",
      "Epoch 00394: val_loss improved from 34897185505.28000 to 34894097350.65600, saving model to best_model.h5\n",
      "Epoch 395/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 29828995596.2880 - val_loss: 34874919124.9920\n",
      "\n",
      "Epoch 00395: val_loss improved from 34894097350.65600 to 34874919124.99200, saving model to best_model.h5\n",
      "Epoch 396/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 29829302089.9556 - val_loss: 34860350537.7280\n",
      "\n",
      "Epoch 00396: val_loss improved from 34874919124.99200 to 34860350537.72800, saving model to best_model.h5\n",
      "Epoch 397/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 29809017804.1173 - val_loss: 34849582579.7120\n",
      "\n",
      "Epoch 00397: val_loss improved from 34860350537.72800 to 34849582579.71200, saving model to best_model.h5\n",
      "Epoch 398/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 48us/step - loss: 29800239515.4204 - val_loss: 34851138633.7280\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 34849582579.71200\n",
      "Epoch 399/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 29814908483.8116 - val_loss: 34851638149.1200\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 34849582579.71200\n",
      "Epoch 400/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 29797488394.2400 - val_loss: 34813784293.3760\n",
      "\n",
      "Epoch 00400: val_loss improved from 34849582579.71200 to 34813784293.37600, saving model to best_model.h5\n",
      "Model score: 0.6566940217993732\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set callback functions to early stop training and save the \n",
    "# best model so far\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=5),\n",
    "            ModelCheckpoint(filepath='best_model.h5',\n",
    "                            monitor='val_loss',\n",
    "                            save_best_only=True,\n",
    "                           verbose=1)]\n",
    "\n",
    "model = nn_model(layers=[20,20,20])\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "history = model.fit(selected_feature_train, price_train,\n",
    "            epochs=400, callbacks=callbacks, batch_size=128,\n",
    "            validation_data=(selected_feature_val, price_val))\n",
    "\n",
    "model_score = score(model.predict(selected_feature_val), price_val)\n",
    "print(f\"Model score: {model_score}\")\n",
    "plot_loss(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You don't have to remember these resources because they're here when you need them\n",
    "https://www.tensorflow.org/api_docs\n",
    "\n",
    "https://keras.io/\n",
    "\n",
    "https://www.tensorflow.org/tutorials/\n",
    "\n",
    "https://www.google.com\n",
    "\n",
    "### Don't trust me, trust your validation errors\n",
    "### Don't look at your test set until you're actually going to test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
