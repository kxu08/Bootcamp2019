{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow & Keras - Basics of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most importantly... resources\n",
    "\n",
    "https://www.tensorflow.org/api_docs\n",
    "\n",
    "https://keras.io/\n",
    "\n",
    "https://www.tensorflow.org/tutorials/\n",
    "\n",
    "https://www.google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF overview\n",
    "\n",
    "* #### \"End-to-end machine learning platform\" \n",
    "\n",
    "    - Not the only one! Check out PyTorch, Theano, Cognitive Toolkit.\n",
    "   \n",
    "* #### Integrates with high-level APIs like Keras\n",
    "* #### Plays nice with Pandas\n",
    "* #### Makes deep learning *fast* and *easy* *\n",
    "    *<sup>\"easy\"</sup>\n",
    "\n",
    "## Tasks for TensorFlow:\n",
    "\n",
    "* #### Regression\n",
    "    - Predict house prices\n",
    "    - Predict drug metabolic rates\n",
    "    - Predict stock trends *\n",
    "    \n",
    "    *<sup>this is super hard</sup>\n",
    "    \n",
    "    \n",
    "\n",
    "* #### Classification\n",
    "    - Cat or dog?\n",
    "    - Malignant or benign cancer from images\n",
    "    ![](media/dr.png)\n",
    "    <span style=\"font-size:0.75em;\">Google AI Blog: Diabetic Retinopathy</span>\n",
    "\n",
    "\n",
    "\n",
    "* #### Dimensionality reduction\n",
    "    - Visualize high-dimensional data in 2 or 3-D space\n",
    "    - Compress representations for successive ML\n",
    "\n",
    "\n",
    "\n",
    "* #### Generative models\n",
    "    - Create new molecules with desirable properties\n",
    "    - Artificially enhance image resolution\n",
    "    ![](media/molecular_gan.png)\n",
    "    <span style=\"font-size:0.75em;\">Kadurin et al., 2017</span>\n",
    "\n",
    "\n",
    "* #### Reinforcement learning\n",
    "    - Can't beat your friends at chess? Make your computer do it\n",
    "\n",
    "\n",
    "\n",
    "* #### Much more...\n",
    "    - Generic math\n",
    "    - Probabilistic programming with TFP\n",
    "    - Automatic differentiation\n",
    "    - ...\n",
    "\n",
    "\n",
    "## Let's Regress\n",
    "\n",
    "### Imports!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name a more iconic duo, I'll wait\n",
    "\n",
    "#### New imports -- TF and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check our versions for good measure -- these programs may have very different behavior version-to-version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading in housing data as with SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7237550310</td>\n",
       "      <td>20140512T000000</td>\n",
       "      <td>1225000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.50</td>\n",
       "      <td>5420</td>\n",
       "      <td>101930</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>3890</td>\n",
       "      <td>1530</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>98053</td>\n",
       "      <td>47.6561</td>\n",
       "      <td>-122.005</td>\n",
       "      <td>4760</td>\n",
       "      <td>101930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1321400060</td>\n",
       "      <td>20140627T000000</td>\n",
       "      <td>257500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1715</td>\n",
       "      <td>6819</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1715</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>98003</td>\n",
       "      <td>47.3097</td>\n",
       "      <td>-122.327</td>\n",
       "      <td>2238</td>\n",
       "      <td>6819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2008000270</td>\n",
       "      <td>20150115T000000</td>\n",
       "      <td>291850.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1060</td>\n",
       "      <td>9711</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1060</td>\n",
       "      <td>0</td>\n",
       "      <td>1963</td>\n",
       "      <td>0</td>\n",
       "      <td>98198</td>\n",
       "      <td>47.4095</td>\n",
       "      <td>-122.315</td>\n",
       "      <td>1650</td>\n",
       "      <td>9711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2414600126</td>\n",
       "      <td>20150415T000000</td>\n",
       "      <td>229500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1780</td>\n",
       "      <td>7470</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>730</td>\n",
       "      <td>1960</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5123</td>\n",
       "      <td>-122.337</td>\n",
       "      <td>1780</td>\n",
       "      <td>8113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3793500160</td>\n",
       "      <td>20150312T000000</td>\n",
       "      <td>323000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1890</td>\n",
       "      <td>6560</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1890</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98038</td>\n",
       "      <td>47.3684</td>\n",
       "      <td>-122.031</td>\n",
       "      <td>2390</td>\n",
       "      <td>7570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1736800520</td>\n",
       "      <td>20150403T000000</td>\n",
       "      <td>662500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3560</td>\n",
       "      <td>9796</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1860</td>\n",
       "      <td>1700</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98007</td>\n",
       "      <td>47.6007</td>\n",
       "      <td>-122.145</td>\n",
       "      <td>2210</td>\n",
       "      <td>8925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9212900260</td>\n",
       "      <td>20140527T000000</td>\n",
       "      <td>468000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1160</td>\n",
       "      <td>6000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>860</td>\n",
       "      <td>300</td>\n",
       "      <td>1942</td>\n",
       "      <td>0</td>\n",
       "      <td>98115</td>\n",
       "      <td>47.6900</td>\n",
       "      <td>-122.292</td>\n",
       "      <td>1330</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>114101516</td>\n",
       "      <td>20140528T000000</td>\n",
       "      <td>310000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1430</td>\n",
       "      <td>19901</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1430</td>\n",
       "      <td>0</td>\n",
       "      <td>1927</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7558</td>\n",
       "      <td>-122.229</td>\n",
       "      <td>1780</td>\n",
       "      <td>12697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6054650070</td>\n",
       "      <td>20141007T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1370</td>\n",
       "      <td>9680</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1370</td>\n",
       "      <td>0</td>\n",
       "      <td>1977</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6127</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1370</td>\n",
       "      <td>10208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1175000570</td>\n",
       "      <td>20150312T000000</td>\n",
       "      <td>530000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1810</td>\n",
       "      <td>4850</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1810</td>\n",
       "      <td>0</td>\n",
       "      <td>1900</td>\n",
       "      <td>0</td>\n",
       "      <td>98107</td>\n",
       "      <td>47.6700</td>\n",
       "      <td>-122.394</td>\n",
       "      <td>1360</td>\n",
       "      <td>4850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9297300055</td>\n",
       "      <td>20150124T000000</td>\n",
       "      <td>650000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2950</td>\n",
       "      <td>5000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1980</td>\n",
       "      <td>970</td>\n",
       "      <td>1979</td>\n",
       "      <td>0</td>\n",
       "      <td>98126</td>\n",
       "      <td>47.5714</td>\n",
       "      <td>-122.375</td>\n",
       "      <td>2140</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1875500060</td>\n",
       "      <td>20140731T000000</td>\n",
       "      <td>395000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1890</td>\n",
       "      <td>14040</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1890</td>\n",
       "      <td>0</td>\n",
       "      <td>1994</td>\n",
       "      <td>0</td>\n",
       "      <td>98019</td>\n",
       "      <td>47.7277</td>\n",
       "      <td>-121.962</td>\n",
       "      <td>1890</td>\n",
       "      <td>14018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6865200140</td>\n",
       "      <td>20140529T000000</td>\n",
       "      <td>485000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1600</td>\n",
       "      <td>4300</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>1916</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6648</td>\n",
       "      <td>-122.343</td>\n",
       "      <td>1610</td>\n",
       "      <td>4300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>16000397</td>\n",
       "      <td>20141205T000000</td>\n",
       "      <td>189000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1200</td>\n",
       "      <td>9850</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1200</td>\n",
       "      <td>0</td>\n",
       "      <td>1921</td>\n",
       "      <td>0</td>\n",
       "      <td>98002</td>\n",
       "      <td>47.3089</td>\n",
       "      <td>-122.210</td>\n",
       "      <td>1060</td>\n",
       "      <td>5095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7983200060</td>\n",
       "      <td>20150424T000000</td>\n",
       "      <td>230000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1250</td>\n",
       "      <td>9774</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1250</td>\n",
       "      <td>0</td>\n",
       "      <td>1969</td>\n",
       "      <td>0</td>\n",
       "      <td>98003</td>\n",
       "      <td>47.3343</td>\n",
       "      <td>-122.306</td>\n",
       "      <td>1280</td>\n",
       "      <td>8850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6300500875</td>\n",
       "      <td>20140514T000000</td>\n",
       "      <td>385000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1620</td>\n",
       "      <td>4980</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>860</td>\n",
       "      <td>760</td>\n",
       "      <td>1947</td>\n",
       "      <td>0</td>\n",
       "      <td>98133</td>\n",
       "      <td>47.7025</td>\n",
       "      <td>-122.341</td>\n",
       "      <td>1400</td>\n",
       "      <td>4980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2524049179</td>\n",
       "      <td>20140826T000000</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3050</td>\n",
       "      <td>44867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2330</td>\n",
       "      <td>720</td>\n",
       "      <td>1968</td>\n",
       "      <td>0</td>\n",
       "      <td>98040</td>\n",
       "      <td>47.5316</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>4110</td>\n",
       "      <td>20336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7137970340</td>\n",
       "      <td>20140703T000000</td>\n",
       "      <td>285000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2270</td>\n",
       "      <td>6300</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2270</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>98092</td>\n",
       "      <td>47.3266</td>\n",
       "      <td>-122.169</td>\n",
       "      <td>2240</td>\n",
       "      <td>7005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8091400200</td>\n",
       "      <td>20140516T000000</td>\n",
       "      <td>252700.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1070</td>\n",
       "      <td>9643</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1070</td>\n",
       "      <td>0</td>\n",
       "      <td>1985</td>\n",
       "      <td>0</td>\n",
       "      <td>98030</td>\n",
       "      <td>47.3533</td>\n",
       "      <td>-122.166</td>\n",
       "      <td>1220</td>\n",
       "      <td>8386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3814700200</td>\n",
       "      <td>20141120T000000</td>\n",
       "      <td>329000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2450</td>\n",
       "      <td>6500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2450</td>\n",
       "      <td>0</td>\n",
       "      <td>1985</td>\n",
       "      <td>0</td>\n",
       "      <td>98030</td>\n",
       "      <td>47.3739</td>\n",
       "      <td>-122.172</td>\n",
       "      <td>2200</td>\n",
       "      <td>6865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1202000200</td>\n",
       "      <td>20141103T000000</td>\n",
       "      <td>233000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1710</td>\n",
       "      <td>4697</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1710</td>\n",
       "      <td>0</td>\n",
       "      <td>1941</td>\n",
       "      <td>0</td>\n",
       "      <td>98002</td>\n",
       "      <td>47.3048</td>\n",
       "      <td>-122.218</td>\n",
       "      <td>1030</td>\n",
       "      <td>4705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1794500383</td>\n",
       "      <td>20140626T000000</td>\n",
       "      <td>937000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2450</td>\n",
       "      <td>2691</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1750</td>\n",
       "      <td>700</td>\n",
       "      <td>1915</td>\n",
       "      <td>0</td>\n",
       "      <td>98119</td>\n",
       "      <td>47.6386</td>\n",
       "      <td>-122.360</td>\n",
       "      <td>1760</td>\n",
       "      <td>3573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3303700376</td>\n",
       "      <td>20141201T000000</td>\n",
       "      <td>667000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1400</td>\n",
       "      <td>1581</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1400</td>\n",
       "      <td>0</td>\n",
       "      <td>1909</td>\n",
       "      <td>0</td>\n",
       "      <td>98112</td>\n",
       "      <td>47.6221</td>\n",
       "      <td>-122.314</td>\n",
       "      <td>1860</td>\n",
       "      <td>3861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5101402488</td>\n",
       "      <td>20140624T000000</td>\n",
       "      <td>438000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1520</td>\n",
       "      <td>6380</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>790</td>\n",
       "      <td>730</td>\n",
       "      <td>1948</td>\n",
       "      <td>0</td>\n",
       "      <td>98115</td>\n",
       "      <td>47.6950</td>\n",
       "      <td>-122.304</td>\n",
       "      <td>1520</td>\n",
       "      <td>6235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1873100390</td>\n",
       "      <td>20150302T000000</td>\n",
       "      <td>719000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2570</td>\n",
       "      <td>7173</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2570</td>\n",
       "      <td>0</td>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>98052</td>\n",
       "      <td>47.7073</td>\n",
       "      <td>-122.110</td>\n",
       "      <td>2630</td>\n",
       "      <td>6026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21583</th>\n",
       "      <td>2025049203</td>\n",
       "      <td>20140610T000000</td>\n",
       "      <td>399950.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>710</td>\n",
       "      <td>1157</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>710</td>\n",
       "      <td>0</td>\n",
       "      <td>1943</td>\n",
       "      <td>0</td>\n",
       "      <td>98102</td>\n",
       "      <td>47.6413</td>\n",
       "      <td>-122.329</td>\n",
       "      <td>1370</td>\n",
       "      <td>1173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21584</th>\n",
       "      <td>952006823</td>\n",
       "      <td>20141202T000000</td>\n",
       "      <td>380000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1260</td>\n",
       "      <td>900</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>940</td>\n",
       "      <td>320</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98116</td>\n",
       "      <td>47.5621</td>\n",
       "      <td>-122.384</td>\n",
       "      <td>1310</td>\n",
       "      <td>1415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21585</th>\n",
       "      <td>3832050760</td>\n",
       "      <td>20140828T000000</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1870</td>\n",
       "      <td>5000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1870</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98042</td>\n",
       "      <td>47.3339</td>\n",
       "      <td>-122.055</td>\n",
       "      <td>2170</td>\n",
       "      <td>5399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21586</th>\n",
       "      <td>2767604724</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>505000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1430</td>\n",
       "      <td>1201</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1430</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98107</td>\n",
       "      <td>47.6707</td>\n",
       "      <td>-122.381</td>\n",
       "      <td>1430</td>\n",
       "      <td>1249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21587</th>\n",
       "      <td>6632300207</td>\n",
       "      <td>20150305T000000</td>\n",
       "      <td>385000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1520</td>\n",
       "      <td>1488</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1520</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7337</td>\n",
       "      <td>-122.309</td>\n",
       "      <td>1520</td>\n",
       "      <td>1497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21588</th>\n",
       "      <td>2767600688</td>\n",
       "      <td>20141113T000000</td>\n",
       "      <td>414500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1210</td>\n",
       "      <td>1278</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1020</td>\n",
       "      <td>190</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98117</td>\n",
       "      <td>47.6756</td>\n",
       "      <td>-122.375</td>\n",
       "      <td>1210</td>\n",
       "      <td>1118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21589</th>\n",
       "      <td>7570050450</td>\n",
       "      <td>20140910T000000</td>\n",
       "      <td>347500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2540</td>\n",
       "      <td>4760</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2540</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>98038</td>\n",
       "      <td>47.3452</td>\n",
       "      <td>-122.022</td>\n",
       "      <td>2540</td>\n",
       "      <td>4571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21590</th>\n",
       "      <td>7430200100</td>\n",
       "      <td>20140514T000000</td>\n",
       "      <td>1222500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4910</td>\n",
       "      <td>9444</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>3110</td>\n",
       "      <td>1800</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6502</td>\n",
       "      <td>-122.066</td>\n",
       "      <td>4560</td>\n",
       "      <td>11063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21591</th>\n",
       "      <td>4140940150</td>\n",
       "      <td>20141002T000000</td>\n",
       "      <td>572000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2770</td>\n",
       "      <td>3852</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2770</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5001</td>\n",
       "      <td>-122.232</td>\n",
       "      <td>1810</td>\n",
       "      <td>5641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21592</th>\n",
       "      <td>1931300412</td>\n",
       "      <td>20150416T000000</td>\n",
       "      <td>475000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1190</td>\n",
       "      <td>1200</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1190</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6542</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1180</td>\n",
       "      <td>1224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21593</th>\n",
       "      <td>8672200110</td>\n",
       "      <td>20150317T000000</td>\n",
       "      <td>1088000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4170</td>\n",
       "      <td>8142</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>4170</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98056</td>\n",
       "      <td>47.5354</td>\n",
       "      <td>-122.181</td>\n",
       "      <td>3030</td>\n",
       "      <td>7980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21594</th>\n",
       "      <td>5087900040</td>\n",
       "      <td>20141017T000000</td>\n",
       "      <td>350000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2500</td>\n",
       "      <td>5995</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98042</td>\n",
       "      <td>47.3749</td>\n",
       "      <td>-122.107</td>\n",
       "      <td>2530</td>\n",
       "      <td>5988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21595</th>\n",
       "      <td>1972201967</td>\n",
       "      <td>20141031T000000</td>\n",
       "      <td>520000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1530</td>\n",
       "      <td>981</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1480</td>\n",
       "      <td>50</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6533</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "      <td>1282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21596</th>\n",
       "      <td>7502800100</td>\n",
       "      <td>20140813T000000</td>\n",
       "      <td>679950.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3600</td>\n",
       "      <td>9437</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3600</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98059</td>\n",
       "      <td>47.4822</td>\n",
       "      <td>-122.131</td>\n",
       "      <td>3550</td>\n",
       "      <td>9421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21597</th>\n",
       "      <td>191100405</td>\n",
       "      <td>20150421T000000</td>\n",
       "      <td>1575000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3410</td>\n",
       "      <td>10125</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>3410</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98040</td>\n",
       "      <td>47.5653</td>\n",
       "      <td>-122.223</td>\n",
       "      <td>2290</td>\n",
       "      <td>10125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21598</th>\n",
       "      <td>8956200760</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>541800.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3118</td>\n",
       "      <td>7866</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3118</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98001</td>\n",
       "      <td>47.2931</td>\n",
       "      <td>-122.264</td>\n",
       "      <td>2673</td>\n",
       "      <td>6500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21599</th>\n",
       "      <td>7202300110</td>\n",
       "      <td>20140915T000000</td>\n",
       "      <td>810000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3990</td>\n",
       "      <td>7838</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3990</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98053</td>\n",
       "      <td>47.6857</td>\n",
       "      <td>-122.046</td>\n",
       "      <td>3370</td>\n",
       "      <td>6814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21600</th>\n",
       "      <td>249000205</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>1537000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4470</td>\n",
       "      <td>8088</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>4470</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98004</td>\n",
       "      <td>47.6321</td>\n",
       "      <td>-122.200</td>\n",
       "      <td>2780</td>\n",
       "      <td>8964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21601</th>\n",
       "      <td>5100403806</td>\n",
       "      <td>20150407T000000</td>\n",
       "      <td>467000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1425</td>\n",
       "      <td>1179</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1425</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.6963</td>\n",
       "      <td>-122.318</td>\n",
       "      <td>1285</td>\n",
       "      <td>1253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21602</th>\n",
       "      <td>844000965</td>\n",
       "      <td>20140626T000000</td>\n",
       "      <td>224000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1500</td>\n",
       "      <td>11968</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98010</td>\n",
       "      <td>47.3095</td>\n",
       "      <td>-122.002</td>\n",
       "      <td>1320</td>\n",
       "      <td>11303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21603</th>\n",
       "      <td>7852140040</td>\n",
       "      <td>20140825T000000</td>\n",
       "      <td>507250.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2270</td>\n",
       "      <td>5536</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2270</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98065</td>\n",
       "      <td>47.5389</td>\n",
       "      <td>-121.881</td>\n",
       "      <td>2270</td>\n",
       "      <td>5731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21604</th>\n",
       "      <td>9834201367</td>\n",
       "      <td>20150126T000000</td>\n",
       "      <td>429000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1490</td>\n",
       "      <td>1126</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1490</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5699</td>\n",
       "      <td>-122.288</td>\n",
       "      <td>1400</td>\n",
       "      <td>1230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21605</th>\n",
       "      <td>3448900210</td>\n",
       "      <td>20141014T000000</td>\n",
       "      <td>610685.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2520</td>\n",
       "      <td>6023</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2520</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98056</td>\n",
       "      <td>47.5137</td>\n",
       "      <td>-122.167</td>\n",
       "      <td>2520</td>\n",
       "      <td>6023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21606</th>\n",
       "      <td>7936000429</td>\n",
       "      <td>20150326T000000</td>\n",
       "      <td>1007500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3510</td>\n",
       "      <td>7200</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2600</td>\n",
       "      <td>910</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5537</td>\n",
       "      <td>-122.398</td>\n",
       "      <td>2050</td>\n",
       "      <td>6200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21607</th>\n",
       "      <td>2997800021</td>\n",
       "      <td>20150219T000000</td>\n",
       "      <td>475000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1310</td>\n",
       "      <td>1294</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1180</td>\n",
       "      <td>130</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98116</td>\n",
       "      <td>47.5773</td>\n",
       "      <td>-122.409</td>\n",
       "      <td>1330</td>\n",
       "      <td>1265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21608</th>\n",
       "      <td>263000018</td>\n",
       "      <td>20140521T000000</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1530</td>\n",
       "      <td>1131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1530</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6993</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21609</th>\n",
       "      <td>6600060120</td>\n",
       "      <td>20150223T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2310</td>\n",
       "      <td>5813</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2310</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5107</td>\n",
       "      <td>-122.362</td>\n",
       "      <td>1830</td>\n",
       "      <td>7200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21610</th>\n",
       "      <td>1523300141</td>\n",
       "      <td>20140623T000000</td>\n",
       "      <td>402101.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1350</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5944</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21611</th>\n",
       "      <td>291310100</td>\n",
       "      <td>20150116T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1600</td>\n",
       "      <td>2388</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>98027</td>\n",
       "      <td>47.5345</td>\n",
       "      <td>-122.069</td>\n",
       "      <td>1410</td>\n",
       "      <td>1287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21612</th>\n",
       "      <td>1523300157</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1076</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5941</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>1357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21613 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id             date      price  bedrooms  bathrooms  \\\n",
       "0      7129300520  20141013T000000   221900.0         3       1.00   \n",
       "1      6414100192  20141209T000000   538000.0         3       2.25   \n",
       "2      5631500400  20150225T000000   180000.0         2       1.00   \n",
       "3      2487200875  20141209T000000   604000.0         4       3.00   \n",
       "4      1954400510  20150218T000000   510000.0         3       2.00   \n",
       "5      7237550310  20140512T000000  1225000.0         4       4.50   \n",
       "6      1321400060  20140627T000000   257500.0         3       2.25   \n",
       "7      2008000270  20150115T000000   291850.0         3       1.50   \n",
       "8      2414600126  20150415T000000   229500.0         3       1.00   \n",
       "9      3793500160  20150312T000000   323000.0         3       2.50   \n",
       "10     1736800520  20150403T000000   662500.0         3       2.50   \n",
       "11     9212900260  20140527T000000   468000.0         2       1.00   \n",
       "12      114101516  20140528T000000   310000.0         3       1.00   \n",
       "13     6054650070  20141007T000000   400000.0         3       1.75   \n",
       "14     1175000570  20150312T000000   530000.0         5       2.00   \n",
       "15     9297300055  20150124T000000   650000.0         4       3.00   \n",
       "16     1875500060  20140731T000000   395000.0         3       2.00   \n",
       "17     6865200140  20140529T000000   485000.0         4       1.00   \n",
       "18       16000397  20141205T000000   189000.0         2       1.00   \n",
       "19     7983200060  20150424T000000   230000.0         3       1.00   \n",
       "20     6300500875  20140514T000000   385000.0         4       1.75   \n",
       "21     2524049179  20140826T000000  2000000.0         3       2.75   \n",
       "22     7137970340  20140703T000000   285000.0         5       2.50   \n",
       "23     8091400200  20140516T000000   252700.0         2       1.50   \n",
       "24     3814700200  20141120T000000   329000.0         3       2.25   \n",
       "25     1202000200  20141103T000000   233000.0         3       2.00   \n",
       "26     1794500383  20140626T000000   937000.0         3       1.75   \n",
       "27     3303700376  20141201T000000   667000.0         3       1.00   \n",
       "28     5101402488  20140624T000000   438000.0         3       1.75   \n",
       "29     1873100390  20150302T000000   719000.0         4       2.50   \n",
       "...           ...              ...        ...       ...        ...   \n",
       "21583  2025049203  20140610T000000   399950.0         2       1.00   \n",
       "21584   952006823  20141202T000000   380000.0         3       2.50   \n",
       "21585  3832050760  20140828T000000   270000.0         3       2.50   \n",
       "21586  2767604724  20141015T000000   505000.0         2       2.50   \n",
       "21587  6632300207  20150305T000000   385000.0         3       2.50   \n",
       "21588  2767600688  20141113T000000   414500.0         2       1.50   \n",
       "21589  7570050450  20140910T000000   347500.0         3       2.50   \n",
       "21590  7430200100  20140514T000000  1222500.0         4       3.50   \n",
       "21591  4140940150  20141002T000000   572000.0         4       2.75   \n",
       "21592  1931300412  20150416T000000   475000.0         3       2.25   \n",
       "21593  8672200110  20150317T000000  1088000.0         5       3.75   \n",
       "21594  5087900040  20141017T000000   350000.0         4       2.75   \n",
       "21595  1972201967  20141031T000000   520000.0         2       2.25   \n",
       "21596  7502800100  20140813T000000   679950.0         5       2.75   \n",
       "21597   191100405  20150421T000000  1575000.0         4       3.25   \n",
       "21598  8956200760  20141013T000000   541800.0         4       2.50   \n",
       "21599  7202300110  20140915T000000   810000.0         4       3.00   \n",
       "21600   249000205  20141015T000000  1537000.0         5       3.75   \n",
       "21601  5100403806  20150407T000000   467000.0         3       2.50   \n",
       "21602   844000965  20140626T000000   224000.0         3       1.75   \n",
       "21603  7852140040  20140825T000000   507250.0         3       2.50   \n",
       "21604  9834201367  20150126T000000   429000.0         3       2.00   \n",
       "21605  3448900210  20141014T000000   610685.0         4       2.50   \n",
       "21606  7936000429  20150326T000000  1007500.0         4       3.50   \n",
       "21607  2997800021  20150219T000000   475000.0         3       2.50   \n",
       "21608   263000018  20140521T000000   360000.0         3       2.50   \n",
       "21609  6600060120  20150223T000000   400000.0         4       2.50   \n",
       "21610  1523300141  20140623T000000   402101.0         2       0.75   \n",
       "21611   291310100  20150116T000000   400000.0         3       2.50   \n",
       "21612  1523300157  20141015T000000   325000.0         2       0.75   \n",
       "\n",
       "       sqft_living  sqft_lot  floors  waterfront  view  ...  grade  \\\n",
       "0             1180      5650     1.0           0     0  ...      7   \n",
       "1             2570      7242     2.0           0     0  ...      7   \n",
       "2              770     10000     1.0           0     0  ...      6   \n",
       "3             1960      5000     1.0           0     0  ...      7   \n",
       "4             1680      8080     1.0           0     0  ...      8   \n",
       "5             5420    101930     1.0           0     0  ...     11   \n",
       "6             1715      6819     2.0           0     0  ...      7   \n",
       "7             1060      9711     1.0           0     0  ...      7   \n",
       "8             1780      7470     1.0           0     0  ...      7   \n",
       "9             1890      6560     2.0           0     0  ...      7   \n",
       "10            3560      9796     1.0           0     0  ...      8   \n",
       "11            1160      6000     1.0           0     0  ...      7   \n",
       "12            1430     19901     1.5           0     0  ...      7   \n",
       "13            1370      9680     1.0           0     0  ...      7   \n",
       "14            1810      4850     1.5           0     0  ...      7   \n",
       "15            2950      5000     2.0           0     3  ...      9   \n",
       "16            1890     14040     2.0           0     0  ...      7   \n",
       "17            1600      4300     1.5           0     0  ...      7   \n",
       "18            1200      9850     1.0           0     0  ...      7   \n",
       "19            1250      9774     1.0           0     0  ...      7   \n",
       "20            1620      4980     1.0           0     0  ...      7   \n",
       "21            3050     44867     1.0           0     4  ...      9   \n",
       "22            2270      6300     2.0           0     0  ...      8   \n",
       "23            1070      9643     1.0           0     0  ...      7   \n",
       "24            2450      6500     2.0           0     0  ...      8   \n",
       "25            1710      4697     1.5           0     0  ...      6   \n",
       "26            2450      2691     2.0           0     0  ...      8   \n",
       "27            1400      1581     1.5           0     0  ...      8   \n",
       "28            1520      6380     1.0           0     0  ...      7   \n",
       "29            2570      7173     2.0           0     0  ...      8   \n",
       "...            ...       ...     ...         ...   ...  ...    ...   \n",
       "21583          710      1157     2.0           0     0  ...      7   \n",
       "21584         1260       900     2.0           0     0  ...      7   \n",
       "21585         1870      5000     2.0           0     0  ...      7   \n",
       "21586         1430      1201     3.0           0     0  ...      8   \n",
       "21587         1520      1488     3.0           0     0  ...      8   \n",
       "21588         1210      1278     2.0           0     0  ...      8   \n",
       "21589         2540      4760     2.0           0     0  ...      8   \n",
       "21590         4910      9444     1.5           0     0  ...     11   \n",
       "21591         2770      3852     2.0           0     0  ...      8   \n",
       "21592         1190      1200     3.0           0     0  ...      8   \n",
       "21593         4170      8142     2.0           0     2  ...     10   \n",
       "21594         2500      5995     2.0           0     0  ...      8   \n",
       "21595         1530       981     3.0           0     0  ...      8   \n",
       "21596         3600      9437     2.0           0     0  ...      9   \n",
       "21597         3410     10125     2.0           0     0  ...     10   \n",
       "21598         3118      7866     2.0           0     2  ...      9   \n",
       "21599         3990      7838     2.0           0     0  ...      9   \n",
       "21600         4470      8088     2.0           0     0  ...     11   \n",
       "21601         1425      1179     3.0           0     0  ...      8   \n",
       "21602         1500     11968     1.0           0     0  ...      6   \n",
       "21603         2270      5536     2.0           0     0  ...      8   \n",
       "21604         1490      1126     3.0           0     0  ...      8   \n",
       "21605         2520      6023     2.0           0     0  ...      9   \n",
       "21606         3510      7200     2.0           0     0  ...      9   \n",
       "21607         1310      1294     2.0           0     0  ...      8   \n",
       "21608         1530      1131     3.0           0     0  ...      8   \n",
       "21609         2310      5813     2.0           0     0  ...      8   \n",
       "21610         1020      1350     2.0           0     0  ...      7   \n",
       "21611         1600      2388     2.0           0     0  ...      8   \n",
       "21612         1020      1076     2.0           0     0  ...      7   \n",
       "\n",
       "       sqft_above  sqft_basement  yr_built  yr_renovated  zipcode      lat  \\\n",
       "0            1180              0      1955             0    98178  47.5112   \n",
       "1            2170            400      1951          1991    98125  47.7210   \n",
       "2             770              0      1933             0    98028  47.7379   \n",
       "3            1050            910      1965             0    98136  47.5208   \n",
       "4            1680              0      1987             0    98074  47.6168   \n",
       "5            3890           1530      2001             0    98053  47.6561   \n",
       "6            1715              0      1995             0    98003  47.3097   \n",
       "7            1060              0      1963             0    98198  47.4095   \n",
       "8            1050            730      1960             0    98146  47.5123   \n",
       "9            1890              0      2003             0    98038  47.3684   \n",
       "10           1860           1700      1965             0    98007  47.6007   \n",
       "11            860            300      1942             0    98115  47.6900   \n",
       "12           1430              0      1927             0    98028  47.7558   \n",
       "13           1370              0      1977             0    98074  47.6127   \n",
       "14           1810              0      1900             0    98107  47.6700   \n",
       "15           1980            970      1979             0    98126  47.5714   \n",
       "16           1890              0      1994             0    98019  47.7277   \n",
       "17           1600              0      1916             0    98103  47.6648   \n",
       "18           1200              0      1921             0    98002  47.3089   \n",
       "19           1250              0      1969             0    98003  47.3343   \n",
       "20            860            760      1947             0    98133  47.7025   \n",
       "21           2330            720      1968             0    98040  47.5316   \n",
       "22           2270              0      1995             0    98092  47.3266   \n",
       "23           1070              0      1985             0    98030  47.3533   \n",
       "24           2450              0      1985             0    98030  47.3739   \n",
       "25           1710              0      1941             0    98002  47.3048   \n",
       "26           1750            700      1915             0    98119  47.6386   \n",
       "27           1400              0      1909             0    98112  47.6221   \n",
       "28            790            730      1948             0    98115  47.6950   \n",
       "29           2570              0      2005             0    98052  47.7073   \n",
       "...           ...            ...       ...           ...      ...      ...   \n",
       "21583         710              0      1943             0    98102  47.6413   \n",
       "21584         940            320      2007             0    98116  47.5621   \n",
       "21585        1870              0      2009             0    98042  47.3339   \n",
       "21586        1430              0      2009             0    98107  47.6707   \n",
       "21587        1520              0      2006             0    98125  47.7337   \n",
       "21588        1020            190      2007             0    98117  47.6756   \n",
       "21589        2540              0      2010             0    98038  47.3452   \n",
       "21590        3110           1800      2007             0    98074  47.6502   \n",
       "21591        2770              0      2014             0    98178  47.5001   \n",
       "21592        1190              0      2008             0    98103  47.6542   \n",
       "21593        4170              0      2006             0    98056  47.5354   \n",
       "21594        2500              0      2008             0    98042  47.3749   \n",
       "21595        1480             50      2006             0    98103  47.6533   \n",
       "21596        3600              0      2014             0    98059  47.4822   \n",
       "21597        3410              0      2007             0    98040  47.5653   \n",
       "21598        3118              0      2014             0    98001  47.2931   \n",
       "21599        3990              0      2003             0    98053  47.6857   \n",
       "21600        4470              0      2008             0    98004  47.6321   \n",
       "21601        1425              0      2008             0    98125  47.6963   \n",
       "21602        1500              0      2014             0    98010  47.3095   \n",
       "21603        2270              0      2003             0    98065  47.5389   \n",
       "21604        1490              0      2014             0    98144  47.5699   \n",
       "21605        2520              0      2014             0    98056  47.5137   \n",
       "21606        2600            910      2009             0    98136  47.5537   \n",
       "21607        1180            130      2008             0    98116  47.5773   \n",
       "21608        1530              0      2009             0    98103  47.6993   \n",
       "21609        2310              0      2014             0    98146  47.5107   \n",
       "21610        1020              0      2009             0    98144  47.5944   \n",
       "21611        1600              0      2004             0    98027  47.5345   \n",
       "21612        1020              0      2008             0    98144  47.5941   \n",
       "\n",
       "          long  sqft_living15  sqft_lot15  \n",
       "0     -122.257           1340        5650  \n",
       "1     -122.319           1690        7639  \n",
       "2     -122.233           2720        8062  \n",
       "3     -122.393           1360        5000  \n",
       "4     -122.045           1800        7503  \n",
       "5     -122.005           4760      101930  \n",
       "6     -122.327           2238        6819  \n",
       "7     -122.315           1650        9711  \n",
       "8     -122.337           1780        8113  \n",
       "9     -122.031           2390        7570  \n",
       "10    -122.145           2210        8925  \n",
       "11    -122.292           1330        6000  \n",
       "12    -122.229           1780       12697  \n",
       "13    -122.045           1370       10208  \n",
       "14    -122.394           1360        4850  \n",
       "15    -122.375           2140        4000  \n",
       "16    -121.962           1890       14018  \n",
       "17    -122.343           1610        4300  \n",
       "18    -122.210           1060        5095  \n",
       "19    -122.306           1280        8850  \n",
       "20    -122.341           1400        4980  \n",
       "21    -122.233           4110       20336  \n",
       "22    -122.169           2240        7005  \n",
       "23    -122.166           1220        8386  \n",
       "24    -122.172           2200        6865  \n",
       "25    -122.218           1030        4705  \n",
       "26    -122.360           1760        3573  \n",
       "27    -122.314           1860        3861  \n",
       "28    -122.304           1520        6235  \n",
       "29    -122.110           2630        6026  \n",
       "...        ...            ...         ...  \n",
       "21583 -122.329           1370        1173  \n",
       "21584 -122.384           1310        1415  \n",
       "21585 -122.055           2170        5399  \n",
       "21586 -122.381           1430        1249  \n",
       "21587 -122.309           1520        1497  \n",
       "21588 -122.375           1210        1118  \n",
       "21589 -122.022           2540        4571  \n",
       "21590 -122.066           4560       11063  \n",
       "21591 -122.232           1810        5641  \n",
       "21592 -122.346           1180        1224  \n",
       "21593 -122.181           3030        7980  \n",
       "21594 -122.107           2530        5988  \n",
       "21595 -122.346           1530        1282  \n",
       "21596 -122.131           3550        9421  \n",
       "21597 -122.223           2290       10125  \n",
       "21598 -122.264           2673        6500  \n",
       "21599 -122.046           3370        6814  \n",
       "21600 -122.200           2780        8964  \n",
       "21601 -122.318           1285        1253  \n",
       "21602 -122.002           1320       11303  \n",
       "21603 -121.881           2270        5731  \n",
       "21604 -122.288           1400        1230  \n",
       "21605 -122.167           2520        6023  \n",
       "21606 -122.398           2050        6200  \n",
       "21607 -122.409           1330        1265  \n",
       "21608 -122.346           1530        1509  \n",
       "21609 -122.362           1830        7200  \n",
       "21610 -122.299           1020        2007  \n",
       "21611 -122.069           1410        1287  \n",
       "21612 -122.299           1020        1357  \n",
       "\n",
       "[21613 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('kc_house_data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_selection = [\"bedrooms\",\"bathrooms\",\"sqft_living\",\"sqft_lot\",\n",
    "                    \"floors\",\"condition\",\"grade\",\"sqft_above\",\n",
    "                    \"sqft_basement\",\"sqft_living15\",\"sqft_lot15\",\n",
    "                    \"lat\", \"long\",\"yr_built\",\"yr_renovated\",\"waterfront\"]\n",
    "\n",
    "selected_feature = np.array(data[column_selection])\n",
    "price = np.array(data[\"price\"])\n",
    "selected_feature_train = selected_feature[:20000]\n",
    "price_train = price[:20000]\n",
    "\n",
    "selected_feature_test = selected_feature[20000:]\n",
    "price_test = price[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y,y_pred):\n",
    "    return np.mean(np.abs(y-y_pred)/y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_len = len(column_selection)\n",
    "model.add(keras.layers.Dense(50, input_dim=input_len, activation='relu'))\n",
    "model.add(keras.layers.Dense(50, activation='relu'))\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 360538814178.9184\n",
      "Epoch 2/50\n",
      "20000/20000 [==============================] - 1s 69us/step - loss: 176711990306.4064\n",
      "Epoch 3/50\n",
      "20000/20000 [==============================] - 1s 64us/step - loss: 76890346409.1648\n",
      "Epoch 4/50\n",
      "20000/20000 [==============================] - 1s 59us/step - loss: 73134375993.3440\n",
      "Epoch 5/50\n",
      "20000/20000 [==============================] - 1s 59us/step - loss: 70548047645.9008\n",
      "Epoch 6/50\n",
      "20000/20000 [==============================] - 1s 60us/step - loss: 67993962171.5968\n",
      "Epoch 7/50\n",
      "20000/20000 [==============================] - 1s 58us/step - loss: 66371519604.3264\n",
      "Epoch 8/50\n",
      "20000/20000 [==============================] - 1s 61us/step - loss: 65063729364.9920\n",
      "Epoch 9/50\n",
      "20000/20000 [==============================] - 1s 57us/step - loss: 64579992367.9232\n",
      "Epoch 10/50\n",
      "20000/20000 [==============================] - 1s 61us/step - loss: 64306968618.5984\n",
      "Epoch 11/50\n",
      "20000/20000 [==============================] - 1s 55us/step - loss: 64717183097.2416\n",
      "Epoch 12/50\n",
      "20000/20000 [==============================] - 1s 64us/step - loss: 64585314179.4816\n",
      "Epoch 13/50\n",
      "20000/20000 [==============================] - 1s 57us/step - loss: 64117513571.5328\n",
      "Epoch 14/50\n",
      "20000/20000 [==============================] - 1s 58us/step - loss: 63914517987.3280\n",
      "Epoch 15/50\n",
      "20000/20000 [==============================] - 1s 58us/step - loss: 63719355436.2368\n",
      "Epoch 16/50\n",
      "20000/20000 [==============================] - 1s 63us/step - loss: 63940309208.2688\n",
      "Epoch 17/50\n",
      "20000/20000 [==============================] - 1s 62us/step - loss: 63937979586.9696\n",
      "Epoch 18/50\n",
      "20000/20000 [==============================] - 1s 58us/step - loss: 63462278555.2384\n",
      "Epoch 19/50\n",
      "20000/20000 [==============================] - 1s 58us/step - loss: 63310684487.6800\n",
      "Epoch 20/50\n",
      "20000/20000 [==============================] - 1s 56us/step - loss: 63336155355.5456\n",
      "Epoch 21/50\n",
      "20000/20000 [==============================] - 1s 55us/step - loss: 62838133522.4320\n",
      "Epoch 22/50\n",
      "20000/20000 [==============================] - 1s 61us/step - loss: 62510018645.1968\n",
      "Epoch 23/50\n",
      "20000/20000 [==============================] - 1s 56us/step - loss: 62357393899.5200\n",
      "Epoch 24/50\n",
      "20000/20000 [==============================] - 1s 58us/step - loss: 62397925313.7408\n",
      "Epoch 25/50\n",
      "20000/20000 [==============================] - 1s 55us/step - loss: 62263991546.6752\n",
      "Epoch 26/50\n",
      "20000/20000 [==============================] - 1s 71us/step - loss: 61730005843.9680\n",
      "Epoch 27/50\n",
      "20000/20000 [==============================] - 1s 62us/step - loss: 61842929208.5248\n",
      "Epoch 28/50\n",
      "20000/20000 [==============================] - 1s 61us/step - loss: 61287780620.6976\n",
      "Epoch 29/50\n",
      "20000/20000 [==============================] - 1s 62us/step - loss: 61345347246.4896\n",
      "Epoch 30/50\n",
      "20000/20000 [==============================] - 1s 58us/step - loss: 60940353614.6432\n",
      "Epoch 31/50\n",
      "20000/20000 [==============================] - 1s 61us/step - loss: 60729951276.2368\n",
      "Epoch 32/50\n",
      "20000/20000 [==============================] - 1s 57us/step - loss: 60197170819.8912\n",
      "Epoch 33/50\n",
      "20000/20000 [==============================] - 1s 57us/step - loss: 60142347118.1824\n",
      "Epoch 34/50\n",
      "20000/20000 [==============================] - 1s 53us/step - loss: 59435284201.4720\n",
      "Epoch 35/50\n",
      "20000/20000 [==============================] - 1s 56us/step - loss: 59408277261.5168\n",
      "Epoch 36/50\n",
      "20000/20000 [==============================] - 1s 56us/step - loss: 59014199115.7760\n",
      "Epoch 37/50\n",
      "20000/20000 [==============================] - 1s 60us/step - loss: 58727108896.3584\n",
      "Epoch 38/50\n",
      "20000/20000 [==============================] - 1s 57us/step - loss: 58939593968.8448\n",
      "Epoch 39/50\n",
      "20000/20000 [==============================] - 1s 56us/step - loss: 58305407044.8128\n",
      "Epoch 40/50\n",
      "20000/20000 [==============================] - 1s 54us/step - loss: 57580179855.7696\n",
      "Epoch 41/50\n",
      "20000/20000 [==============================] - 1s 59us/step - loss: 57185575888.4864\n",
      "Epoch 42/50\n",
      "20000/20000 [==============================] - 1s 54us/step - loss: 57051105761.6896\n",
      "Epoch 43/50\n",
      "20000/20000 [==============================] - 1s 55us/step - loss: 56863647029.6576\n",
      "Epoch 44/50\n",
      "20000/20000 [==============================] - 1s 60us/step - loss: 56564608663.5520\n",
      "Epoch 45/50\n",
      "20000/20000 [==============================] - 1s 56us/step - loss: 56938799818.3424\n",
      "Epoch 46/50\n",
      "20000/20000 [==============================] - 2s 86us/step - loss: 56004492047.1552\n",
      "Epoch 47/50\n",
      "20000/20000 [==============================] - 1s 63us/step - loss: 55471756869.6320\n",
      "Epoch 48/50\n",
      "20000/20000 [==============================] - 1s 55us/step - loss: 55045969149.9520\n",
      "Epoch 49/50\n",
      "20000/20000 [==============================] - 1s 59us/step - loss: 54925083947.8272\n",
      "Epoch 50/50\n",
      "20000/20000 [==============================] - 1s 53us/step - loss: 54652484753.8176\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(selected_feature_train, price_train, epochs=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5347612664268558"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(selected_feature_test)\n",
    "score(preds, price_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Like SKLearn, it's easy to train and evaluate simple models.\n",
    "#### ... but we should try to do better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Deep Learning -- What you need to know\n",
    "### Train, Validation, Test:\n",
    "   * Optimize parameters with Train (weights, biases)\n",
    "   * Optimize hyperparameters with Validation (layer width & depth, activation functions, etc.)\n",
    "   * Optimize NOTHING with Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split out a validation set for hyperparameter optimization\n",
    "\n",
    "selected_feature_train = selected_feature[:18000]\n",
    "price_train = price[:18000]\n",
    "\n",
    "selected_feature_val = selected_feature[18000:20000]\n",
    "price_val = price[18000:20000]\n",
    "\n",
    "selected_feature_test = selected_feature[20000:]\n",
    "price_test = price[20000:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the future, try better validation schemes like [k-fold cross validation](https://chrisalbon.com/deep_learning/keras/k-fold_cross-validating_neural_networks/), though 80/20 or 90/10 train/val like this works in a pinch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a hyperparameter optimization:\n",
    "\n",
    "### Try three activation functions to use for dense layers in the neural network above. Save the model that achieves the best validation loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint: [activation functions](http://letmegooglethat.com/?q=keras+activation+functions)\n",
    "\n",
    "#### Hint: `model.fit` has argument \"`validation_data`\" which takes a tuple of features and targets\n",
    "\n",
    "#### Hint: Use `model.save(\"filename.h5\")` to save a model locally. If you want to use it later, just call `keras.models.load_model(\"filename.h5\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For easy looping, define neural network model as a function\n",
    "def nn_model(optimizer='adam',\n",
    "             activation='relu',\n",
    "             layers=[20,20],\n",
    "             loss='mean_squared_error'):\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(50, input_dim=input_len, activation=activ))\n",
    "    model.add(keras.layers.Dense(50, activation=activ))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 2s 99us/step - loss: 532974.7174 - val_loss: 557915.6670\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 532968.7602 - val_loss: 557910.8610\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532964.2037 - val_loss: 557906.4430\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 532959.7246 - val_loss: 557901.8270\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 532955.3759 - val_loss: 557897.6330\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532950.7931 - val_loss: 557892.8650\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 532946.3322 - val_loss: 557888.6070\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 532941.9631 - val_loss: 557883.9945\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532937.6026 - val_loss: 557879.7920\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532933.2963 - val_loss: 557875.6650\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 532929.0053 - val_loss: 557871.4670\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532924.7087 - val_loss: 557866.8730\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 532920.4375 - val_loss: 557862.7180\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 532916.1712 - val_loss: 557858.4835\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 532911.9066 - val_loss: 557853.9945\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 532907.6387 - val_loss: 557849.8125\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 532903.3873 - val_loss: 557845.6990\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532899.1466 - val_loss: 557841.5335\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532894.9057 - val_loss: 557837.0550\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 532890.6471 - val_loss: 557832.8650\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532886.4000 - val_loss: 557828.6670\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 532882.1700 - val_loss: 557824.5060\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 532877.9314 - val_loss: 557820.0105\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 532873.6860 - val_loss: 557815.8230\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 532869.4526 - val_loss: 557811.7380\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 532865.2294 - val_loss: 557807.6650\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 532860.9917 - val_loss: 557803.4790\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532856.7554 - val_loss: 557798.9855\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 532852.5114 - val_loss: 557794.8320\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532848.2853 - val_loss: 557790.6110\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 532844.0533 - val_loss: 557786.4430\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 1s 61us/step - loss: 532839.8176 - val_loss: 557781.9765\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532835.5835 - val_loss: 557777.7980\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 532831.3533 - val_loss: 557773.6930\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 532827.1217 - val_loss: 557769.5335\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 532822.8975 - val_loss: 557765.0325\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 532818.1664 - val_loss: 557759.9945\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532813.4953 - val_loss: 557755.6930\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532808.9958 - val_loss: 557751.0485\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 532804.5038 - val_loss: 557746.6670\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 532800.0723 - val_loss: 557742.3360\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 532795.6228 - val_loss: 557737.7920\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 532791.2164 - val_loss: 557733.5335\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 532786.8159 - val_loss: 557728.9855\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 532782.4059 - val_loss: 557724.6410\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532778.0180 - val_loss: 557720.0105\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 532773.6054 - val_loss: 557715.7920\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 532769.2358 - val_loss: 557711.5375\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532764.8543 - val_loss: 557706.9935\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532760.4571 - val_loss: 557702.6670\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 2s 97us/step - loss: 532969.1696 - val_loss: 557907.9900\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 532959.7298 - val_loss: 557900.1570\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 532952.0963 - val_loss: 557892.7580\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532944.6678 - val_loss: 557885.5155\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 532937.3105 - val_loss: 557877.9420\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532929.9998 - val_loss: 557870.7900\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 1s 60us/step - loss: 532922.7172 - val_loss: 557863.6350\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 532915.4564 - val_loss: 557856.0390\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 532908.2083 - val_loss: 557848.8890\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 532900.9683 - val_loss: 557841.7320\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 532893.7448 - val_loss: 557834.6070\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532886.5173 - val_loss: 557827.4790\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 532879.2965 - val_loss: 557819.9565\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 532872.0824 - val_loss: 557812.8360\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 532864.8647 - val_loss: 557805.6930\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 1s 66us/step - loss: 532857.6686 - val_loss: 557798.5080\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 1s 65us/step - loss: 532850.4608 - val_loss: 557791.4690\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 1s 63us/step - loss: 532843.2531 - val_loss: 557783.9360\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 1s 66us/step - loss: 532836.0522 - val_loss: 557776.8360\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 1s 60us/step - loss: 532828.8512 - val_loss: 557769.6930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 532821.6599 - val_loss: 557762.4860\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 532814.4595 - val_loss: 557755.4690\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 532807.2582 - val_loss: 557747.9335\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 1s 60us/step - loss: 532800.0660 - val_loss: 557740.8360\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 532792.8648 - val_loss: 557733.6970\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 532785.6789 - val_loss: 557726.5770\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 1s 39us/step - loss: 532778.4769 - val_loss: 557719.4790\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 532771.2833 - val_loss: 557711.9525\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 1s 69us/step - loss: 532764.0988 - val_loss: 557704.8545\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 1s 61us/step - loss: 532756.9055 - val_loss: 557697.7030\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 1s 61us/step - loss: 532749.7160 - val_loss: 557690.6030\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 1s 61us/step - loss: 532742.5183 - val_loss: 557683.4930\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 1s 67us/step - loss: 532735.3206 - val_loss: 557675.9765\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 1s 62us/step - loss: 532728.1320 - val_loss: 557668.8625\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 1s 60us/step - loss: 532720.9374 - val_loss: 557661.7320\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 1s 66us/step - loss: 532713.7546 - val_loss: 557654.6070\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 1s 65us/step - loss: 532706.5564 - val_loss: 557647.4970\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 1s 64us/step - loss: 532699.3692 - val_loss: 557639.9805\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 1s 65us/step - loss: 532692.1754 - val_loss: 557632.8730\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 1s 63us/step - loss: 532684.9742 - val_loss: 557625.7820\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 1s 64us/step - loss: 532677.8007 - val_loss: 557618.6290\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 1s 62us/step - loss: 532670.6046 - val_loss: 557611.5335\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 1s 63us/step - loss: 532663.4173 - val_loss: 557604.0085\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 1s 65us/step - loss: 532656.2242 - val_loss: 557596.9855\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 1s 61us/step - loss: 532649.0298 - val_loss: 557589.7920\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 1s 70us/step - loss: 532641.8514 - val_loss: 557582.6450\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 1s 63us/step - loss: 532634.6524 - val_loss: 557575.5375\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 1s 64us/step - loss: 532627.4666 - val_loss: 557568.3360\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 1s 62us/step - loss: 532620.2627 - val_loss: 557560.9955\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 1s 67us/step - loss: 532613.0711 - val_loss: 557553.7980\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 2s 104us/step - loss: 472804.7996 - val_loss: 401794.4585\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 301376.5402 - val_loss: 235273.2935\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 178595.2921 - val_loss: 176629.2709\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 165651.6534 - val_loss: 173566.6380\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 162754.4571 - val_loss: 171222.4909\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 160621.4162 - val_loss: 170948.7162\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 159335.8002 - val_loss: 169169.4987\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 158542.5143 - val_loss: 169141.4376\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 157844.0023 - val_loss: 168250.6416\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 157694.5680 - val_loss: 168510.1495\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 157078.3578 - val_loss: 168850.4590\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 157045.7321 - val_loss: 167598.6501\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 156469.7378 - val_loss: 167802.5931\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 156637.3876 - val_loss: 168461.2541\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 156263.6403 - val_loss: 167139.3281\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 155866.1031 - val_loss: 167115.5520\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 155648.9677 - val_loss: 169172.7690\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 155551.2565 - val_loss: 167427.2250\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 155356.3006 - val_loss: 167137.6839\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 155440.9879 - val_loss: 166605.6699\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 155125.8086 - val_loss: 167200.9721\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 154980.2649 - val_loss: 166520.6943\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 154682.3014 - val_loss: 166382.0510\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 154527.8592 - val_loss: 166337.1187\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 154374.8120 - val_loss: 166231.6698\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 154422.7759 - val_loss: 166034.6841\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 154115.4780 - val_loss: 166270.4028\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 154166.1842 - val_loss: 166475.6230\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 154001.0935 - val_loss: 165962.5161\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 153761.4238 - val_loss: 166618.1141\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 153866.3554 - val_loss: 165904.9729\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 153517.0859 - val_loss: 166580.4061\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 153603.6987 - val_loss: 166669.9042\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 153311.8284 - val_loss: 165695.1759\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 153228.3786 - val_loss: 165212.6807\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 153040.6459 - val_loss: 165467.0003\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 153202.5647 - val_loss: 166550.3391\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 152949.3330 - val_loss: 165592.7661\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 152810.0278 - val_loss: 164956.9771\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 152602.4397 - val_loss: 165401.1629\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 45us/step - loss: 152549.3295 - val_loss: 165636.6327\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 152364.9077 - val_loss: 166133.8819\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 152662.0058 - val_loss: 164888.3620\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 151970.5305 - val_loss: 164769.1478\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 152018.1717 - val_loss: 164714.2631\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 151872.6236 - val_loss: 165175.4650\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 151553.5553 - val_loss: 163971.6240\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 151431.7064 - val_loss: 164684.3101\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 151289.4227 - val_loss: 164164.0941\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 151136.9308 - val_loss: 163754.5761\n",
      "BEST ACTIVATION FUNCTION relu WITH SCORE 0.6217208129164856\n"
     ]
    }
   ],
   "source": [
    "best_score = 1000.0 # bad\n",
    "\n",
    "# loop over chosen activation functions, train, evaluate on validation\n",
    "for activ in ['sigmoid', 'tanh', 'relu']:\n",
    "    model = nn_model(activation=activ)\n",
    "\n",
    "    history = model.fit(selected_feature_train, price_train,\n",
    "                epochs=50, batch_size=128,\n",
    "                validation_data=(selected_feature_val, price_val))\n",
    "    model_score = score(model.predict(selected_feature_val), price_val)\n",
    "\n",
    "    if model_score < best_score:\n",
    "        best_score = model_score\n",
    "        best_activ = activ\n",
    "        best_model = model\n",
    "        best_train = history\n",
    "\n",
    "print(f\"BEST ACTIVATION FUNCTION {best_activ} WITH SCORE {best_score}\")\n",
    "best_model.save(\"awesome_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize your training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot loss during training\n",
    "def plot_loss(hist):\n",
    "    %matplotlib inline\n",
    "    plt.title('Training Curve')\n",
    "    plt.plot(hist.history['loss'], label='train')\n",
    "    plt.plot(hist.history['val_loss'], label='validation')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Mean squared error\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(best_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Standardize your features:\n",
    "* Typically assumes normally distributed feature, shifting mean to 0 and standard deviation to 1\n",
    "* In theory does not matter for neural networks\n",
    "* In practice tends to matter for neural networks\n",
    "* Scale if using:\n",
    "    - Logistic regression\n",
    "    - Support vector machines\n",
    "    - Perceptrons\n",
    "    - Neural networks\n",
    "    - Principle component analysis\n",
    "* Don't bother if using:\n",
    "    - \"Forest\" methods\n",
    "    - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.158106372019069e-17\n",
      "1.0\n",
      "0.016119707489855875\n",
      "0.9970087526331801\n",
      "0.09330835574898895\n",
      "0.9414925643047213\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate StandardScaler\n",
    "in_scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler to the training set and perform the transformation\n",
    "selected_feature_train = in_scaler.fit_transform(selected_feature_train)\n",
    "\n",
    "# Use the fitted scaler to transform validation and test features\n",
    "selected_feature_val = in_scaler.transform(selected_feature_val)\n",
    "selected_feature_test = in_scaler.transform(selected_feature_test)\n",
    "\n",
    "# Check appropriate scaling\n",
    "print(np.mean(selected_feature_train[:,0]))\n",
    "print(np.std(selected_feature_train[:,0]))\n",
    "\n",
    "print(np.mean(selected_feature_val[:,0]))\n",
    "print(np.std(selected_feature_val[:,0]))\n",
    "\n",
    "print(np.mean(selected_feature_test[:,0]))\n",
    "print(np.std(selected_feature_test[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "18000/18000 [==============================] - 2s 108us/step - loss: 416645927723.9182 - val_loss: 456240232923.1359\n",
      "Epoch 2/100\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 416187103605.1911 - val_loss: 455147792367.6160\n",
      "Epoch 3/100\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 414336983935.6587 - val_loss: 451913923690.4960\n",
      "Epoch 4/100\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 410058997308.5298 - val_loss: 445496835964.9279\n",
      "Epoch 5/100\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 402618084345.6284 - val_loss: 435216807624.7040\n",
      "Epoch 6/100\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 391501845626.8800 - val_loss: 420637195894.7840\n",
      "Epoch 7/100\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 376546572690.3182 - val_loss: 401769660678.1440\n",
      "Epoch 8/100\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 357702829400.9742 - val_loss: 378589209690.1120\n",
      "Epoch 9/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 335341150302.6631 - val_loss: 351823382446.0800\n",
      "Epoch 10/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 310123299573.3049 - val_loss: 322592855556.0960\n",
      "Epoch 11/100\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 283027313553.8631 - val_loss: 291508735180.8000\n",
      "Epoch 12/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 255111234264.1778 - val_loss: 260367809249.2800\n",
      "Epoch 13/100\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 227178161577.0738 - val_loss: 229734605520.8960\n",
      "Epoch 14/100\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 200397042920.1067 - val_loss: 200771407839.2320\n",
      "Epoch 15/100\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 175821481749.1627 - val_loss: 175137526382.5920\n",
      "Epoch 16/100\n",
      "18000/18000 [==============================] - 1s 59us/step - loss: 153973956542.4640 - val_loss: 152862858805.2480\n",
      "Epoch 17/100\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 135376233773.2835 - val_loss: 134154258612.2240\n",
      "Epoch 18/100\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 120114381834.0125 - val_loss: 119361942192.1280\n",
      "Epoch 19/100\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 107981201418.9227 - val_loss: 107878251495.4240\n",
      "Epoch 20/100\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 98654568695.5804 - val_loss: 99286808395.7760\n",
      "Epoch 21/100\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 91702646210.5600 - val_loss: 93118149361.6640\n",
      "Epoch 22/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 86569095943.5093 - val_loss: 88522137731.0720\n",
      "Epoch 23/100\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 82765000401.8062 - val_loss: 85170708152.3200\n",
      "Epoch 24/100\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 79900168202.9227 - val_loss: 82635609079.8080\n",
      "Epoch 25/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 77699282989.5111 - val_loss: 80646954221.5680\n",
      "Epoch 26/100\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 75922185180.5013 - val_loss: 78978957901.8240\n",
      "Epoch 27/100\n",
      "18000/18000 [==============================] - 1s 69us/step - loss: 74438817917.6107 - val_loss: 77570686124.0320\n",
      "Epoch 28/100\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 73161846991.5307 - val_loss: 76324637048.8320\n",
      "Epoch 29/100\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 72013772161.0240 - val_loss: 75190801661.9520\n",
      "Epoch 30/100\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 70977901727.2889 - val_loss: 74151349714.9440\n",
      "Epoch 31/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 69997556873.4436 - val_loss: 73177726648.3200\n",
      "Epoch 32/100\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 69081263297.8773 - val_loss: 72247403380.7360\n",
      "Epoch 33/100\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 68198701887.9431 - val_loss: 71358892015.6160\n",
      "Epoch 34/100\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 67352521131.8044 - val_loss: 70502465372.1600\n",
      "Epoch 35/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 66531922579.0009 - val_loss: 69669957107.7120\n",
      "Epoch 36/100\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 65718275691.8613 - val_loss: 68853849948.1600\n",
      "Epoch 37/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 64921477047.1822 - val_loss: 68044942999.5520\n",
      "Epoch 38/100\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 64143362515.8542 - val_loss: 67259565670.4000\n",
      "Epoch 39/100\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 63366376878.5351 - val_loss: 66486845112.3200\n",
      "Epoch 40/100\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 62582597011.2284 - val_loss: 65699212427.2640\n",
      "Epoch 41/100\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 61808136246.6133 - val_loss: 64935213596.6720\n",
      "Epoch 42/100\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 61044295350.4996 - val_loss: 64171037884.4160\n",
      "Epoch 43/100\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 60278395003.7902 - val_loss: 63404927090.6880\n",
      "Epoch 44/100\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 59528468336.1849 - val_loss: 62652887367.6800\n",
      "Epoch 45/100\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 58758556447.1751 - val_loss: 61904679895.0400\n",
      "Epoch 46/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 58004192231.4240 - val_loss: 61153097187.3280\n",
      "Epoch 47/100\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 57246720349.5253 - val_loss: 60409889914.8800\n",
      "Epoch 48/100\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 56487952216.5191 - val_loss: 59669742288.8960\n",
      "Epoch 49/100\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 55723326018.9013 - val_loss: 58925996408.8320\n",
      "Epoch 50/100\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 54974660644.4089 - val_loss: 58181650546.6880\n",
      "Epoch 51/100\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 54218661123.4133 - val_loss: 57454665990.1440\n",
      "Epoch 52/100\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 53459035306.2116 - val_loss: 56727317086.2080\n",
      "Epoch 53/100\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 52708696078.5636 - val_loss: 55997075259.3920\n",
      "Epoch 54/100\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 51960347565.1698 - val_loss: 55274317742.0800\n",
      "Epoch 55/100\n",
      "18000/18000 [==============================] - 1s 35us/step - loss: 51216513266.1191 - val_loss: 54578591105.0240\n",
      "Epoch 56/100\n",
      "18000/18000 [==============================] - 1s 36us/step - loss: 50468591046.2009 - val_loss: 53867890671.6160\n",
      "Epoch 57/100\n",
      "18000/18000 [==============================] - 1s 38us/step - loss: 49721928730.3964 - val_loss: 53166344798.2080\n",
      "Epoch 58/100\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 49000098765.9378 - val_loss: 52478417043.4560\n",
      "Epoch 59/100\n",
      "18000/18000 [==============================] - 1s 37us/step - loss: 48277290831.4169 - val_loss: 51801329238.0160\n",
      "Epoch 60/100\n",
      "18000/18000 [==============================] - 1s 38us/step - loss: 47568173112.4338 - val_loss: 51126590832.6400\n",
      "Epoch 61/100\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 46867635353.8276 - val_loss: 50467348086.7840\n",
      "Epoch 62/100\n",
      "18000/18000 [==============================] - 1s 36us/step - loss: 46178647599.7867 - val_loss: 49827878076.4160\n",
      "Epoch 63/100\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 45493354332.1600 - val_loss: 49219458891.7760\n",
      "Epoch 64/100\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 44829549840.1564 - val_loss: 48611331473.4080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 44170255604.8498 - val_loss: 48015201599.4880\n",
      "Epoch 66/100\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 43533550001.2658 - val_loss: 47448121901.0560\n",
      "Epoch 67/100\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 42918467369.1876 - val_loss: 46893685932.0320\n",
      "Epoch 68/100\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 42330640038.1156 - val_loss: 46384785424.3840\n",
      "Epoch 69/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 41772635333.5182 - val_loss: 45899980472.3200\n",
      "Epoch 70/100\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 41235732339.8258 - val_loss: 45429848506.3680\n",
      "Epoch 71/100\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 40746181543.7084 - val_loss: 45038650654.7200\n",
      "Epoch 72/100\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 40283457057.2231 - val_loss: 44660966359.0400\n",
      "Epoch 73/100\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 39864766795.3209 - val_loss: 44311001268.2240\n",
      "Epoch 74/100\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 39467315742.4924 - val_loss: 44008656732.1600\n",
      "Epoch 75/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 39118717754.4818 - val_loss: 43763722649.6000\n",
      "Epoch 76/100\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 38793891352.1209 - val_loss: 43525809143.8080\n",
      "Epoch 77/100\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 38496555845.4044 - val_loss: 43303319437.3120\n",
      "Epoch 78/100\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 38225519538.6311 - val_loss: 43096354390.0160\n",
      "Epoch 79/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 37970481186.5884 - val_loss: 42919705378.8160\n",
      "Epoch 80/100\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 37738036250.3964 - val_loss: 42755601858.5600\n",
      "Epoch 81/100\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 37523169349.1769 - val_loss: 42608295444.4800\n",
      "Epoch 82/100\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 37319464645.0631 - val_loss: 42458685112.3200\n",
      "Epoch 83/100\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 37132166292.3662 - val_loss: 42326751379.4560\n",
      "Epoch 84/100\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 36957021241.3440 - val_loss: 42207569903.6160\n",
      "Epoch 85/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 36796438890.2684 - val_loss: 42064152690.6880\n",
      "Epoch 86/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 36645155052.6578 - val_loss: 41963410292.7360\n",
      "Epoch 87/100\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 36484818771.0578 - val_loss: 41835063083.0080\n",
      "Epoch 88/100\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 36346824267.0933 - val_loss: 41733294096.3840\n",
      "Epoch 89/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 36216476623.7582 - val_loss: 41609562718.2080\n",
      "Epoch 90/100\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 36085904568.7751 - val_loss: 41514856579.0720\n",
      "Epoch 91/100\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 35959754518.9831 - val_loss: 41406293016.5760\n",
      "Epoch 92/100\n",
      "18000/18000 [==============================] - 1s 59us/step - loss: 35846716802.8444 - val_loss: 41336929058.8160\n",
      "Epoch 93/100\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 35757141042.0622 - val_loss: 41212377530.3680\n",
      "Epoch 94/100\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 35631784167.1964 - val_loss: 41119472975.8720\n",
      "Epoch 95/100\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 35534758976.6258 - val_loss: 41038881325.0560\n",
      "Epoch 96/100\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 35440355244.2596 - val_loss: 40946202378.2400\n",
      "Epoch 97/100\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 35342117688.6613 - val_loss: 40881955274.7520\n",
      "Epoch 98/100\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 35255169104.0996 - val_loss: 40815325937.6640\n",
      "Epoch 99/100\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 35169811242.0978 - val_loss: 40697617088.5120\n",
      "Epoch 100/100\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 35084695353.5716 - val_loss: 40613025349.6320\n",
      "0.7173365658198675\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = nn_model()\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "history = model.fit(selected_feature_train, price_train,\n",
    "            epochs=100, batch_size=128,\n",
    "            validation_data=(selected_feature_val, price_val))\n",
    "model_score = score(model.predict(selected_feature_val), price_val)\n",
    "print(model_score)\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the future, consider standardizing outputs as well\n",
    "\n",
    "### Regularize:\n",
    "* Heavily parameterized models like neural networks are prone to overfitting\n",
    "* Popular off-the-shelf tools exist to regularize models and prevent overfitting:\n",
    "    - L2 regularization (weight decay)\n",
    "    - Dropout\n",
    "    - Batch normalization\n",
    "    \n",
    "#### These tools come as standard Keras/TF layers!\n",
    "`model.add(keras.layers.Dropout(rate)`\n",
    "`model.add(keras.layers.ActivityRegularization(l1=0.0, l2=0.0)`\n",
    "`model.add(keras.layers.BatchNormalization())`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping and model checkpointing:\n",
    "#### It's unlikely the last iteration is the best, and who knows how long until the thing is converged. Just grab the best validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/400\n",
      "18000/18000 [==============================] - 2s 122us/step - loss: 416645088222.3218 - val_loss: 456236671434.7521\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 456236671434.75201, saving model to best_model.h5\n",
      "Epoch 2/400\n",
      "18000/18000 [==============================] - 1s 62us/step - loss: 416197657056.5974 - val_loss: 455164537864.1920\n",
      "\n",
      "Epoch 00002: val_loss improved from 456236671434.75201 to 455164537864.19202, saving model to best_model.h5\n",
      "Epoch 3/400\n",
      "18000/18000 [==============================] - 1s 66us/step - loss: 414400001401.7422 - val_loss: 452025910820.8641\n",
      "\n",
      "Epoch 00003: val_loss improved from 455164537864.19202 to 452025910820.86401, saving model to best_model.h5\n",
      "Epoch 4/400\n",
      "18000/18000 [==============================] - 1s 63us/step - loss: 410371744945.4933 - val_loss: 445906115887.1040\n",
      "\n",
      "Epoch 00004: val_loss improved from 452025910820.86401 to 445906115887.10400, saving model to best_model.h5\n",
      "Epoch 5/400\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 403276284967.1395 - val_loss: 435968490078.2080\n",
      "\n",
      "Epoch 00005: val_loss improved from 445906115887.10400 to 435968490078.20801, saving model to best_model.h5\n",
      "Epoch 6/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 392518803273.9555 - val_loss: 421799506477.0560\n",
      "\n",
      "Epoch 00006: val_loss improved from 435968490078.20801 to 421799506477.05603, saving model to best_model.h5\n",
      "Epoch 7/400\n",
      "18000/18000 [==============================] - 1s 65us/step - loss: 377905444203.1786 - val_loss: 403042142257.1520\n",
      "\n",
      "Epoch 00007: val_loss improved from 421799506477.05603 to 403042142257.15198, saving model to best_model.h5\n",
      "Epoch 8/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 359439171758.7627 - val_loss: 380294197673.9840\n",
      "\n",
      "Epoch 00008: val_loss improved from 403042142257.15198 to 380294197673.98401, saving model to best_model.h5\n",
      "Epoch 9/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 337293403342.6205 - val_loss: 353580240863.2320\n",
      "\n",
      "Epoch 00009: val_loss improved from 380294197673.98401 to 353580240863.23199, saving model to best_model.h5\n",
      "Epoch 10/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 312120224048.0142 - val_loss: 323927445864.4480\n",
      "\n",
      "Epoch 00010: val_loss improved from 353580240863.23199 to 323927445864.44800, saving model to best_model.h5\n",
      "Epoch 11/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 284900121611.8329 - val_loss: 292531108839.4240\n",
      "\n",
      "Epoch 00011: val_loss improved from 323927445864.44800 to 292531108839.42401, saving model to best_model.h5\n",
      "Epoch 12/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 256797564250.7947 - val_loss: 261136424370.1760\n",
      "\n",
      "Epoch 00012: val_loss improved from 292531108839.42401 to 261136424370.17599, saving model to best_model.h5\n",
      "Epoch 13/400\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 228902969555.1715 - val_loss: 230564901683.2000\n",
      "\n",
      "Epoch 00013: val_loss improved from 261136424370.17599 to 230564901683.20001, saving model to best_model.h5\n",
      "Epoch 14/400\n",
      "18000/18000 [==============================] - 1s 64us/step - loss: 202254983204.4089 - val_loss: 201884320661.5040\n",
      "\n",
      "Epoch 00014: val_loss improved from 230564901683.20001 to 201884320661.50400, saving model to best_model.h5\n",
      "Epoch 15/400\n",
      "18000/18000 [==============================] - 1s 61us/step - loss: 177782115015.7938 - val_loss: 176104090370.0480\n",
      "\n",
      "Epoch 00015: val_loss improved from 201884320661.50400 to 176104090370.04800, saving model to best_model.h5\n",
      "Epoch 16/400\n",
      "18000/18000 [==============================] - 1s 62us/step - loss: 156188070830.9902 - val_loss: 154209125662.7200\n",
      "\n",
      "Epoch 00016: val_loss improved from 176104090370.04800 to 154209125662.72000, saving model to best_model.h5\n",
      "Epoch 17/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 137806850788.9209 - val_loss: 136010958635.0080\n",
      "\n",
      "Epoch 00017: val_loss improved from 154209125662.72000 to 136010958635.00800, saving model to best_model.h5\n",
      "Epoch 18/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 122601081201.5502 - val_loss: 121301015920.6400\n",
      "\n",
      "Epoch 00018: val_loss improved from 136010958635.00800 to 121301015920.64000, saving model to best_model.h5\n",
      "Epoch 19/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 110464811663.3600 - val_loss: 109919576457.2160\n",
      "\n",
      "Epoch 00019: val_loss improved from 121301015920.64000 to 109919576457.21600, saving model to best_model.h5\n",
      "Epoch 20/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 101121743585.2800 - val_loss: 101571065085.9520\n",
      "\n",
      "Epoch 00020: val_loss improved from 109919576457.21600 to 101571065085.95200, saving model to best_model.h5\n",
      "Epoch 21/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 94068384865.3938 - val_loss: 95244408913.9200\n",
      "\n",
      "Epoch 00021: val_loss improved from 101571065085.95200 to 95244408913.92000, saving model to best_model.h5\n",
      "Epoch 22/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 88756543400.6187 - val_loss: 90569775710.2080\n",
      "\n",
      "Epoch 00022: val_loss improved from 95244408913.92000 to 90569775710.20799, saving model to best_model.h5\n",
      "Epoch 23/400\n",
      "18000/18000 [==============================] - 1s 59us/step - loss: 84712594633.1591 - val_loss: 87002215809.0240\n",
      "\n",
      "Epoch 00023: val_loss improved from 90569775710.20799 to 87002215809.02400, saving model to best_model.h5\n",
      "Epoch 24/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 81603228612.8356 - val_loss: 84256221429.7600\n",
      "\n",
      "Epoch 00024: val_loss improved from 87002215809.02400 to 84256221429.75999, saving model to best_model.h5\n",
      "Epoch 25/400\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 79173733173.0204 - val_loss: 82061945143.2960\n",
      "\n",
      "Epoch 00025: val_loss improved from 84256221429.75999 to 82061945143.29601, saving model to best_model.h5\n",
      "Epoch 26/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 77199145748.2524 - val_loss: 80246319415.2960\n",
      "\n",
      "Epoch 00026: val_loss improved from 82061945143.29601 to 80246319415.29601, saving model to best_model.h5\n",
      "Epoch 27/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 75571666361.4578 - val_loss: 78672987553.7920\n",
      "\n",
      "Epoch 00027: val_loss improved from 80246319415.29601 to 78672987553.79201, saving model to best_model.h5\n",
      "Epoch 28/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 74170761594.6525 - val_loss: 77334466396.1600\n",
      "\n",
      "Epoch 00028: val_loss improved from 78672987553.79201 to 77334466396.16000, saving model to best_model.h5\n",
      "Epoch 29/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 72970862060.4302 - val_loss: 76142404042.7520\n",
      "\n",
      "Epoch 00029: val_loss improved from 77334466396.16000 to 76142404042.75200, saving model to best_model.h5\n",
      "Epoch 30/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 71886676237.4258 - val_loss: 75062966616.0640\n",
      "\n",
      "Epoch 00030: val_loss improved from 76142404042.75200 to 75062966616.06400, saving model to best_model.h5\n",
      "Epoch 31/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 70911220005.0916 - val_loss: 74074677116.9280\n",
      "\n",
      "Epoch 00031: val_loss improved from 75062966616.06400 to 74074677116.92799, saving model to best_model.h5\n",
      "Epoch 32/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 69992349397.4471 - val_loss: 73150938775.5520\n",
      "\n",
      "Epoch 00032: val_loss improved from 74074677116.92799 to 73150938775.55200, saving model to best_model.h5\n",
      "Epoch 33/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 69135029010.4320 - val_loss: 72271365406.7200\n",
      "\n",
      "Epoch 00033: val_loss improved from 73150938775.55200 to 72271365406.72000, saving model to best_model.h5\n",
      "Epoch 34/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 68318729768.5049 - val_loss: 71427720773.6320\n",
      "\n",
      "Epoch 00034: val_loss improved from 72271365406.72000 to 71427720773.63200, saving model to best_model.h5\n",
      "Epoch 35/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 67522541114.7093 - val_loss: 70637041188.8640\n",
      "\n",
      "Epoch 00035: val_loss improved from 71427720773.63200 to 70637041188.86400, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/400\n",
      "18000/18000 [==============================] - 2s 98us/step - loss: 66751103209.0169 - val_loss: 69857330593.7920\n",
      "\n",
      "Epoch 00036: val_loss improved from 70637041188.86400 to 69857330593.79201, saving model to best_model.h5\n",
      "Epoch 37/400\n",
      "18000/18000 [==============================] - 1s 65us/step - loss: 66001783550.4071 - val_loss: 69088201703.4240\n",
      "\n",
      "Epoch 00037: val_loss improved from 69857330593.79201 to 69088201703.42400, saving model to best_model.h5\n",
      "Epoch 38/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 65259026411.9751 - val_loss: 68354475753.4720\n",
      "\n",
      "Epoch 00038: val_loss improved from 69088201703.42400 to 68354475753.47200, saving model to best_model.h5\n",
      "Epoch 39/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 64521657971.1431 - val_loss: 67613030645.7600\n",
      "\n",
      "Epoch 00039: val_loss improved from 68354475753.47200 to 67613030645.76000, saving model to best_model.h5\n",
      "Epoch 40/400\n",
      "18000/18000 [==============================] - 1s 59us/step - loss: 63805681441.9058 - val_loss: 66886318161.9200\n",
      "\n",
      "Epoch 00040: val_loss improved from 67613030645.76000 to 66886318161.92000, saving model to best_model.h5\n",
      "Epoch 41/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 63085259157.9591 - val_loss: 66176570851.3280\n",
      "\n",
      "Epoch 00041: val_loss improved from 66886318161.92000 to 66176570851.32800, saving model to best_model.h5\n",
      "Epoch 42/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 62352147873.7920 - val_loss: 65469578805.2480\n",
      "\n",
      "Epoch 00042: val_loss improved from 66176570851.32800 to 65469578805.24800, saving model to best_model.h5\n",
      "Epoch 43/400\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 61642574128.0142 - val_loss: 64760455593.9840\n",
      "\n",
      "Epoch 00043: val_loss improved from 65469578805.24800 to 64760455593.98400, saving model to best_model.h5\n",
      "Epoch 44/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 60922253053.4969 - val_loss: 64051841368.0640\n",
      "\n",
      "Epoch 00044: val_loss improved from 64760455593.98400 to 64051841368.06400, saving model to best_model.h5\n",
      "Epoch 45/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 60212032700.4160 - val_loss: 63356539469.8240\n",
      "\n",
      "Epoch 00045: val_loss improved from 64051841368.06400 to 63356539469.82400, saving model to best_model.h5\n",
      "Epoch 46/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 59512277110.3289 - val_loss: 62666123214.8480\n",
      "\n",
      "Epoch 00046: val_loss improved from 63356539469.82400 to 62666123214.84800, saving model to best_model.h5\n",
      "Epoch 47/400\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 58790632022.9262 - val_loss: 61966838890.4960\n",
      "\n",
      "Epoch 00047: val_loss improved from 62666123214.84800 to 61966838890.49600, saving model to best_model.h5\n",
      "Epoch 48/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 58078336268.5156 - val_loss: 61275309015.0400\n",
      "\n",
      "Epoch 00048: val_loss improved from 61966838890.49600 to 61275309015.04000, saving model to best_model.h5\n",
      "Epoch 49/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 57367972988.7004 - val_loss: 60577946206.2080\n",
      "\n",
      "Epoch 00049: val_loss improved from 61275309015.04000 to 60577946206.20800, saving model to best_model.h5\n",
      "Epoch 50/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 56665332933.5182 - val_loss: 59889384423.4240\n",
      "\n",
      "Epoch 00050: val_loss improved from 60577946206.20800 to 59889384423.42400, saving model to best_model.h5\n",
      "Epoch 51/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 55965308481.9911 - val_loss: 59207354744.8320\n",
      "\n",
      "Epoch 00051: val_loss improved from 59889384423.42400 to 59207354744.83200, saving model to best_model.h5\n",
      "Epoch 52/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 55272883413.9022 - val_loss: 58513943822.3360\n",
      "\n",
      "Epoch 00052: val_loss improved from 59207354744.83200 to 58513943822.33600, saving model to best_model.h5\n",
      "Epoch 53/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 54556523415.3244 - val_loss: 57838029209.6000\n",
      "\n",
      "Epoch 00053: val_loss improved from 58513943822.33600 to 57838029209.60000, saving model to best_model.h5\n",
      "Epoch 54/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 53875024891.4489 - val_loss: 57163240898.5600\n",
      "\n",
      "Epoch 00054: val_loss improved from 57838029209.60000 to 57163240898.56000, saving model to best_model.h5\n",
      "Epoch 55/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 53175751162.0835 - val_loss: 56490331766.7840\n",
      "\n",
      "Epoch 00055: val_loss improved from 57163240898.56000 to 56490331766.78400, saving model to best_model.h5\n",
      "Epoch 56/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 52504312232.1636 - val_loss: 55827264897.0240\n",
      "\n",
      "Epoch 00056: val_loss improved from 56490331766.78400 to 55827264897.02400, saving model to best_model.h5\n",
      "Epoch 57/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 51812337684.0249 - val_loss: 55167151210.4960\n",
      "\n",
      "Epoch 00057: val_loss improved from 55827264897.02400 to 55167151210.49600, saving model to best_model.h5\n",
      "Epoch 58/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 51132023271.8791 - val_loss: 54514120097.7920\n",
      "\n",
      "Epoch 00058: val_loss improved from 55167151210.49600 to 54514120097.79200, saving model to best_model.h5\n",
      "Epoch 59/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 50464173471.9716 - val_loss: 53875745554.4320\n",
      "\n",
      "Epoch 00059: val_loss improved from 54514120097.79200 to 53875745554.43200, saving model to best_model.h5\n",
      "Epoch 60/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 49799531772.1316 - val_loss: 53240766070.7840\n",
      "\n",
      "Epoch 00060: val_loss improved from 53875745554.43200 to 53240766070.78400, saving model to best_model.h5\n",
      "Epoch 61/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 49152544068.9493 - val_loss: 52635786477.5680\n",
      "\n",
      "Epoch 00061: val_loss improved from 53240766070.78400 to 52635786477.56800, saving model to best_model.h5\n",
      "Epoch 62/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 48499499022.5636 - val_loss: 52019728154.6240\n",
      "\n",
      "Epoch 00062: val_loss improved from 52635786477.56800 to 52019728154.62400, saving model to best_model.h5\n",
      "Epoch 63/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 47870956175.3600 - val_loss: 51429909331.9680\n",
      "\n",
      "Epoch 00063: val_loss improved from 52019728154.62400 to 51429909331.96800, saving model to best_model.h5\n",
      "Epoch 64/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 47250644325.7173 - val_loss: 50849696153.6000\n",
      "\n",
      "Epoch 00064: val_loss improved from 51429909331.96800 to 50849696153.60000, saving model to best_model.h5\n",
      "Epoch 65/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 46627214440.6756 - val_loss: 50274833858.5600\n",
      "\n",
      "Epoch 00065: val_loss improved from 50849696153.60000 to 50274833858.56000, saving model to best_model.h5\n",
      "Epoch 66/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 46028059985.6924 - val_loss: 49711005827.0720\n",
      "\n",
      "Epoch 00066: val_loss improved from 50274833858.56000 to 49711005827.07200, saving model to best_model.h5\n",
      "Epoch 67/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 45431022849.5929 - val_loss: 49183650480.1280\n",
      "\n",
      "Epoch 00067: val_loss improved from 49711005827.07200 to 49183650480.12800, saving model to best_model.h5\n",
      "Epoch 68/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 44862129751.8364 - val_loss: 48653637189.6320\n",
      "\n",
      "Epoch 00068: val_loss improved from 49183650480.12800 to 48653637189.63200, saving model to best_model.h5\n",
      "Epoch 69/400\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 44295542971.5058 - val_loss: 48132611080.1920\n",
      "\n",
      "Epoch 00069: val_loss improved from 48653637189.63200 to 48132611080.19200, saving model to best_model.h5\n",
      "Epoch 70/400\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 43738164989.4969 - val_loss: 47630259355.6480\n",
      "\n",
      "Epoch 00070: val_loss improved from 48132611080.19200 to 47630259355.64800, saving model to best_model.h5\n",
      "Epoch 71/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 48us/step - loss: 43203405716.5938 - val_loss: 47153690738.6880\n",
      "\n",
      "Epoch 00071: val_loss improved from 47630259355.64800 to 47153690738.68800, saving model to best_model.h5\n",
      "Epoch 72/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 42679820593.8347 - val_loss: 46684520611.8400\n",
      "\n",
      "Epoch 00072: val_loss improved from 47153690738.68800 to 46684520611.84000, saving model to best_model.h5\n",
      "Epoch 73/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 42188650075.4773 - val_loss: 46244600971.2640\n",
      "\n",
      "Epoch 00073: val_loss improved from 46684520611.84000 to 46244600971.26400, saving model to best_model.h5\n",
      "Epoch 74/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 41699817934.3929 - val_loss: 45842708004.8640\n",
      "\n",
      "Epoch 00074: val_loss improved from 46244600971.26400 to 45842708004.86400, saving model to best_model.h5\n",
      "Epoch 75/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 41247992497.9484 - val_loss: 45455436611.5840\n",
      "\n",
      "Epoch 00075: val_loss improved from 45842708004.86400 to 45455436611.58400, saving model to best_model.h5\n",
      "Epoch 76/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 40832375728.8107 - val_loss: 45075864322.0480\n",
      "\n",
      "Epoch 00076: val_loss improved from 45455436611.58400 to 45075864322.04800, saving model to best_model.h5\n",
      "Epoch 77/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 40425152253.4969 - val_loss: 44754962612.2240\n",
      "\n",
      "Epoch 00077: val_loss improved from 45075864322.04800 to 44754962612.22400, saving model to best_model.h5\n",
      "Epoch 78/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 40050075356.7289 - val_loss: 44456154431.4880\n",
      "\n",
      "Epoch 00078: val_loss improved from 44754962612.22400 to 44456154431.48800, saving model to best_model.h5\n",
      "Epoch 79/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 39703157855.5733 - val_loss: 44172761563.1360\n",
      "\n",
      "Epoch 00079: val_loss improved from 44456154431.48800 to 44172761563.13600, saving model to best_model.h5\n",
      "Epoch 80/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 39387283319.4667 - val_loss: 43907886809.0880\n",
      "\n",
      "Epoch 00080: val_loss improved from 44172761563.13600 to 43907886809.08800, saving model to best_model.h5\n",
      "Epoch 81/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 39083220001.6782 - val_loss: 43669057470.4640\n",
      "\n",
      "Epoch 00081: val_loss improved from 43907886809.08800 to 43669057470.46400, saving model to best_model.h5\n",
      "Epoch 82/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 38810774277.6889 - val_loss: 43458989686.7840\n",
      "\n",
      "Epoch 00082: val_loss improved from 43669057470.46400 to 43458989686.78400, saving model to best_model.h5\n",
      "Epoch 83/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 38558436786.1760 - val_loss: 43272881373.1840\n",
      "\n",
      "Epoch 00083: val_loss improved from 43458989686.78400 to 43272881373.18400, saving model to best_model.h5\n",
      "Epoch 84/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 38332917643.4916 - val_loss: 43102741364.7360\n",
      "\n",
      "Epoch 00084: val_loss improved from 43272881373.18400 to 43102741364.73600, saving model to best_model.h5\n",
      "Epoch 85/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 38134555435.9182 - val_loss: 42940679946.2400\n",
      "\n",
      "Epoch 00085: val_loss improved from 43102741364.73600 to 42940679946.24000, saving model to best_model.h5\n",
      "Epoch 86/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 37955865983.2036 - val_loss: 42818599026.6880\n",
      "\n",
      "Epoch 00086: val_loss improved from 42940679946.24000 to 42818599026.68800, saving model to best_model.h5\n",
      "Epoch 87/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 37772205965.3120 - val_loss: 42683706572.8000\n",
      "\n",
      "Epoch 00087: val_loss improved from 42818599026.68800 to 42683706572.80000, saving model to best_model.h5\n",
      "Epoch 88/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 37610136460.4018 - val_loss: 42555996372.9920\n",
      "\n",
      "Epoch 00088: val_loss improved from 42683706572.80000 to 42555996372.99200, saving model to best_model.h5\n",
      "Epoch 89/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 37465811721.3298 - val_loss: 42423765237.7600\n",
      "\n",
      "Epoch 00089: val_loss improved from 42555996372.99200 to 42423765237.76000, saving model to best_model.h5\n",
      "Epoch 90/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 37325928704.6827 - val_loss: 42354548867.0720\n",
      "\n",
      "Epoch 00090: val_loss improved from 42423765237.76000 to 42354548867.07200, saving model to best_model.h5\n",
      "Epoch 91/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 37198905689.8844 - val_loss: 42212640423.9360\n",
      "\n",
      "Epoch 00091: val_loss improved from 42354548867.07200 to 42212640423.93600, saving model to best_model.h5\n",
      "Epoch 92/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 37075291034.9653 - val_loss: 42115949166.5920\n",
      "\n",
      "Epoch 00092: val_loss improved from 42212640423.93600 to 42115949166.59200, saving model to best_model.h5\n",
      "Epoch 93/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 36960614470.9973 - val_loss: 42047881707.5200\n",
      "\n",
      "Epoch 00093: val_loss improved from 42115949166.59200 to 42047881707.52000, saving model to best_model.h5\n",
      "Epoch 94/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 36847697748.8782 - val_loss: 41958313590.7840\n",
      "\n",
      "Epoch 00094: val_loss improved from 42047881707.52000 to 41958313590.78400, saving model to best_model.h5\n",
      "Epoch 95/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 36746772348.9280 - val_loss: 41892034248.7040\n",
      "\n",
      "Epoch 00095: val_loss improved from 41958313590.78400 to 41892034248.70400, saving model to best_model.h5\n",
      "Epoch 96/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 36648524440.4622 - val_loss: 41777164386.3040\n",
      "\n",
      "Epoch 00096: val_loss improved from 41892034248.70400 to 41777164386.30400, saving model to best_model.h5\n",
      "Epoch 97/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 36547636146.6311 - val_loss: 41662185406.4640\n",
      "\n",
      "Epoch 00097: val_loss improved from 41777164386.30400 to 41662185406.46400, saving model to best_model.h5\n",
      "Epoch 98/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 36446482890.7520 - val_loss: 41581772079.1040\n",
      "\n",
      "Epoch 00098: val_loss improved from 41662185406.46400 to 41581772079.10400, saving model to best_model.h5\n",
      "Epoch 99/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 36346986167.4098 - val_loss: 41517011468.2880\n",
      "\n",
      "Epoch 00099: val_loss improved from 41581772079.10400 to 41517011468.28800, saving model to best_model.h5\n",
      "Epoch 100/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 36259087999.8862 - val_loss: 41427063930.8800\n",
      "\n",
      "Epoch 00100: val_loss improved from 41517011468.28800 to 41427063930.88000, saving model to best_model.h5\n",
      "Epoch 101/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 36178341631.3173 - val_loss: 41330681839.6160\n",
      "\n",
      "Epoch 00101: val_loss improved from 41427063930.88000 to 41330681839.61600, saving model to best_model.h5\n",
      "Epoch 102/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 36082635148.8569 - val_loss: 41224239513.6000\n",
      "\n",
      "Epoch 00102: val_loss improved from 41330681839.61600 to 41224239513.60000, saving model to best_model.h5\n",
      "Epoch 103/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 35997897573.2622 - val_loss: 41157933662.2080\n",
      "\n",
      "Epoch 00103: val_loss improved from 41224239513.60000 to 41157933662.20800, saving model to best_model.h5\n",
      "Epoch 104/400\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 35904001887.8009 - val_loss: 41071589883.9040\n",
      "\n",
      "Epoch 00104: val_loss improved from 41157933662.20800 to 41071589883.90400, saving model to best_model.h5\n",
      "Epoch 105/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 35819676627.3991 - val_loss: 40975216181.2480\n",
      "\n",
      "Epoch 00105: val_loss improved from 41071589883.90400 to 40975216181.24800, saving model to best_model.h5\n",
      "Epoch 106/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 49us/step - loss: 35734134747.5911 - val_loss: 40943851732.9920\n",
      "\n",
      "Epoch 00106: val_loss improved from 40975216181.24800 to 40943851732.99200, saving model to best_model.h5\n",
      "Epoch 107/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 35662241264.0711 - val_loss: 40829336387.5840\n",
      "\n",
      "Epoch 00107: val_loss improved from 40943851732.99200 to 40829336387.58400, saving model to best_model.h5\n",
      "Epoch 108/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 35598459100.2738 - val_loss: 40757999697.9200\n",
      "\n",
      "Epoch 00108: val_loss improved from 40829336387.58400 to 40757999697.92000, saving model to best_model.h5\n",
      "Epoch 109/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 35503797390.9049 - val_loss: 40652122750.9760\n",
      "\n",
      "Epoch 00109: val_loss improved from 40757999697.92000 to 40652122750.97600, saving model to best_model.h5\n",
      "Epoch 110/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 35426803278.7342 - val_loss: 40556733726.7200\n",
      "\n",
      "Epoch 00110: val_loss improved from 40652122750.97600 to 40556733726.72000, saving model to best_model.h5\n",
      "Epoch 111/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 35356519759.8720 - val_loss: 40471838654.4640\n",
      "\n",
      "Epoch 00111: val_loss improved from 40556733726.72000 to 40471838654.46400, saving model to best_model.h5\n",
      "Epoch 112/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 35281241465.7422 - val_loss: 40392378253.3120\n",
      "\n",
      "Epoch 00112: val_loss improved from 40471838654.46400 to 40392378253.31200, saving model to best_model.h5\n",
      "Epoch 113/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 35210804501.6178 - val_loss: 40340045004.8000\n",
      "\n",
      "Epoch 00113: val_loss improved from 40392378253.31200 to 40340045004.80000, saving model to best_model.h5\n",
      "Epoch 114/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 35140929020.8142 - val_loss: 40258438103.0400\n",
      "\n",
      "Epoch 00114: val_loss improved from 40340045004.80000 to 40258438103.04000, saving model to best_model.h5\n",
      "Epoch 115/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 35077332802.6738 - val_loss: 40180619378.6880\n",
      "\n",
      "Epoch 00115: val_loss improved from 40258438103.04000 to 40180619378.68800, saving model to best_model.h5\n",
      "Epoch 116/400\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 35005742954.7236 - val_loss: 40121166626.8160\n",
      "\n",
      "Epoch 00116: val_loss improved from 40180619378.68800 to 40121166626.81600, saving model to best_model.h5\n",
      "Epoch 117/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 34944534608.0996 - val_loss: 40068653776.8960\n",
      "\n",
      "Epoch 00117: val_loss improved from 40121166626.81600 to 40068653776.89600, saving model to best_model.h5\n",
      "Epoch 118/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 34878449454.6489 - val_loss: 40017548607.4880\n",
      "\n",
      "Epoch 00118: val_loss improved from 40068653776.89600 to 40017548607.48800, saving model to best_model.h5\n",
      "Epoch 119/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 34823080516.7218 - val_loss: 39914759979.0080\n",
      "\n",
      "Epoch 00119: val_loss improved from 40017548607.48800 to 39914759979.00800, saving model to best_model.h5\n",
      "Epoch 120/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 34754789215.8009 - val_loss: 39857587814.4000\n",
      "\n",
      "Epoch 00120: val_loss improved from 39914759979.00800 to 39857587814.40000, saving model to best_model.h5\n",
      "Epoch 121/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 34694084513.3369 - val_loss: 39801243566.0800\n",
      "\n",
      "Epoch 00121: val_loss improved from 39857587814.40000 to 39801243566.08000, saving model to best_model.h5\n",
      "Epoch 122/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 34627529823.5733 - val_loss: 39761618239.4880\n",
      "\n",
      "Epoch 00122: val_loss improved from 39801243566.08000 to 39761618239.48800, saving model to best_model.h5\n",
      "Epoch 123/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 34572439009.5076 - val_loss: 39677768040.4480\n",
      "\n",
      "Epoch 00123: val_loss improved from 39761618239.48800 to 39677768040.44800, saving model to best_model.h5\n",
      "Epoch 124/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 34525818759.8507 - val_loss: 39604220788.7360\n",
      "\n",
      "Epoch 00124: val_loss improved from 39677768040.44800 to 39604220788.73600, saving model to best_model.h5\n",
      "Epoch 125/400\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 34456151676.2453 - val_loss: 39527747256.3200\n",
      "\n",
      "Epoch 00125: val_loss improved from 39604220788.73600 to 39527747256.32000, saving model to best_model.h5\n",
      "Epoch 126/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 34412487147.5200 - val_loss: 39495456587.7760\n",
      "\n",
      "Epoch 00126: val_loss improved from 39527747256.32000 to 39495456587.77600, saving model to best_model.h5\n",
      "Epoch 127/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 34360288342.4711 - val_loss: 39408326049.7920\n",
      "\n",
      "Epoch 00127: val_loss improved from 39495456587.77600 to 39408326049.79200, saving model to best_model.h5\n",
      "Epoch 128/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 34299272559.7298 - val_loss: 39382138880.0000\n",
      "\n",
      "Epoch 00128: val_loss improved from 39408326049.79200 to 39382138880.00000, saving model to best_model.h5\n",
      "Epoch 129/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 34249322011.7618 - val_loss: 39304483110.9120\n",
      "\n",
      "Epoch 00129: val_loss improved from 39382138880.00000 to 39304483110.91200, saving model to best_model.h5\n",
      "Epoch 130/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 34207225879.6658 - val_loss: 39256446763.0080\n",
      "\n",
      "Epoch 00130: val_loss improved from 39304483110.91200 to 39256446763.00800, saving model to best_model.h5\n",
      "Epoch 131/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 34147421315.9822 - val_loss: 39216078454.7840\n",
      "\n",
      "Epoch 00131: val_loss improved from 39256446763.00800 to 39216078454.78400, saving model to best_model.h5\n",
      "Epoch 132/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 34096112285.9236 - val_loss: 39169674608.6400\n",
      "\n",
      "Epoch 00132: val_loss improved from 39216078454.78400 to 39169674608.64000, saving model to best_model.h5\n",
      "Epoch 133/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 34042727195.5342 - val_loss: 39110353518.5920\n",
      "\n",
      "Epoch 00133: val_loss improved from 39169674608.64000 to 39110353518.59200, saving model to best_model.h5\n",
      "Epoch 134/400\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 34000066867.6551 - val_loss: 39095101784.0640\n",
      "\n",
      "Epoch 00134: val_loss improved from 39110353518.59200 to 39095101784.06400, saving model to best_model.h5\n",
      "Epoch 135/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 33945684069.9449 - val_loss: 39013146099.7120\n",
      "\n",
      "Epoch 00135: val_loss improved from 39095101784.06400 to 39013146099.71200, saving model to best_model.h5\n",
      "Epoch 136/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 33900133646.3360 - val_loss: 38977756069.8880\n",
      "\n",
      "Epoch 00136: val_loss improved from 39013146099.71200 to 38977756069.88800, saving model to best_model.h5\n",
      "Epoch 137/400\n",
      "18000/18000 [==============================] - 1s 37us/step - loss: 33863613846.8693 - val_loss: 38907818999.8080\n",
      "\n",
      "Epoch 00137: val_loss improved from 38977756069.88800 to 38907818999.80800, saving model to best_model.h5\n",
      "Epoch 138/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 33816919667.1431 - val_loss: 38869480570.8800\n",
      "\n",
      "Epoch 00138: val_loss improved from 38907818999.80800 to 38869480570.88000, saving model to best_model.h5\n",
      "Epoch 139/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 33769286493.0702 - val_loss: 38835417055.2320\n",
      "\n",
      "Epoch 00139: val_loss improved from 38869480570.88000 to 38835417055.23200, saving model to best_model.h5\n",
      "Epoch 140/400\n",
      "18000/18000 [==============================] - 1s 38us/step - loss: 33736138264.1209 - val_loss: 38772498497.5360\n",
      "\n",
      "Epoch 00140: val_loss improved from 38835417055.23200 to 38772498497.53600, saving model to best_model.h5\n",
      "Epoch 141/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 48us/step - loss: 33696992580.9493 - val_loss: 38764493045.7600\n",
      "\n",
      "Epoch 00141: val_loss improved from 38772498497.53600 to 38764493045.76000, saving model to best_model.h5\n",
      "Epoch 142/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 33650815842.5316 - val_loss: 38692661329.9200\n",
      "\n",
      "Epoch 00142: val_loss improved from 38764493045.76000 to 38692661329.92000, saving model to best_model.h5\n",
      "Epoch 143/400\n",
      "18000/18000 [==============================] - 1s 61us/step - loss: 33599817602.3893 - val_loss: 38661002297.3440\n",
      "\n",
      "Epoch 00143: val_loss improved from 38692661329.92000 to 38661002297.34400, saving model to best_model.h5\n",
      "Epoch 144/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 33556876561.9769 - val_loss: 38601671704.5760\n",
      "\n",
      "Epoch 00144: val_loss improved from 38661002297.34400 to 38601671704.57600, saving model to best_model.h5\n",
      "Epoch 145/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 33527061834.4107 - val_loss: 38563541614.5920\n",
      "\n",
      "Epoch 00145: val_loss improved from 38601671704.57600 to 38563541614.59200, saving model to best_model.h5\n",
      "Epoch 146/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 33482383301.7458 - val_loss: 38534326190.0800\n",
      "\n",
      "Epoch 00146: val_loss improved from 38563541614.59200 to 38534326190.08000, saving model to best_model.h5\n",
      "Epoch 147/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 33450574816.1422 - val_loss: 38526453383.1680\n",
      "\n",
      "Epoch 00147: val_loss improved from 38534326190.08000 to 38526453383.16800, saving model to best_model.h5\n",
      "Epoch 148/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 33417307979.7760 - val_loss: 38463937380.3520\n",
      "\n",
      "Epoch 00148: val_loss improved from 38526453383.16800 to 38463937380.35200, saving model to best_model.h5\n",
      "Epoch 149/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 33383097546.9796 - val_loss: 38405969543.1680\n",
      "\n",
      "Epoch 00149: val_loss improved from 38463937380.35200 to 38405969543.16800, saving model to best_model.h5\n",
      "Epoch 150/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 33343809164.6293 - val_loss: 38398706647.0400\n",
      "\n",
      "Epoch 00150: val_loss improved from 38405969543.16800 to 38398706647.04000, saving model to best_model.h5\n",
      "Epoch 151/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 33314583004.9564 - val_loss: 38365830545.4080\n",
      "\n",
      "Epoch 00151: val_loss improved from 38398706647.04000 to 38365830545.40800, saving model to best_model.h5\n",
      "Epoch 152/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 33269216420.7502 - val_loss: 38295932043.2640\n",
      "\n",
      "Epoch 00152: val_loss improved from 38365830545.40800 to 38295932043.26400, saving model to best_model.h5\n",
      "Epoch 153/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 33237643158.4142 - val_loss: 38252653740.0320\n",
      "\n",
      "Epoch 00153: val_loss improved from 38295932043.26400 to 38252653740.03200, saving model to best_model.h5\n",
      "Epoch 154/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 33207404207.2178 - val_loss: 38242748104.7040\n",
      "\n",
      "Epoch 00154: val_loss improved from 38252653740.03200 to 38242748104.70400, saving model to best_model.h5\n",
      "Epoch 155/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 33181476101.2338 - val_loss: 38207427149.8240\n",
      "\n",
      "Epoch 00155: val_loss improved from 38242748104.70400 to 38207427149.82400, saving model to best_model.h5\n",
      "Epoch 156/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 33148104620.2596 - val_loss: 38219259052.0320\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 38207427149.82400\n",
      "Epoch 157/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 33117534981.6889 - val_loss: 38169959399.4240\n",
      "\n",
      "Epoch 00157: val_loss improved from 38207427149.82400 to 38169959399.42400, saving model to best_model.h5\n",
      "Epoch 158/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 33083978980.4658 - val_loss: 38128672538.6240\n",
      "\n",
      "Epoch 00158: val_loss improved from 38169959399.42400 to 38128672538.62400, saving model to best_model.h5\n",
      "Epoch 159/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 33055623803.3351 - val_loss: 38104472518.6560\n",
      "\n",
      "Epoch 00159: val_loss improved from 38128672538.62400 to 38104472518.65600, saving model to best_model.h5\n",
      "Epoch 160/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 33030845819.5627 - val_loss: 38050051588.0960\n",
      "\n",
      "Epoch 00160: val_loss improved from 38104472518.65600 to 38050051588.09600, saving model to best_model.h5\n",
      "Epoch 161/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 33003038649.0027 - val_loss: 38024709996.5440\n",
      "\n",
      "Epoch 00161: val_loss improved from 38050051588.09600 to 38024709996.54400, saving model to best_model.h5\n",
      "Epoch 162/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 32978092068.4089 - val_loss: 37985452359.6800\n",
      "\n",
      "Epoch 00162: val_loss improved from 38024709996.54400 to 37985452359.68000, saving model to best_model.h5\n",
      "Epoch 163/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 32952904169.6996 - val_loss: 37950014291.9680\n",
      "\n",
      "Epoch 00163: val_loss improved from 37985452359.68000 to 37950014291.96800, saving model to best_model.h5\n",
      "Epoch 164/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 32922112342.2436 - val_loss: 37944827445.2480\n",
      "\n",
      "Epoch 00164: val_loss improved from 37950014291.96800 to 37944827445.24800, saving model to best_model.h5\n",
      "Epoch 165/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 32901939822.5920 - val_loss: 37927616806.9120\n",
      "\n",
      "Epoch 00165: val_loss improved from 37944827445.24800 to 37927616806.91200, saving model to best_model.h5\n",
      "Epoch 166/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 32874172237.5964 - val_loss: 37875793133.5680\n",
      "\n",
      "Epoch 00166: val_loss improved from 37927616806.91200 to 37875793133.56800, saving model to best_model.h5\n",
      "Epoch 167/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 32855672127.4880 - val_loss: 37850992934.9120\n",
      "\n",
      "Epoch 00167: val_loss improved from 37875793133.56800 to 37850992934.91200, saving model to best_model.h5\n",
      "Epoch 168/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 32833501797.4898 - val_loss: 37854491049.9840\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 37850992934.91200\n",
      "Epoch 169/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 32802107472.0996 - val_loss: 37792669990.9120\n",
      "\n",
      "Epoch 00169: val_loss improved from 37850992934.91200 to 37792669990.91200, saving model to best_model.h5\n",
      "Epoch 170/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 32793616982.9262 - val_loss: 37775277621.2480\n",
      "\n",
      "Epoch 00170: val_loss improved from 37792669990.91200 to 37775277621.24800, saving model to best_model.h5\n",
      "Epoch 171/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 32764825609.1022 - val_loss: 37755452555.2640\n",
      "\n",
      "Epoch 00171: val_loss improved from 37775277621.24800 to 37755452555.26400, saving model to best_model.h5\n",
      "Epoch 172/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 32735820910.1369 - val_loss: 37756339093.5040\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 37755452555.26400\n",
      "Epoch 173/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 32725035748.9209 - val_loss: 37703976288.2560\n",
      "\n",
      "Epoch 00173: val_loss improved from 37755452555.26400 to 37703976288.25600, saving model to best_model.h5\n",
      "Epoch 174/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 32696046611.1147 - val_loss: 37715366707.2000\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 37703976288.25600\n",
      "Epoch 175/400\n",
      "18000/18000 [==============================] - 1s 38us/step - loss: 32676534059.0080 - val_loss: 37698144731.1360\n",
      "\n",
      "Epoch 00175: val_loss improved from 37703976288.25600 to 37698144731.13600, saving model to best_model.h5\n",
      "Epoch 176/400\n",
      "18000/18000 [==============================] - 1s 39us/step - loss: 32661015862.3858 - val_loss: 37658896662.5280\n",
      "\n",
      "Epoch 00176: val_loss improved from 37698144731.13600 to 37658896662.52800, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 32635938470.1156 - val_loss: 37633564672.0000\n",
      "\n",
      "Epoch 00177: val_loss improved from 37658896662.52800 to 37633564672.00000, saving model to best_model.h5\n",
      "Epoch 178/400\n",
      "18000/18000 [==============================] - 1s 61us/step - loss: 32617037973.2764 - val_loss: 37613019136.0000\n",
      "\n",
      "Epoch 00178: val_loss improved from 37633564672.00000 to 37613019136.00000, saving model to best_model.h5\n",
      "Epoch 179/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 32604205929.8133 - val_loss: 37593009618.9440\n",
      "\n",
      "Epoch 00179: val_loss improved from 37613019136.00000 to 37593009618.94400, saving model to best_model.h5\n",
      "Epoch 180/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 32580118923.0364 - val_loss: 37583775039.4880\n",
      "\n",
      "Epoch 00180: val_loss improved from 37593009618.94400 to 37583775039.48800, saving model to best_model.h5\n",
      "Epoch 181/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 32562514065.6356 - val_loss: 37531538358.2720\n",
      "\n",
      "Epoch 00181: val_loss improved from 37583775039.48800 to 37531538358.27200, saving model to best_model.h5\n",
      "Epoch 182/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 32539511848.9600 - val_loss: 37535664275.4560\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 37531538358.27200\n",
      "Epoch 183/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 32516917669.4329 - val_loss: 37530230816.7680\n",
      "\n",
      "Epoch 00183: val_loss improved from 37531538358.27200 to 37530230816.76800, saving model to best_model.h5\n",
      "Epoch 184/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 32516181002.0124 - val_loss: 37478621970.4320\n",
      "\n",
      "Epoch 00184: val_loss improved from 37530230816.76800 to 37478621970.43200, saving model to best_model.h5\n",
      "Epoch 185/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 32489994611.3707 - val_loss: 37462110240.7680\n",
      "\n",
      "Epoch 00185: val_loss improved from 37478621970.43200 to 37462110240.76800, saving model to best_model.h5\n",
      "Epoch 186/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 32473291463.7938 - val_loss: 37428731379.7120\n",
      "\n",
      "Epoch 00186: val_loss improved from 37462110240.76800 to 37428731379.71200, saving model to best_model.h5\n",
      "Epoch 187/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 32459515437.9662 - val_loss: 37460495073.2800\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 37428731379.71200\n",
      "Epoch 188/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 32428426166.2720 - val_loss: 37445266735.1040\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 37428731379.71200\n",
      "Epoch 189/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 32415488225.7351 - val_loss: 37419267293.1840\n",
      "\n",
      "Epoch 00189: val_loss improved from 37428731379.71200 to 37419267293.18400, saving model to best_model.h5\n",
      "Epoch 190/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 32401736013.1413 - val_loss: 37395112722.4320\n",
      "\n",
      "Epoch 00190: val_loss improved from 37419267293.18400 to 37395112722.43200, saving model to best_model.h5\n",
      "Epoch 191/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 32382055048.0782 - val_loss: 37396381433.8560\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 37395112722.43200\n",
      "Epoch 192/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 32367028943.0756 - val_loss: 37372049457.1520\n",
      "\n",
      "Epoch 00192: val_loss improved from 37395112722.43200 to 37372049457.15200, saving model to best_model.h5\n",
      "Epoch 193/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 32348244435.8542 - val_loss: 37340409069.5680\n",
      "\n",
      "Epoch 00193: val_loss improved from 37372049457.15200 to 37340409069.56800, saving model to best_model.h5\n",
      "Epoch 194/400\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 32334587252.2809 - val_loss: 37352293957.6320\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 37340409069.56800\n",
      "Epoch 195/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 32309357840.1564 - val_loss: 37316448616.4480\n",
      "\n",
      "Epoch 00195: val_loss improved from 37340409069.56800 to 37316448616.44800, saving model to best_model.h5\n",
      "Epoch 196/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 32297495629.3689 - val_loss: 37289642000.3840\n",
      "\n",
      "Epoch 00196: val_loss improved from 37316448616.44800 to 37289642000.38400, saving model to best_model.h5\n",
      "Epoch 197/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 32276998524.4729 - val_loss: 37288914550.7840\n",
      "\n",
      "Epoch 00197: val_loss improved from 37289642000.38400 to 37288914550.78400, saving model to best_model.h5\n",
      "Epoch 198/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 32264187106.6453 - val_loss: 37284457676.8000\n",
      "\n",
      "Epoch 00198: val_loss improved from 37288914550.78400 to 37284457676.80000, saving model to best_model.h5\n",
      "Epoch 199/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 32245145574.5138 - val_loss: 37217404649.4720\n",
      "\n",
      "Epoch 00199: val_loss improved from 37284457676.80000 to 37217404649.47200, saving model to best_model.h5\n",
      "Epoch 200/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 32229146667.6907 - val_loss: 37223668711.4240\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 37217404649.47200\n",
      "Epoch 201/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 32218305704.3911 - val_loss: 37222694715.3920\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 37217404649.47200\n",
      "Epoch 202/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 32197559502.6204 - val_loss: 37184812810.2400\n",
      "\n",
      "Epoch 00202: val_loss improved from 37217404649.47200 to 37184812810.24000, saving model to best_model.h5\n",
      "Epoch 203/400\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 32177727078.4000 - val_loss: 37198403403.7760\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 37184812810.24000\n",
      "Epoch 204/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 32168662237.1840 - val_loss: 37165747732.4800\n",
      "\n",
      "Epoch 00204: val_loss improved from 37184812810.24000 to 37165747732.48000, saving model to best_model.h5\n",
      "Epoch 205/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 32151417207.0116 - val_loss: 37141246738.4320\n",
      "\n",
      "Epoch 00205: val_loss improved from 37165747732.48000 to 37141246738.43200, saving model to best_model.h5\n",
      "Epoch 206/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 32129678367.8578 - val_loss: 37141981790.2080\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 37141246738.43200\n",
      "Epoch 207/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 32116178041.0596 - val_loss: 37134686453.7600\n",
      "\n",
      "Epoch 00207: val_loss improved from 37141246738.43200 to 37134686453.76000, saving model to best_model.h5\n",
      "Epoch 208/400\n",
      "18000/18000 [==============================] - 1s 64us/step - loss: 32096660937.8418 - val_loss: 37101655097.3440\n",
      "\n",
      "Epoch 00208: val_loss improved from 37134686453.76000 to 37101655097.34400, saving model to best_model.h5\n",
      "Epoch 209/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 32091785536.3982 - val_loss: 37086656135.1680\n",
      "\n",
      "Epoch 00209: val_loss improved from 37101655097.34400 to 37086656135.16800, saving model to best_model.h5\n",
      "Epoch 210/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 32068059618.4178 - val_loss: 37059672178.6880\n",
      "\n",
      "Epoch 00210: val_loss improved from 37086656135.16800 to 37059672178.68800, saving model to best_model.h5\n",
      "Epoch 211/400\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 32055697000.2204 - val_loss: 37083339522.0480\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 37059672178.68800\n",
      "Epoch 212/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 32046330178.2187 - val_loss: 37048108417.0240\n",
      "\n",
      "Epoch 00212: val_loss improved from 37059672178.68800 to 37048108417.02400, saving model to best_model.h5\n",
      "Epoch 213/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 32022041417.0453 - val_loss: 37044407304.1920\n",
      "\n",
      "Epoch 00213: val_loss improved from 37048108417.02400 to 37044407304.19200, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 32003748646.4569 - val_loss: 37021387816.9600\n",
      "\n",
      "Epoch 00214: val_loss improved from 37044407304.19200 to 37021387816.96000, saving model to best_model.h5\n",
      "Epoch 215/400\n",
      "18000/18000 [==============================] - 1s 75us/step - loss: 31999111532.0889 - val_loss: 37007264350.2080\n",
      "\n",
      "Epoch 00215: val_loss improved from 37021387816.96000 to 37007264350.20800, saving model to best_model.h5\n",
      "Epoch 216/400\n",
      "18000/18000 [==============================] - 2s 84us/step - loss: 31977107976.6471 - val_loss: 36994526281.7280\n",
      "\n",
      "Epoch 00216: val_loss improved from 37007264350.20800 to 36994526281.72800, saving model to best_model.h5\n",
      "Epoch 217/400\n",
      "18000/18000 [==============================] - 1s 78us/step - loss: 31959212053.8453 - val_loss: 36972025118.7200\n",
      "\n",
      "Epoch 00217: val_loss improved from 36994526281.72800 to 36972025118.72000, saving model to best_model.h5\n",
      "Epoch 218/400\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 31954721705.5289 - val_loss: 36935423000.5760\n",
      "\n",
      "Epoch 00218: val_loss improved from 36972025118.72000 to 36935423000.57600, saving model to best_model.h5\n",
      "Epoch 219/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 31950323566.3644 - val_loss: 36941218086.9120\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 36935423000.57600\n",
      "Epoch 220/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 31939434417.7209 - val_loss: 36939945443.3280\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 36935423000.57600\n",
      "Epoch 221/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 31906310677.3902 - val_loss: 36920803098.6240\n",
      "\n",
      "Epoch 00221: val_loss improved from 36935423000.57600 to 36920803098.62400, saving model to best_model.h5\n",
      "Epoch 222/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 31881853155.5556 - val_loss: 36892125331.4560\n",
      "\n",
      "Epoch 00222: val_loss improved from 36920803098.62400 to 36892125331.45600, saving model to best_model.h5\n",
      "Epoch 223/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 31880552184.0356 - val_loss: 36838996180.9920\n",
      "\n",
      "Epoch 00223: val_loss improved from 36892125331.45600 to 36838996180.99200, saving model to best_model.h5\n",
      "Epoch 224/400\n",
      "18000/18000 [==============================] - 1s 60us/step - loss: 31862093484.4871 - val_loss: 36860079603.7120\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 36838996180.99200\n",
      "Epoch 225/400\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 31843552403.4560 - val_loss: 36859384856.5760\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 36838996180.99200\n",
      "Epoch 226/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 31824666460.1600 - val_loss: 36862026219.5200\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 36838996180.99200\n",
      "Epoch 227/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 31816235497.6996 - val_loss: 36823217733.6320\n",
      "\n",
      "Epoch 00227: val_loss improved from 36838996180.99200 to 36823217733.63200, saving model to best_model.h5\n",
      "Epoch 228/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 31806031262.1511 - val_loss: 36850191958.0160\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 36823217733.63200\n",
      "Epoch 229/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 31787117929.3582 - val_loss: 36816547053.5680\n",
      "\n",
      "Epoch 00229: val_loss improved from 36823217733.63200 to 36816547053.56800, saving model to best_model.h5\n",
      "Epoch 230/400\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 31766114152.9031 - val_loss: 36814727610.3680\n",
      "\n",
      "Epoch 00230: val_loss improved from 36816547053.56800 to 36814727610.36800, saving model to best_model.h5\n",
      "Epoch 231/400\n",
      "18000/18000 [==============================] - 1s 64us/step - loss: 31757652838.1724 - val_loss: 36765158473.7280\n",
      "\n",
      "Epoch 00231: val_loss improved from 36814727610.36800 to 36765158473.72800, saving model to best_model.h5\n",
      "Epoch 232/400\n",
      "18000/18000 [==============================] - 1s 59us/step - loss: 31740605198.7911 - val_loss: 36747704860.6720\n",
      "\n",
      "Epoch 00232: val_loss improved from 36765158473.72800 to 36747704860.67200, saving model to best_model.h5\n",
      "Epoch 233/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 31731440579.0151 - val_loss: 36745336356.8640\n",
      "\n",
      "Epoch 00233: val_loss improved from 36747704860.67200 to 36745336356.86400, saving model to best_model.h5\n",
      "Epoch 234/400\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 31713387362.5316 - val_loss: 36743014580.2240\n",
      "\n",
      "Epoch 00234: val_loss improved from 36745336356.86400 to 36743014580.22400, saving model to best_model.h5\n",
      "Epoch 235/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 31699588863.3173 - val_loss: 36732203204.6080\n",
      "\n",
      "Epoch 00235: val_loss improved from 36743014580.22400 to 36732203204.60800, saving model to best_model.h5\n",
      "Epoch 236/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 31678410482.5742 - val_loss: 36681068740.6080\n",
      "\n",
      "Epoch 00236: val_loss improved from 36732203204.60800 to 36681068740.60800, saving model to best_model.h5\n",
      "Epoch 237/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 31670218509.8809 - val_loss: 36676168122.3680\n",
      "\n",
      "Epoch 00237: val_loss improved from 36681068740.60800 to 36676168122.36800, saving model to best_model.h5\n",
      "Epoch 238/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 31655184072.7040 - val_loss: 36661178466.3040\n",
      "\n",
      "Epoch 00238: val_loss improved from 36676168122.36800 to 36661178466.30400, saving model to best_model.h5\n",
      "Epoch 239/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 31631932710.0018 - val_loss: 36656254484.4800\n",
      "\n",
      "Epoch 00239: val_loss improved from 36661178466.30400 to 36656254484.48000, saving model to best_model.h5\n",
      "Epoch 240/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 31626633656.5476 - val_loss: 36629744254.9760\n",
      "\n",
      "Epoch 00240: val_loss improved from 36656254484.48000 to 36629744254.97600, saving model to best_model.h5\n",
      "Epoch 241/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 31605153353.2729 - val_loss: 36621814005.7600\n",
      "\n",
      "Epoch 00241: val_loss improved from 36629744254.97600 to 36621814005.76000, saving model to best_model.h5\n",
      "Epoch 242/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 31605823591.7653 - val_loss: 36633182863.3600\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 36621814005.76000\n",
      "Epoch 243/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 31590333273.4293 - val_loss: 36614702694.4000\n",
      "\n",
      "Epoch 00243: val_loss improved from 36621814005.76000 to 36614702694.40000, saving model to best_model.h5\n",
      "Epoch 244/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 31571505446.9120 - val_loss: 36618101948.4160\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 36614702694.40000\n",
      "Epoch 245/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 31558577040.0427 - val_loss: 36592116924.4160\n",
      "\n",
      "Epoch 00245: val_loss improved from 36614702694.40000 to 36592116924.41600, saving model to best_model.h5\n",
      "Epoch 246/400\n",
      "18000/18000 [==============================] - 1s 39us/step - loss: 31544750859.1502 - val_loss: 36586302242.8160\n",
      "\n",
      "Epoch 00246: val_loss improved from 36592116924.41600 to 36586302242.81600, saving model to best_model.h5\n",
      "Epoch 247/400\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 31525130673.2658 - val_loss: 36547147399.1680\n",
      "\n",
      "Epoch 00247: val_loss improved from 36586302242.81600 to 36547147399.16800, saving model to best_model.h5\n",
      "Epoch 248/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 31512735372.6293 - val_loss: 36526568865.7920\n",
      "\n",
      "Epoch 00248: val_loss improved from 36547147399.16800 to 36526568865.79200, saving model to best_model.h5\n",
      "Epoch 249/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 31500005751.0116 - val_loss: 36523007213.5680\n",
      "\n",
      "Epoch 00249: val_loss improved from 36526568865.79200 to 36523007213.56800, saving model to best_model.h5\n",
      "Epoch 250/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 48us/step - loss: 31489480582.0302 - val_loss: 36488400338.9440\n",
      "\n",
      "Epoch 00250: val_loss improved from 36523007213.56800 to 36488400338.94400, saving model to best_model.h5\n",
      "Epoch 251/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 31462804297.0453 - val_loss: 36496707354.6240\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 36488400338.94400\n",
      "Epoch 252/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 31456202369.7067 - val_loss: 36490625253.3760\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 36488400338.94400\n",
      "Epoch 253/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 31436065061.0916 - val_loss: 36462682800.1280\n",
      "\n",
      "Epoch 00253: val_loss improved from 36488400338.94400 to 36462682800.12800, saving model to best_model.h5\n",
      "Epoch 254/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 31425151979.0649 - val_loss: 36441182273.5360\n",
      "\n",
      "Epoch 00254: val_loss improved from 36462682800.12800 to 36441182273.53600, saving model to best_model.h5\n",
      "Epoch 255/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 31407648087.1538 - val_loss: 36433441849.3440\n",
      "\n",
      "Epoch 00255: val_loss improved from 36441182273.53600 to 36433441849.34400, saving model to best_model.h5\n",
      "Epoch 256/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 31400567377.4649 - val_loss: 36424686927.8720\n",
      "\n",
      "Epoch 00256: val_loss improved from 36433441849.34400 to 36424686927.87200, saving model to best_model.h5\n",
      "Epoch 257/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 31386028430.6773 - val_loss: 36445117612.0320\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 36424686927.87200\n",
      "Epoch 258/400\n",
      "18000/18000 [==============================] - 1s 66us/step - loss: 31366815514.6240 - val_loss: 36394383769.6000\n",
      "\n",
      "Epoch 00258: val_loss improved from 36424686927.87200 to 36394383769.60000, saving model to best_model.h5\n",
      "Epoch 259/400\n",
      "18000/18000 [==============================] - 1s 74us/step - loss: 31379215322.6809 - val_loss: 36365538295.8080\n",
      "\n",
      "Epoch 00259: val_loss improved from 36394383769.60000 to 36365538295.80800, saving model to best_model.h5\n",
      "Epoch 260/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 31340146032.6400 - val_loss: 36365515489.2800\n",
      "\n",
      "Epoch 00260: val_loss improved from 36365538295.80800 to 36365515489.28000, saving model to best_model.h5\n",
      "Epoch 261/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 31331032241.4933 - val_loss: 36358770458.6240\n",
      "\n",
      "Epoch 00261: val_loss improved from 36365515489.28000 to 36358770458.62400, saving model to best_model.h5\n",
      "Epoch 262/400\n",
      "18000/18000 [==============================] - 1s 59us/step - loss: 31317652830.4356 - val_loss: 36330879451.1360\n",
      "\n",
      "Epoch 00262: val_loss improved from 36358770458.62400 to 36330879451.13600, saving model to best_model.h5\n",
      "Epoch 263/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 31296119976.3911 - val_loss: 36335805988.8640\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 36330879451.13600\n",
      "Epoch 264/400\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 31283491154.6027 - val_loss: 36322100805.6320\n",
      "\n",
      "Epoch 00264: val_loss improved from 36330879451.13600 to 36322100805.63200, saving model to best_model.h5\n",
      "Epoch 265/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 31282344416.5973 - val_loss: 36300193202.1760\n",
      "\n",
      "Epoch 00265: val_loss improved from 36322100805.63200 to 36300193202.17600, saving model to best_model.h5\n",
      "Epoch 266/400\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 31266257429.3902 - val_loss: 36271672885.2480\n",
      "\n",
      "Epoch 00266: val_loss improved from 36300193202.17600 to 36271672885.24800, saving model to best_model.h5\n",
      "Epoch 267/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 31250019778.5600 - val_loss: 36256078069.7600\n",
      "\n",
      "Epoch 00267: val_loss improved from 36271672885.24800 to 36256078069.76000, saving model to best_model.h5\n",
      "Epoch 268/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 31232010440.2489 - val_loss: 36266413162.4960\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 36256078069.76000\n",
      "Epoch 269/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 31216689428.7076 - val_loss: 36263435632.6400\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 36256078069.76000\n",
      "Epoch 270/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 31206588874.7520 - val_loss: 36269499285.5040\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 36256078069.76000\n",
      "Epoch 271/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 31205506482.1760 - val_loss: 36213286240.2560\n",
      "\n",
      "Epoch 00271: val_loss improved from 36256078069.76000 to 36213286240.25600, saving model to best_model.h5\n",
      "Epoch 272/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 31182315215.9858 - val_loss: 36219400650.7520\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 36213286240.25600\n",
      "Epoch 273/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 31173912250.1404 - val_loss: 36206979973.1200\n",
      "\n",
      "Epoch 00273: val_loss improved from 36213286240.25600 to 36206979973.12000, saving model to best_model.h5\n",
      "Epoch 274/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 31166880006.1440 - val_loss: 36170306125.8240\n",
      "\n",
      "Epoch 00274: val_loss improved from 36206979973.12000 to 36170306125.82400, saving model to best_model.h5\n",
      "Epoch 275/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 31146362383.0187 - val_loss: 36195888988.1600\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 36170306125.82400\n",
      "Epoch 276/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 31142428375.2676 - val_loss: 36165664145.4080\n",
      "\n",
      "Epoch 00276: val_loss improved from 36170306125.82400 to 36165664145.40800, saving model to best_model.h5\n",
      "Epoch 277/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 31124615845.2053 - val_loss: 36169707782.1440\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 36165664145.40800\n",
      "Epoch 278/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 31100163336.8747 - val_loss: 36131608035.3280\n",
      "\n",
      "Epoch 00278: val_loss improved from 36165664145.40800 to 36131608035.32800, saving model to best_model.h5\n",
      "Epoch 279/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 31089171908.3804 - val_loss: 36140763807.7440\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 36131608035.32800\n",
      "Epoch 280/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 31079429992.9031 - val_loss: 36141871988.7360\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 36131608035.32800\n",
      "Epoch 281/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 31066960437.2480 - val_loss: 36105098133.5040\n",
      "\n",
      "Epoch 00281: val_loss improved from 36131608035.32800 to 36105098133.50400, saving model to best_model.h5\n",
      "Epoch 282/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 31057614793.3867 - val_loss: 36110745534.4640\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 36105098133.50400\n",
      "Epoch 283/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 31036260210.0053 - val_loss: 36085244133.3760\n",
      "\n",
      "Epoch 00283: val_loss improved from 36105098133.50400 to 36085244133.37600, saving model to best_model.h5\n",
      "Epoch 284/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 31028746780.6720 - val_loss: 36086799663.1040\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 36085244133.37600\n",
      "Epoch 285/400\n",
      "18000/18000 [==============================] - 1s 41us/step - loss: 31012319033.5716 - val_loss: 36074516742.1440\n",
      "\n",
      "Epoch 00285: val_loss improved from 36085244133.37600 to 36074516742.14400, saving model to best_model.h5\n",
      "Epoch 286/400\n",
      "18000/18000 [==============================] - 1s 39us/step - loss: 31011528225.2231 - val_loss: 36052159496.1920\n",
      "\n",
      "Epoch 00286: val_loss improved from 36074516742.14400 to 36052159496.19200, saving model to best_model.h5\n",
      "Epoch 287/400\n",
      "18000/18000 [==============================] - 1s 39us/step - loss: 31000685656.2916 - val_loss: 36031472205.8240\n",
      "\n",
      "Epoch 00287: val_loss improved from 36052159496.19200 to 36031472205.82400, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 288/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30988646835.0862 - val_loss: 36013876576.2560\n",
      "\n",
      "Epoch 00288: val_loss improved from 36031472205.82400 to 36013876576.25600, saving model to best_model.h5\n",
      "Epoch 289/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 30969067481.7707 - val_loss: 36004972625.9200\n",
      "\n",
      "Epoch 00289: val_loss improved from 36013876576.25600 to 36004972625.92000, saving model to best_model.h5\n",
      "Epoch 290/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 30956659987.7973 - val_loss: 36003181821.9520\n",
      "\n",
      "Epoch 00290: val_loss improved from 36004972625.92000 to 36003181821.95200, saving model to best_model.h5\n",
      "Epoch 291/400\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 30937490618.5956 - val_loss: 35994889388.0320\n",
      "\n",
      "Epoch 00291: val_loss improved from 36003181821.95200 to 35994889388.03200, saving model to best_model.h5\n",
      "Epoch 292/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 30939410779.7049 - val_loss: 36014300987.3920\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 35994889388.03200\n",
      "Epoch 293/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 30914949549.6249 - val_loss: 35962321829.8880\n",
      "\n",
      "Epoch 00293: val_loss improved from 35994889388.03200 to 35962321829.88800, saving model to best_model.h5\n",
      "Epoch 294/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 30901831532.5440 - val_loss: 35939181592.5760\n",
      "\n",
      "Epoch 00294: val_loss improved from 35962321829.88800 to 35939181592.57600, saving model to best_model.h5\n",
      "Epoch 295/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 30892168495.1040 - val_loss: 35954100535.2960\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 35939181592.57600\n",
      "Epoch 296/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 30879724493.9378 - val_loss: 35933691510.7840\n",
      "\n",
      "Epoch 00296: val_loss improved from 35939181592.57600 to 35933691510.78400, saving model to best_model.h5\n",
      "Epoch 297/400\n",
      "18000/18000 [==============================] - 1s 57us/step - loss: 30867505221.1769 - val_loss: 35907763404.8000\n",
      "\n",
      "Epoch 00297: val_loss improved from 35933691510.78400 to 35907763404.80000, saving model to best_model.h5\n",
      "Epoch 298/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 30850268208.2418 - val_loss: 35916411404.2880\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 35907763404.80000\n",
      "Epoch 299/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30840671953.8062 - val_loss: 35895010426.8800\n",
      "\n",
      "Epoch 00299: val_loss improved from 35907763404.80000 to 35895010426.88000, saving model to best_model.h5\n",
      "Epoch 300/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 30832298917.8880 - val_loss: 35905051951.1040\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 35895010426.88000\n",
      "Epoch 301/400\n",
      "18000/18000 [==============================] - 1s 62us/step - loss: 30819702614.6987 - val_loss: 35861322596.3520\n",
      "\n",
      "Epoch 00301: val_loss improved from 35895010426.88000 to 35861322596.35200, saving model to best_model.h5\n",
      "Epoch 302/400\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 30809013610.2684 - val_loss: 35848004370.4320\n",
      "\n",
      "Epoch 00302: val_loss improved from 35861322596.35200 to 35848004370.43200, saving model to best_model.h5\n",
      "Epoch 303/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 30805141001.5573 - val_loss: 35830478733.3120\n",
      "\n",
      "Epoch 00303: val_loss improved from 35848004370.43200 to 35830478733.31200, saving model to best_model.h5\n",
      "Epoch 304/400\n",
      "18000/18000 [==============================] - 1s 58us/step - loss: 30785445412.8640 - val_loss: 35806505500.6720\n",
      "\n",
      "Epoch 00304: val_loss improved from 35830478733.31200 to 35806505500.67200, saving model to best_model.h5\n",
      "Epoch 305/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 30772118749.1840 - val_loss: 35830345433.0880\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 35806505500.67200\n",
      "Epoch 306/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 30762816565.7031 - val_loss: 35802365263.8720\n",
      "\n",
      "Epoch 00306: val_loss improved from 35806505500.67200 to 35802365263.87200, saving model to best_model.h5\n",
      "Epoch 307/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30748818644.0818 - val_loss: 35788917702.6560\n",
      "\n",
      "Epoch 00307: val_loss improved from 35802365263.87200 to 35788917702.65600, saving model to best_model.h5\n",
      "Epoch 308/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 30740448156.7858 - val_loss: 35777588166.6560\n",
      "\n",
      "Epoch 00308: val_loss improved from 35788917702.65600 to 35777588166.65600, saving model to best_model.h5\n",
      "Epoch 309/400\n",
      "18000/18000 [==============================] - 1s 55us/step - loss: 30721635410.8302 - val_loss: 35760634331.1360\n",
      "\n",
      "Epoch 00309: val_loss improved from 35777588166.65600 to 35760634331.13600, saving model to best_model.h5\n",
      "Epoch 310/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 30710855046.4853 - val_loss: 35738324500.4800\n",
      "\n",
      "Epoch 00310: val_loss improved from 35760634331.13600 to 35738324500.48000, saving model to best_model.h5\n",
      "Epoch 311/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 30701429398.6418 - val_loss: 35733868183.5520\n",
      "\n",
      "Epoch 00311: val_loss improved from 35738324500.48000 to 35733868183.55200, saving model to best_model.h5\n",
      "Epoch 312/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 30689093932.3733 - val_loss: 35723935907.8400\n",
      "\n",
      "Epoch 00312: val_loss improved from 35733868183.55200 to 35723935907.84000, saving model to best_model.h5\n",
      "Epoch 313/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30679519679.8293 - val_loss: 35707708276.7360\n",
      "\n",
      "Epoch 00313: val_loss improved from 35723935907.84000 to 35707708276.73600, saving model to best_model.h5\n",
      "Epoch 314/400\n",
      "18000/18000 [==============================] - 1s 53us/step - loss: 30667599049.1591 - val_loss: 35704586960.8960\n",
      "\n",
      "Epoch 00314: val_loss improved from 35707708276.73600 to 35704586960.89600, saving model to best_model.h5\n",
      "Epoch 315/400\n",
      "18000/18000 [==============================] - 1s 50us/step - loss: 30654091562.5529 - val_loss: 35702940434.4320\n",
      "\n",
      "Epoch 00315: val_loss improved from 35704586960.89600 to 35702940434.43200, saving model to best_model.h5\n",
      "Epoch 316/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30635524210.6880 - val_loss: 35690823319.5520\n",
      "\n",
      "Epoch 00316: val_loss improved from 35702940434.43200 to 35690823319.55200, saving model to best_model.h5\n",
      "Epoch 317/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30636343388.8427 - val_loss: 35694366523.3920\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 35690823319.55200\n",
      "Epoch 318/400\n",
      "18000/18000 [==============================] - 1s 39us/step - loss: 30618975629.7671 - val_loss: 35675014496.2560\n",
      "\n",
      "Epoch 00318: val_loss improved from 35690823319.55200 to 35675014496.25600, saving model to best_model.h5\n",
      "Epoch 319/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30608550168.3484 - val_loss: 35657420210.1760\n",
      "\n",
      "Epoch 00319: val_loss improved from 35675014496.25600 to 35657420210.17600, saving model to best_model.h5\n",
      "Epoch 320/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 30592677988.1244 - val_loss: 35644005908.4800\n",
      "\n",
      "Epoch 00320: val_loss improved from 35657420210.17600 to 35644005908.48000, saving model to best_model.h5\n",
      "Epoch 321/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 30594313173.2196 - val_loss: 35636143489.0240\n",
      "\n",
      "Epoch 00321: val_loss improved from 35644005908.48000 to 35636143489.02400, saving model to best_model.h5\n",
      "Epoch 322/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30572102301.9236 - val_loss: 35627690917.8880\n",
      "\n",
      "Epoch 00322: val_loss improved from 35636143489.02400 to 35627690917.88800, saving model to best_model.h5\n",
      "Epoch 323/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 30564800866.9867 - val_loss: 35587345711.1040\n",
      "\n",
      "Epoch 00323: val_loss improved from 35627690917.88800 to 35587345711.10400, saving model to best_model.h5\n",
      "Epoch 324/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 47us/step - loss: 30554882730.6667 - val_loss: 35575017570.3040\n",
      "\n",
      "Epoch 00324: val_loss improved from 35587345711.10400 to 35575017570.30400, saving model to best_model.h5\n",
      "Epoch 325/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30537830861.4827 - val_loss: 35585016168.4480\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 35575017570.30400\n",
      "Epoch 326/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 30530092886.6987 - val_loss: 35569878368.2560\n",
      "\n",
      "Epoch 00326: val_loss improved from 35575017570.30400 to 35569878368.25600, saving model to best_model.h5\n",
      "Epoch 327/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30516127121.4080 - val_loss: 35591999946.7520\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 35569878368.25600\n",
      "Epoch 328/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30514390080.6258 - val_loss: 35568368189.4400\n",
      "\n",
      "Epoch 00328: val_loss improved from 35569878368.25600 to 35568368189.44000, saving model to best_model.h5\n",
      "Epoch 329/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30495993373.5822 - val_loss: 35548152397.8240\n",
      "\n",
      "Epoch 00329: val_loss improved from 35568368189.44000 to 35548152397.82400, saving model to best_model.h5\n",
      "Epoch 330/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30476560902.8267 - val_loss: 35516302622.7200\n",
      "\n",
      "Epoch 00330: val_loss improved from 35548152397.82400 to 35516302622.72000, saving model to best_model.h5\n",
      "Epoch 331/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30481813835.3209 - val_loss: 35518094409.7280\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 35516302622.72000\n",
      "Epoch 332/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30461238891.8613 - val_loss: 35499839619.0720\n",
      "\n",
      "Epoch 00332: val_loss improved from 35516302622.72000 to 35499839619.07200, saving model to best_model.h5\n",
      "Epoch 333/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30456196093.2693 - val_loss: 35514405355.5200\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 35499839619.07200\n",
      "Epoch 334/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30442760532.4231 - val_loss: 35488580304.8960\n",
      "\n",
      "Epoch 00334: val_loss improved from 35499839619.07200 to 35488580304.89600, saving model to best_model.h5\n",
      "Epoch 335/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30434869300.7929 - val_loss: 35477035843.5840\n",
      "\n",
      "Epoch 00335: val_loss improved from 35488580304.89600 to 35477035843.58400, saving model to best_model.h5\n",
      "Epoch 336/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30420854454.4996 - val_loss: 35459008954.3680\n",
      "\n",
      "Epoch 00336: val_loss improved from 35477035843.58400 to 35459008954.36800, saving model to best_model.h5\n",
      "Epoch 337/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30399696222.4356 - val_loss: 35463389085.6960\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 35459008954.36800\n",
      "Epoch 338/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30391954442.0124 - val_loss: 35453361586.1760\n",
      "\n",
      "Epoch 00338: val_loss improved from 35459008954.36800 to 35453361586.17600, saving model to best_model.h5\n",
      "Epoch 339/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30379755849.5004 - val_loss: 35449332629.5040\n",
      "\n",
      "Epoch 00339: val_loss improved from 35453361586.17600 to 35449332629.50400, saving model to best_model.h5\n",
      "Epoch 340/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 30376568284.0462 - val_loss: 35441894653.9520\n",
      "\n",
      "Epoch 00340: val_loss improved from 35449332629.50400 to 35441894653.95200, saving model to best_model.h5\n",
      "Epoch 341/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30361936330.7520 - val_loss: 35407701311.4880\n",
      "\n",
      "Epoch 00341: val_loss improved from 35441894653.95200 to 35407701311.48800, saving model to best_model.h5\n",
      "Epoch 342/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30345900430.6773 - val_loss: 35428090478.5920\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 35407701311.48800\n",
      "Epoch 343/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 30344359695.7013 - val_loss: 35402642227.2000\n",
      "\n",
      "Epoch 00343: val_loss improved from 35407701311.48800 to 35402642227.20000, saving model to best_model.h5\n",
      "Epoch 344/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 30327811524.3804 - val_loss: 35374384185.3440\n",
      "\n",
      "Epoch 00344: val_loss improved from 35402642227.20000 to 35374384185.34400, saving model to best_model.h5\n",
      "Epoch 345/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 30316676794.1404 - val_loss: 35368942305.2800\n",
      "\n",
      "Epoch 00345: val_loss improved from 35374384185.34400 to 35368942305.28000, saving model to best_model.h5\n",
      "Epoch 346/400\n",
      "18000/18000 [==============================] - 1s 61us/step - loss: 30312112291.8400 - val_loss: 35348974665.7280\n",
      "\n",
      "Epoch 00346: val_loss improved from 35368942305.28000 to 35348974665.72800, saving model to best_model.h5\n",
      "Epoch 347/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 30298328842.2400 - val_loss: 35351982080.0000\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 35348974665.72800\n",
      "Epoch 348/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 30296980094.9760 - val_loss: 35341099106.3040\n",
      "\n",
      "Epoch 00348: val_loss improved from 35348974665.72800 to 35341099106.30400, saving model to best_model.h5\n",
      "Epoch 349/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 30282037792.3129 - val_loss: 35364069900.2880\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 35341099106.30400\n",
      "Epoch 350/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 30267946607.5022 - val_loss: 35333255462.9120\n",
      "\n",
      "Epoch 00350: val_loss improved from 35341099106.30400 to 35333255462.91200, saving model to best_model.h5\n",
      "Epoch 351/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30256030629.8880 - val_loss: 35307365564.4160\n",
      "\n",
      "Epoch 00351: val_loss improved from 35333255462.91200 to 35307365564.41600, saving model to best_model.h5\n",
      "Epoch 352/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 30253091873.6782 - val_loss: 35293578330.1120\n",
      "\n",
      "Epoch 00352: val_loss improved from 35307365564.41600 to 35293578330.11200, saving model to best_model.h5\n",
      "Epoch 353/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 30231672768.2844 - val_loss: 35283245531.1360\n",
      "\n",
      "Epoch 00353: val_loss improved from 35293578330.11200 to 35283245531.13600, saving model to best_model.h5\n",
      "Epoch 354/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30219834137.7138 - val_loss: 35283831390.2080\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 35283245531.13600\n",
      "Epoch 355/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 30210993047.3244 - val_loss: 35273904947.2000\n",
      "\n",
      "Epoch 00355: val_loss improved from 35283245531.13600 to 35273904947.20000, saving model to best_model.h5\n",
      "Epoch 356/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 30208400787.2284 - val_loss: 35264711294.9760\n",
      "\n",
      "Epoch 00356: val_loss improved from 35273904947.20000 to 35264711294.97600, saving model to best_model.h5\n",
      "Epoch 357/400\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 30192333793.9627 - val_loss: 35246008893.4400\n",
      "\n",
      "Epoch 00357: val_loss improved from 35264711294.97600 to 35246008893.44000, saving model to best_model.h5\n",
      "Epoch 358/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30191091189.5324 - val_loss: 35216056975.3600\n",
      "\n",
      "Epoch 00358: val_loss improved from 35246008893.44000 to 35216056975.36000, saving model to best_model.h5\n",
      "Epoch 359/400\n",
      "18000/18000 [==============================] - 1s 42us/step - loss: 30176771710.9760 - val_loss: 35206169985.0240\n",
      "\n",
      "Epoch 00359: val_loss improved from 35216056975.36000 to 35206169985.02400, saving model to best_model.h5\n",
      "Epoch 360/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30162339958.3289 - val_loss: 35199309119.4880\n",
      "\n",
      "Epoch 00360: val_loss improved from 35206169985.02400 to 35199309119.48800, saving model to best_model.h5\n",
      "Epoch 361/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 43us/step - loss: 30152967640.4053 - val_loss: 35193901350.9120\n",
      "\n",
      "Epoch 00361: val_loss improved from 35199309119.48800 to 35193901350.91200, saving model to best_model.h5\n",
      "Epoch 362/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30139642995.5982 - val_loss: 35197942824.9600\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 35193901350.91200\n",
      "Epoch 363/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30128694460.4160 - val_loss: 35189572599.8080\n",
      "\n",
      "Epoch 00363: val_loss improved from 35193901350.91200 to 35189572599.80800, saving model to best_model.h5\n",
      "Epoch 364/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30123290600.3342 - val_loss: 35177944580.0960\n",
      "\n",
      "Epoch 00364: val_loss improved from 35189572599.80800 to 35177944580.09600, saving model to best_model.h5\n",
      "Epoch 365/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30118028900.5796 - val_loss: 35179561418.7520\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 35177944580.09600\n",
      "Epoch 366/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30099817840.6400 - val_loss: 35147152588.8000\n",
      "\n",
      "Epoch 00366: val_loss improved from 35177944580.09600 to 35147152588.80000, saving model to best_model.h5\n",
      "Epoch 367/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30101348291.9253 - val_loss: 35153502502.9120\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 35147152588.80000\n",
      "Epoch 368/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 30086162189.8809 - val_loss: 35156997111.8080\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 35147152588.80000\n",
      "Epoch 369/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30085005824.4551 - val_loss: 35126371090.4320\n",
      "\n",
      "Epoch 00369: val_loss improved from 35147152588.80000 to 35126371090.43200, saving model to best_model.h5\n",
      "Epoch 370/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30063486551.8364 - val_loss: 35130953367.5520\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 35126371090.43200\n",
      "Epoch 371/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 30047530849.6213 - val_loss: 35087920594.9440\n",
      "\n",
      "Epoch 00371: val_loss improved from 35126371090.43200 to 35087920594.94400, saving model to best_model.h5\n",
      "Epoch 372/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30048522400.1991 - val_loss: 35088450650.1120\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 35087920594.94400\n",
      "Epoch 373/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 30051494130.1191 - val_loss: 35059838222.3360\n",
      "\n",
      "Epoch 00373: val_loss improved from 35087920594.94400 to 35059838222.33600, saving model to best_model.h5\n",
      "Epoch 374/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 30025816741.2053 - val_loss: 35052700401.6640\n",
      "\n",
      "Epoch 00374: val_loss improved from 35059838222.33600 to 35052700401.66400, saving model to best_model.h5\n",
      "Epoch 375/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 30011144670.7769 - val_loss: 35059193348.0960\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 35052700401.66400\n",
      "Epoch 376/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 30001709564.8142 - val_loss: 35045699944.4480\n",
      "\n",
      "Epoch 00376: val_loss improved from 35052700401.66400 to 35045699944.44800, saving model to best_model.h5\n",
      "Epoch 377/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 29994223519.5164 - val_loss: 35032854003.7120\n",
      "\n",
      "Epoch 00377: val_loss improved from 35045699944.44800 to 35032854003.71200, saving model to best_model.h5\n",
      "Epoch 378/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 29985945174.0160 - val_loss: 35029241495.5520\n",
      "\n",
      "Epoch 00378: val_loss improved from 35032854003.71200 to 35029241495.55200, saving model to best_model.h5\n",
      "Epoch 379/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 29985638463.7156 - val_loss: 35025131110.4000\n",
      "\n",
      "Epoch 00379: val_loss improved from 35029241495.55200 to 35025131110.40000, saving model to best_model.h5\n",
      "Epoch 380/400\n",
      "18000/18000 [==============================] - 1s 51us/step - loss: 29968314204.1600 - val_loss: 35003656110.0800\n",
      "\n",
      "Epoch 00380: val_loss improved from 35025131110.40000 to 35003656110.08000, saving model to best_model.h5\n",
      "Epoch 381/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 29969023465.6996 - val_loss: 34983852834.8160\n",
      "\n",
      "Epoch 00381: val_loss improved from 35003656110.08000 to 34983852834.81600, saving model to best_model.h5\n",
      "Epoch 382/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 29949909381.5751 - val_loss: 34972298936.3200\n",
      "\n",
      "Epoch 00382: val_loss improved from 34983852834.81600 to 34972298936.32000, saving model to best_model.h5\n",
      "Epoch 383/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 29938800787.4560 - val_loss: 34982526484.4800\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 34972298936.32000\n",
      "Epoch 384/400\n",
      "18000/18000 [==============================] - 1s 52us/step - loss: 29930644444.5013 - val_loss: 34969992364.0320\n",
      "\n",
      "Epoch 00384: val_loss improved from 34972298936.32000 to 34969992364.03200, saving model to best_model.h5\n",
      "Epoch 385/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 29926263983.6729 - val_loss: 34951936507.9040\n",
      "\n",
      "Epoch 00385: val_loss improved from 34969992364.03200 to 34951936507.90400, saving model to best_model.h5\n",
      "Epoch 386/400\n",
      "18000/18000 [==============================] - 1s 48us/step - loss: 29913018054.8836 - val_loss: 34954905485.3120\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 34951936507.90400\n",
      "Epoch 387/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 29901476444.3876 - val_loss: 34941263675.3920\n",
      "\n",
      "Epoch 00387: val_loss improved from 34951936507.90400 to 34941263675.39200, saving model to best_model.h5\n",
      "Epoch 388/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 29894713626.1689 - val_loss: 34931351224.3200\n",
      "\n",
      "Epoch 00388: val_loss improved from 34941263675.39200 to 34931351224.32000, saving model to best_model.h5\n",
      "Epoch 389/400\n",
      "18000/18000 [==============================] - 1s 49us/step - loss: 29892421113.1733 - val_loss: 34915141091.3280\n",
      "\n",
      "Epoch 00389: val_loss improved from 34931351224.32000 to 34915141091.32800, saving model to best_model.h5\n",
      "Epoch 390/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 29875464767.2604 - val_loss: 34929495441.4080\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 34915141091.32800\n",
      "Epoch 391/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 29880819313.3227 - val_loss: 34900234731.5200\n",
      "\n",
      "Epoch 00391: val_loss improved from 34915141091.32800 to 34900234731.52000, saving model to best_model.h5\n",
      "Epoch 392/400\n",
      "18000/18000 [==============================] - 1s 40us/step - loss: 29861296963.5840 - val_loss: 34897185505.2800\n",
      "\n",
      "Epoch 00392: val_loss improved from 34900234731.52000 to 34897185505.28000, saving model to best_model.h5\n",
      "Epoch 393/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 29851920405.8453 - val_loss: 34903928111.1040\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 34897185505.28000\n",
      "Epoch 394/400\n",
      "18000/18000 [==============================] - 1s 45us/step - loss: 29837986234.3680 - val_loss: 34894097350.6560\n",
      "\n",
      "Epoch 00394: val_loss improved from 34897185505.28000 to 34894097350.65600, saving model to best_model.h5\n",
      "Epoch 395/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 29828995596.2880 - val_loss: 34874919124.9920\n",
      "\n",
      "Epoch 00395: val_loss improved from 34894097350.65600 to 34874919124.99200, saving model to best_model.h5\n",
      "Epoch 396/400\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 29829302089.9556 - val_loss: 34860350537.7280\n",
      "\n",
      "Epoch 00396: val_loss improved from 34874919124.99200 to 34860350537.72800, saving model to best_model.h5\n",
      "Epoch 397/400\n",
      "18000/18000 [==============================] - 1s 44us/step - loss: 29809017804.1173 - val_loss: 34849582579.7120\n",
      "\n",
      "Epoch 00397: val_loss improved from 34860350537.72800 to 34849582579.71200, saving model to best_model.h5\n",
      "Epoch 398/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 48us/step - loss: 29800239515.4204 - val_loss: 34851138633.7280\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 34849582579.71200\n",
      "Epoch 399/400\n",
      "18000/18000 [==============================] - 1s 47us/step - loss: 29814908483.8116 - val_loss: 34851638149.1200\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 34849582579.71200\n",
      "Epoch 400/400\n",
      "18000/18000 [==============================] - 1s 46us/step - loss: 29797488394.2400 - val_loss: 34813784293.3760\n",
      "\n",
      "Epoch 00400: val_loss improved from 34849582579.71200 to 34813784293.37600, saving model to best_model.h5\n",
      "Model score: 0.6566940217993732\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set callback functions to early stop training and save the \n",
    "# best model so far\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=5),\n",
    "            ModelCheckpoint(filepath='best_model.h5',\n",
    "                            monitor='val_loss',\n",
    "                            save_best_only=True,\n",
    "                           verbose=1)]\n",
    "\n",
    "model = nn_model(layers=[20,20,20])\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "history = model.fit(selected_feature_train, price_train,\n",
    "            epochs=400, callbacks=callbacks, batch_size=128,\n",
    "            validation_data=(selected_feature_val, price_val))\n",
    "\n",
    "model_score = score(model.predict(selected_feature_val), price_val)\n",
    "print(f\"Model score: {model_score}\")\n",
    "plot_loss(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You don't have to remember these resources because they're here when you need them\n",
    "https://www.tensorflow.org/api_docs\n",
    "\n",
    "https://keras.io/\n",
    "\n",
    "https://www.tensorflow.org/tutorials/\n",
    "\n",
    "https://www.google.com\n",
    "\n",
    "### Don't trust me, trust your validation errors\n",
    "### Don't look at your test set until you're actually going to test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
