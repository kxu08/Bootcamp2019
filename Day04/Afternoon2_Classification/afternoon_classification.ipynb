{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification -- Images & Hands-On\n",
    "\n",
    "## Table of Contents\n",
    "<ol>\n",
    "    <li>Processing of complicated data like images</li>\n",
    "    <li>Thinking about models to use for image classification</li>\n",
    "    <li>Implementation of common models</li>\n",
    "    <li>Convolutional neural networks -- an ML greatest hit</li>\n",
    "</ol>\n",
    "\n",
    "## 1. Processing of complicated data like images\n",
    "\n",
    "#### Suppose we begin with colored 32 x 32 pixel images of objects we wish to classify.\n",
    "\n",
    "![](cifar.png)\n",
    "<span style=\"font-size:0.75em;\">CIFAR-10 Krizhevsky et al.</span>\n",
    "\n",
    "### How can we encode the information from one image?\n",
    "![](corgis.png)\n",
    "![](doge.png)\n",
    "<span style=\"font-size:0.75em;\">commonlounge.com; subsubroutine.com</span>\n",
    "\n",
    "### Let's start with a simpler example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As long as each data point is of the same shape, we can unroll these 2- or 3-tensors into long vectors\n",
    "- How many dimensions in each CIFAR-10 data point? Remember this number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our classification output will be a vector of 0s except for the target class, which should be a 1.\n",
    "### Presently our output is instead encoded as a single ordinal variable between 0 and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ord = y # In case we need this later\n",
    "\n",
    "# Encode each element of y as 10-length \"one-hot vector\" with binary elements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lastly, let's choose an error metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(true, pred):\n",
    "    # some accuracy metric\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Thinking about models to use for image classification\n",
    "\n",
    "### k-Nearest neighbors\n",
    "\n",
    "* 1-Nearest Neighbors (aka nearest neighbors)\n",
    "    - Use some distance metric to compare each 784-D vector to all training examples\n",
    "    - Order samples by distance\n",
    "    - Classify the same as the smallest distance example\n",
    "* k-Nearest Neighbors\n",
    "    - Classify by committee based on several small distances\n",
    "* Where do these fail?\n",
    "* How do these scale with training examples?\n",
    "\n",
    "### Logistic regression\n",
    "* Optimal parameters attained from maximizing the likelihood of dataset, aka minimizing the negative log-likelihood\n",
    "$$\\mathcal{L}(\\theta = \\{W,b\\},\\mathcal{D}) = \\sum_{i=0}^{|\\mathcal{D}|} log(P(Y = y^{(i)} | x^{(i)},W,b))$$\n",
    "![](log_reg.png)\n",
    "* \"nonlinear\" -- though always depends directly on weighted sums of pixels\n",
    "\n",
    "### Random forest classifiers\n",
    "* \"Split\" predictions based on pixels or collection of pixels\n",
    "* Truly nonlinear\n",
    "\n",
    "### Feed-forward neural networks\n",
    "* Nonlinear\n",
    "* Permits \"communication\" between pixels via dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation of common models\n",
    "### WAIT what haven't I done yet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create equivalent accuracy score for ordinal outputs\n",
    "def score_ord(true, preds):\n",
    "    acc = 0\n",
    "    for i in range(len(true)):\n",
    "        if true[i] == preds[i]:\n",
    "            acc += 1\n",
    "    score = acc / len(true)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score_ord(y_ord_val,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest classifiers - you try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-forward neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9708333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier(hidden_layer_sizes=(100,50,20))\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_val)\n",
    "print(score(y_val, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](conv_net.png)\n",
    "<span style=\"font-size:0.75em;\">easy-tensorflow.com</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generally, a <i>filter</i> is a rectangular $k \\times l$ matrix of weights.\n",
    "* A single <i>filter</i> traverses an image, elementwise multiplying pixels in its range, adding, and performing a nonlinearity.\n",
    "* The new <i>feature map</i> generated is typically the same size or smaller than the input.\n",
    "* Many <i>feature maps</i> are generated with different <i>filters</i>, each with different weights\n",
    "* <i>Pooling layers</i> serve to reduce feature map dimensionality.\n",
    "* The CNN concludes with generic Dense layers\n",
    "\n",
    "### Examines local areas of photographs -- takes full photo matrix as input, not flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y), (X_test, y_test) = mnist.load_data()\n",
    "y_train = keras.utils.to_categorical(y)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppose I have validated the following hyperparameters such that I believe they are optimal. <i>Now</i> we can test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose train and test, add an additional 1 rank to indicate\n",
    "# greyscale for special layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/Keras-2.2.4-py3.7.egg/keras/backend/tensorflow_backend.py:3651: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/25\n",
      "60000/60000 [==============================] - 227s 4ms/step - loss: 0.2564 - acc: 0.9220\n",
      "Epoch 2/25\n",
      "60000/60000 [==============================] - 230s 4ms/step - loss: 0.0658 - acc: 0.9803\n",
      "Epoch 3/25\n",
      "60000/60000 [==============================] - 231s 4ms/step - loss: 0.0503 - acc: 0.9848\n",
      "Epoch 4/25\n",
      "60000/60000 [==============================] - 230s 4ms/step - loss: 0.0414 - acc: 0.9872\n",
      "Epoch 5/25\n",
      "60000/60000 [==============================] - 230s 4ms/step - loss: 0.0352 - acc: 0.9890\n",
      "Epoch 6/25\n",
      "60000/60000 [==============================] - 231s 4ms/step - loss: 0.0328 - acc: 0.9900\n",
      "Epoch 7/25\n",
      "60000/60000 [==============================] - 231s 4ms/step - loss: 0.0276 - acc: 0.9916\n",
      "Epoch 8/25\n",
      "60000/60000 [==============================] - 232s 4ms/step - loss: 0.0259 - acc: 0.9918\n",
      "Epoch 9/25\n",
      "60000/60000 [==============================] - 234s 4ms/step - loss: 0.0240 - acc: 0.9927\n",
      "Epoch 10/25\n",
      "60000/60000 [==============================] - 235s 4ms/step - loss: 0.0232 - acc: 0.9928\n",
      "Epoch 11/25\n",
      "60000/60000 [==============================] - 245s 4ms/step - loss: 0.0216 - acc: 0.9933\n",
      "Epoch 12/25\n",
      "60000/60000 [==============================] - 243s 4ms/step - loss: 0.0190 - acc: 0.9940\n",
      "Epoch 13/25\n",
      "60000/60000 [==============================] - 254s 4ms/step - loss: 0.0220 - acc: 0.9929\n",
      "Epoch 14/25\n",
      "60000/60000 [==============================] - 264s 4ms/step - loss: 0.0189 - acc: 0.9943\n",
      "Epoch 15/25\n",
      "60000/60000 [==============================] - 274s 5ms/step - loss: 0.0155 - acc: 0.9950\n",
      "Epoch 16/25\n",
      "60000/60000 [==============================] - 296s 5ms/step - loss: 0.0165 - acc: 0.9948\n",
      "Epoch 17/25\n",
      "60000/60000 [==============================] - 309s 5ms/step - loss: 0.0151 - acc: 0.9956\n",
      "Epoch 18/25\n",
      "60000/60000 [==============================] - 301s 5ms/step - loss: 0.0124 - acc: 0.9959\n",
      "Epoch 19/25\n",
      "60000/60000 [==============================] - 234s 4ms/step - loss: 0.0137 - acc: 0.9957\n",
      "Epoch 20/25\n",
      "60000/60000 [==============================] - 235s 4ms/step - loss: 0.0116 - acc: 0.9963\n",
      "Epoch 21/25\n",
      "60000/60000 [==============================] - 233s 4ms/step - loss: 0.0126 - acc: 0.9957\n",
      "Epoch 22/25\n",
      "60000/60000 [==============================] - 234s 4ms/step - loss: 0.0103 - acc: 0.9966\n",
      "Epoch 23/25\n",
      "60000/60000 [==============================] - 237s 4ms/step - loss: 0.0103 - acc: 0.9966\n",
      "Epoch 24/25\n",
      "60000/60000 [==============================] - 237s 4ms/step - loss: 0.0097 - acc: 0.9968\n",
      "Epoch 25/25\n",
      "60000/60000 [==============================] - 231s 4ms/step - loss: 0.0108 - acc: 0.9966\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Dropout, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32,kernel_size=3,activation='relu',input_shape=(28,28,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,kernel_size=3,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(64,kernel_size=3,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,kernel_size=3,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=128, epochs=25, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](KaggleMNIST.png)\n",
    "<span style=\"font-size:0.75em;\">Kaggle - Chris Deotte 2018</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9953\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_test)\n",
    "final = np.zeros_like(preds)\n",
    "final[np.arange(len(preds)), preds.argmax(1)] = 1\n",
    "\n",
    "print(score(y_test, final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"nice_cnn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A word on inductive bias and domain knowledge\n",
    "* CNNs take advantage of our understanding of local features in mapping images to semantic meaning (number labels, dog/cat/plane)\n",
    "* \"A universal function approximator\": The infinitely large dense NN can fit any analytic function exactly with enough data.\n",
    "    - \"Not really\": We rarely have \"enough data\" and can't train infinitely large NNs\n",
    "    - The name of the game is making the network size and data requirements <i>practical</i>\n",
    "* The state of the art usually comes from understanding your problem first"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp",
   "language": "python",
   "name": "bootcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
