{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification -- Images & Hands-On\n",
    "\n",
    "## Table of Contents\n",
    "<ol>\n",
    "    <li>Processing of complicated data like images</li>\n",
    "    <li>Thinking about models to use for image classification</li>\n",
    "    <li>Implementation of common models</li>\n",
    "    <li>Convolutional neural networks -- an ML greatest hit</li>\n",
    "</ol>\n",
    "\n",
    "## 1. Processing of complicated data like images\n",
    "\n",
    "#### Suppose we begin with colored 32 x 32 pixel images of objects we wish to classify.\n",
    "\n",
    "![](cifar.png)\n",
    "<span style=\"font-size:0.75em;\">CIFAR-10 Krizhevsky et al.</span>\n",
    "\n",
    "### How can we encode the information from one image?\n",
    "![](corgis.png)\n",
    "![](doge.png)\n",
    "<span style=\"font-size:0.75em;\">commonlounge.com; subsubroutine.com</span>\n",
    "\n",
    "### Let's start with a simpler example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd18d72b4e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "(X, y), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(type(X))\n",
    "print(X.shape, y.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "plt.imshow(X[0], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[20125], cmap=\"Greys\")\n",
    "print(np.max(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As long as each data point is of the same shape, we can unroll these 2- or 3-tensors into long vectors\n",
    "- How many dimensions in each CIFAR-10 data point? Remember this number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Flatten each X\n",
    "X = X.reshape(X.shape[0], X.shape[1]*X.shape[2])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our classification output will be a vector of 0s except for the target class, which should be a 1.\n",
    "### Presently our output is instead encoded as a single ordinal variable between 0 and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Encode each element of y as 10-length \"one-hot vector\" with binary elements\n",
    "y_ord = y\n",
    "\n",
    "y = keras.utils.to_categorical(y)\n",
    "print(y.shape)\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lastly, let's choose an error metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(true, pred):\n",
    "    acc = 0\n",
    "    for i in range(len(true)):\n",
    "        if np.sum(np.dot(true[i], pred[i])) == 1.0:\n",
    "            acc += 1\n",
    "    score = acc / len(true)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Thinking about models to use for image classification\n",
    "\n",
    "### k-Nearest neighbors\n",
    "\n",
    "* 1-Nearest Neighbors (aka nearest neighbors)\n",
    "    - Use some distance metric to compare each 784-D vector to all others\n",
    "    - Order samples by distance\n",
    "    - Classify the same as the smallest distance example\n",
    "* k-Nearest Neighbors\n",
    "    - Classify by committee based on several small distances\n",
    "* Where do these fail?\n",
    "* How do these scale with training examples?\n",
    "\n",
    "### Logistic regression\n",
    "* Optimal parameters attained from maximizing the likelihood of dataset, aka minimizing the negative log-likelihood\n",
    "$$\\mathcal{L}(\\theta = \\{W,b\\},\\mathcal{D}) = \\sum_{i=0}^{|\\mathcal{D}|} log(P(Y = y^{(i)} | x^{(i)},W,b))$$\n",
    "![](log_reg.png)\n",
    "* \"nonlinear\" -- though always depends directly on weighted sums of pixels\n",
    "\n",
    "### Random forest classifiers\n",
    "* \"Split\" predictions based on pixels or collection of pixels\n",
    "* Truly nonlinear\n",
    "\n",
    "### Feed-forward neural networks\n",
    "* Nonlinear\n",
    "* Permits \"communication\" between pixels via dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation of common models\n",
    "### WAIT what haven't I done yet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train val split before model choosing\n",
    "X_train = X[:54000]\n",
    "y_train = y[:54000]\n",
    "y_ord_train = y_ord[:54000]\n",
    "\n",
    "X_val = X[54000:]\n",
    "y_val = y[54000:]\n",
    "y_ord_val = y_ord[54000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_val[:100])\n",
    "print(score(y_val[:100], preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 41 epochs took 20 seconds\n",
      "convergence after 41 epochs took 20 seconds\n",
      "convergence after 44 epochs took 22 seconds\n",
      "convergence after 53 epochs took 29 seconds\n",
      "convergence after 45 epochs took 31 seconds\n",
      "convergence after 48 epochs took 34 seconds\n",
      "convergence after 54 epochs took 37 seconds\n",
      "convergence after 47 epochs took 31 seconds\n",
      "convergence after 45 epochs took 23 seconds\n",
      "convergence after 53 epochs took 27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:  1.3min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='saga', tol=0.01, n_jobs=4, verbose=1)\n",
    "\n",
    "model.fit(X_train, y_ord_train)\n",
    "preds = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create equivalent accuracy score for ordinal outputs\n",
    "def score_ord(true, preds):\n",
    "    acc = 0\n",
    "    for i in range(len(true)):\n",
    "        if true[i] == preds[i]:\n",
    "            acc += 1\n",
    "    score = acc / len(true)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.931\n"
     ]
    }
   ],
   "source": [
    "print(score_ord(y_ord_val,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8836666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_val)\n",
    "print(score(y_val, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-forward neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9708333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier(hidden_layer_sizes=(100,50,20))\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_val)\n",
    "print(score(y_val, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](conv_net.png)\n",
    "<span style=\"font-size:0.75em;\">easy-tensorflow.com</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A <i>filter</i> might be a square $k \\times k$ matrix of weights.\n",
    "* A single <i>filter</i> traverses an image, elementwise multiplying pixels in its range, adding, and performing a nonlinearity.\n",
    "* The new <i>feature map</i> generated is typically the same size or smaller than the input.\n",
    "* Many <i>feature maps</i> are generated with different <i>filters</i>, each with different weights\n",
    "* <i>Pooling layers</i> serve to reduce feature map dimensionality.\n",
    "* The CNN concludes with generic Dense layers\n",
    "\n",
    "### Examines local areas of photographs -- takes full photo matrix as input, not flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y), (X_test, y_test) = mnist.load_data()\n",
    "y_train = keras.utils.to_categorical(y)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppose I have validated the following hyperparameters such that I believe they are optimal. <i>Now</i> we can test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X.reshape(60000,28,28,1)\n",
    "# Additional 1 rank to indicate greyscale for special layers\n",
    "\n",
    "X_test = X_test.reshape(10000,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/Keras-2.2.4-py3.7.egg/keras/backend/tensorflow_backend.py:3651: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/25\n",
      "60000/60000 [==============================] - 227s 4ms/step - loss: 0.2564 - acc: 0.9220\n",
      "Epoch 2/25\n",
      "60000/60000 [==============================] - 230s 4ms/step - loss: 0.0658 - acc: 0.9803\n",
      "Epoch 3/25\n",
      "60000/60000 [==============================] - 231s 4ms/step - loss: 0.0503 - acc: 0.9848\n",
      "Epoch 4/25\n",
      "60000/60000 [==============================] - 230s 4ms/step - loss: 0.0414 - acc: 0.9872\n",
      "Epoch 5/25\n",
      "60000/60000 [==============================] - 230s 4ms/step - loss: 0.0352 - acc: 0.9890\n",
      "Epoch 6/25\n",
      "60000/60000 [==============================] - 231s 4ms/step - loss: 0.0328 - acc: 0.9900\n",
      "Epoch 7/25\n",
      "60000/60000 [==============================] - 231s 4ms/step - loss: 0.0276 - acc: 0.9916\n",
      "Epoch 8/25\n",
      "60000/60000 [==============================] - 232s 4ms/step - loss: 0.0259 - acc: 0.9918\n",
      "Epoch 9/25\n",
      "60000/60000 [==============================] - 234s 4ms/step - loss: 0.0240 - acc: 0.9927\n",
      "Epoch 10/25\n",
      "60000/60000 [==============================] - 235s 4ms/step - loss: 0.0232 - acc: 0.9928\n",
      "Epoch 11/25\n",
      "60000/60000 [==============================] - 245s 4ms/step - loss: 0.0216 - acc: 0.9933\n",
      "Epoch 12/25\n",
      "60000/60000 [==============================] - 243s 4ms/step - loss: 0.0190 - acc: 0.9940\n",
      "Epoch 13/25\n",
      "60000/60000 [==============================] - 254s 4ms/step - loss: 0.0220 - acc: 0.9929\n",
      "Epoch 14/25\n",
      "60000/60000 [==============================] - 264s 4ms/step - loss: 0.0189 - acc: 0.9943\n",
      "Epoch 15/25\n",
      "60000/60000 [==============================] - 274s 5ms/step - loss: 0.0155 - acc: 0.9950\n",
      "Epoch 16/25\n",
      "60000/60000 [==============================] - 296s 5ms/step - loss: 0.0165 - acc: 0.9948\n",
      "Epoch 17/25\n",
      "60000/60000 [==============================] - 309s 5ms/step - loss: 0.0151 - acc: 0.9956\n",
      "Epoch 18/25\n",
      "60000/60000 [==============================] - 301s 5ms/step - loss: 0.0124 - acc: 0.9959\n",
      "Epoch 19/25\n",
      "60000/60000 [==============================] - 234s 4ms/step - loss: 0.0137 - acc: 0.9957\n",
      "Epoch 20/25\n",
      "60000/60000 [==============================] - 235s 4ms/step - loss: 0.0116 - acc: 0.9963\n",
      "Epoch 21/25\n",
      "60000/60000 [==============================] - 233s 4ms/step - loss: 0.0126 - acc: 0.9957\n",
      "Epoch 22/25\n",
      "60000/60000 [==============================] - 234s 4ms/step - loss: 0.0103 - acc: 0.9966\n",
      "Epoch 23/25\n",
      "60000/60000 [==============================] - 237s 4ms/step - loss: 0.0103 - acc: 0.9966\n",
      "Epoch 24/25\n",
      "60000/60000 [==============================] - 237s 4ms/step - loss: 0.0097 - acc: 0.9968\n",
      "Epoch 25/25\n",
      "60000/60000 [==============================] - 231s 4ms/step - loss: 0.0108 - acc: 0.9966\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Dropout, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32,kernel_size=3,activation='relu',input_shape=(28,28,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,kernel_size=3,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(64,kernel_size=3,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,kernel_size=3,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=128, epochs=25, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](KaggleMNIST.png)\n",
    "<span style=\"font-size:0.75em;\">Kaggle - Chris Deotte 2018</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9953\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_test)\n",
    "final = np.zeros_like(preds)\n",
    "final[np.arange(len(preds)), preds.argmax(1)] = 1\n",
    "\n",
    "print(score(y_test, final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"nice_cnn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A word on inductive bias and domain knowledge\n",
    "* CNNs take advantage of our understanding of local features in mapping images to semantic meaning (number labels, dog/cat/plane)\n",
    "* \"A universal function approximator\": The infinitely large dense NN can fit any analytic function exactly with enough data.\n",
    "    - \"Not really\": We rarely have \"enough data\" and can't train infinitely large NNs\n",
    "    - The name of the game is making the network size and data requirements <i>practical</i>\n",
    "* The state of the art usually comes from understanding your problem first"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
